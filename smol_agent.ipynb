{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNg943vjxiDn3K8/0OHc51b",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7abbb9ab6661477e824627e7f683ec9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_360764be05d642ffaa9a9cedca713193"
          }
        },
        "07e59bbe384a46808c48449637dcfdf7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b674c7dc2154d498300f0401b09522f",
            "placeholder": "​",
            "style": "IPY_MODEL_9daf7f90dd1d4a74a2c9d1bab9910de7",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "82a77bba8e63468cac5f6b1eba7fc7e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_c40f23cb34884bafb87485b799dd06d4",
            "placeholder": "​",
            "style": "IPY_MODEL_65b4a578274f442eaa2de3d7ef724482",
            "value": ""
          }
        },
        "104a6a228d594b48aba0579dee4c50cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_623809f599004cd4b5a55111ee75abe0",
            "style": "IPY_MODEL_2a032bacc24b4a7db46cbc6c744c3a9c",
            "value": true
          }
        },
        "a2555f9b79464ad4b63962ec04c8b19c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_88d2ab3cba0346b2863b4e86a0394ada",
            "style": "IPY_MODEL_cf0ce3f6cfe041fd897b3bfbac78849d",
            "tooltip": ""
          }
        },
        "46d7e3b3fd1e45608a081d4edf9db16d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c69354cfb8a04d6d84c662ccd1d1ebb8",
            "placeholder": "​",
            "style": "IPY_MODEL_ae3a54e0308e46079686cc0199ff591e",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "360764be05d642ffaa9a9cedca713193": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "0b674c7dc2154d498300f0401b09522f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9daf7f90dd1d4a74a2c9d1bab9910de7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c40f23cb34884bafb87485b799dd06d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "65b4a578274f442eaa2de3d7ef724482": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "623809f599004cd4b5a55111ee75abe0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a032bacc24b4a7db46cbc6c744c3a9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "88d2ab3cba0346b2863b4e86a0394ada": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf0ce3f6cfe041fd897b3bfbac78849d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "c69354cfb8a04d6d84c662ccd1d1ebb8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae3a54e0308e46079686cc0199ff591e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0eeb0b846ea94cb8a57961c08723661a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5b8db74df2e64394baedfc98bab8acdf",
            "placeholder": "​",
            "style": "IPY_MODEL_b68d755544bc4309a7c5a4309f046ce1",
            "value": "Connecting..."
          }
        },
        "5b8db74df2e64394baedfc98bab8acdf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b68d755544bc4309a7c5a4309f046ce1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SunkaraboinaPraveenKumar/Machine_Learning_Projects/blob/main/smol_agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_7O1O6W000H",
        "outputId": "7c94ef65-fb33-45b3-c094-1e4b1a83629f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.2/57.2 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.4/320.4 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m54.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.7/81.7 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m48.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m87.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.10.1 requires pandas<2.2.3dev0,>=2.0, but you have pandas 2.2.3 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.27.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2024.12.14)\n"
          ]
        }
      ],
      "source": [
        "!pip install -q smolagents\n",
        "!pip install huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "7abbb9ab6661477e824627e7f683ec9b",
            "07e59bbe384a46808c48449637dcfdf7",
            "82a77bba8e63468cac5f6b1eba7fc7e2",
            "104a6a228d594b48aba0579dee4c50cd",
            "a2555f9b79464ad4b63962ec04c8b19c",
            "46d7e3b3fd1e45608a081d4edf9db16d",
            "360764be05d642ffaa9a9cedca713193",
            "0b674c7dc2154d498300f0401b09522f",
            "9daf7f90dd1d4a74a2c9d1bab9910de7",
            "c40f23cb34884bafb87485b799dd06d4",
            "65b4a578274f442eaa2de3d7ef724482",
            "623809f599004cd4b5a55111ee75abe0",
            "2a032bacc24b4a7db46cbc6c744c3a9c",
            "88d2ab3cba0346b2863b4e86a0394ada",
            "cf0ce3f6cfe041fd897b3bfbac78849d",
            "c69354cfb8a04d6d84c662ccd1d1ebb8",
            "ae3a54e0308e46079686cc0199ff591e",
            "0eeb0b846ea94cb8a57961c08723661a",
            "5b8db74df2e64394baedfc98bab8acdf",
            "b68d755544bc4309a7c5a4309f046ce1"
          ]
        },
        "id": "xLLc81RE08cO",
        "outputId": "766f63b3-bc04-4c20-ac9b-000e5cc5c7f1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7abbb9ab6661477e824627e7f683ec9b"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from smolagents import CodeAgent, DuckDuckGoSearchTool, HfApiModel"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6KUzhMzd1Bxw",
        "outputId": "ce34531a-1888-40ad-dedc-dcf93f617e4d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_config.py:345: UserWarning: Valid config keys have changed in V2:\n",
            "* 'fields' has been removed\n",
            "  warnings.warn(message, UserWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent = CodeAgent(tools=[DuckDuckGoSearchTool()], model=HfApiModel())"
      ],
      "metadata": {
        "id": "Ooog5T6R1K9D"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJtS6yUq1gv6",
        "outputId": "46495938-2924-42d5-d194-ca7f45a3eeeb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<smolagents.agents.CodeAgent at 0x789b99001e10>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent.run(\"How many seconds would it take for a leopard at full speed to run through Pont des Arts?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WAbjPAil1lGP",
        "outputId": "d7cbf995-ac41-4330-c6fe-00edbd3d552d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[38;2;212;183;2m╭─\u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╮\u001b[0m\n",
              "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
              "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mHow many seconds would it take for a leopard at full speed to run through Pont des Arts?\u001b[0m                        \u001b[38;2;212;183;2m│\u001b[0m\n",
              "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
              "\u001b[38;2;212;183;2m╰─\u001b[0m\u001b[38;2;212;183;2m HfApiModel - Qwen/Qwen2.5-Coder-32B-Instruct \u001b[0m\u001b[38;2;212;183;2m─────────────────────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╯\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">╭──────────────────────────────────────────────────── </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ────────────────────────────────────────────────────╮</span>\n",
              "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
              "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">How many seconds would it take for a leopard at full speed to run through Pont des Arts?</span>                        <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
              "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
              "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">╰─ HfApiModel - Qwen/Qwen2.5-Coder-32B-Instruct ──────────────────────────────────────────────────────────────────╯</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m0\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "╭─ \u001b[1mExecuting this code:\u001b[0m ──────────────────────────────────────────────────────────────────────────────────────────╮\n",
              "│ \u001b[1;38;2;227;227;221;48;2;39;40;34m  \u001b[0m\u001b[38;2;101;102;96;48;2;39;40;34m1 \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mpont_des_arts_length\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mweb_search\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mquery\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mLength of Pont des Arts\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                         \u001b[0m │\n",
              "│ \u001b[1;38;2;227;227;221;48;2;39;40;34m  \u001b[0m\u001b[38;2;101;102;96;48;2;39;40;34m2 \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mleopard_speed\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mweb_search\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mquery\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mTop speed of leopard\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                   \u001b[0m │\n",
              "│ \u001b[1;38;2;227;227;221;48;2;39;40;34m  \u001b[0m\u001b[38;2;101;102;96;48;2;39;40;34m3 \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mprint\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mPont des Arts:\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mpont_des_arts_length\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                              \u001b[0m │\n",
              "│ \u001b[1;38;2;227;227;221;48;2;39;40;34m  \u001b[0m\u001b[38;2;101;102;96;48;2;39;40;34m4 \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mprint\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mLeopard speed:\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mleopard_speed\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                     \u001b[0m │\n",
              "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─ <span style=\"font-weight: bold\">Executing this code:</span> ──────────────────────────────────────────────────────────────────────────────────────────╮\n",
              "│ <span style=\"color: #e3e3dd; text-decoration-color: #e3e3dd; background-color: #272822; font-weight: bold\">  </span><span style=\"color: #656660; text-decoration-color: #656660; background-color: #272822\">1 </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">pont_des_arts_length </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> web_search(query</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"Length of Pont des Arts\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">                                         </span> │\n",
              "│ <span style=\"color: #e3e3dd; text-decoration-color: #e3e3dd; background-color: #272822; font-weight: bold\">  </span><span style=\"color: #656660; text-decoration-color: #656660; background-color: #272822\">2 </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">leopard_speed </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> web_search(query</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"Top speed of leopard\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">                                                   </span> │\n",
              "│ <span style=\"color: #e3e3dd; text-decoration-color: #e3e3dd; background-color: #272822; font-weight: bold\">  </span><span style=\"color: #656660; text-decoration-color: #656660; background-color: #272822\">3 </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">print(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"Pont des Arts:\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">, pont_des_arts_length)</span><span style=\"background-color: #272822\">                                                              </span> │\n",
              "│ <span style=\"color: #e3e3dd; text-decoration-color: #e3e3dd; background-color: #272822; font-weight: bold\">  </span><span style=\"color: #656660; text-decoration-color: #656660; background-color: #272822\">4 </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">print(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"Leopard speed:\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">, leopard_speed)</span><span style=\"background-color: #272822\">                                                                     </span> │\n",
              "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mExecution logs:\u001b[0m\n",
              "Pont des Arts: ## Search Results\n",
              "\n",
              "[Pont des Arts - Wikipedia](https://en.wikipedia.org/wiki/Pont_des_Arts)\n",
              "The Pont des Arts (French pronunciation: [pɔ̃ dez‿aʁ]) or Passerelle des Arts ([pasʁɛl-]) is a pedestrian bridge in\n",
              "Paris which crosses the River Seine. It links the Institut de France and the central square ( cour carrée ) of the \n",
              "Palais du Louvre , (which had been termed the \"Palais des Arts\" under the First French Empire ).\n",
              "\n",
              "[10 Picturesque Facts About The Pont Des Arts](https://art-facts.com/pont-des-arts-facts/)\n",
              "Today, the number of arches has been reduced to just 7 in order to align it with the Pont Neuf, the oldest standing\n",
              "bridge in Paris, which is located just east of the Pont des Arts at the western tip of the \"Île de la Cité.\". Even \n",
              "though the bridge doesn't look that big at first sight, it still has a length of 155 meters (509 feet) and a width \n",
              "of 11 meters (36 feet).\n",
              "\n",
              "[Pont des Arts — Wikipédia](https://fr.wikipedia.org/wiki/Pont_des_Arts)\n",
              "Jean Béraud, Un jour de vent sur le pont des Arts (vers 1880-1881), Metropolitan Museum of Art . Jean Béraud, Un \n",
              "jour de vent sur le pont des Arts (vers 1880-1881), Metropolitan Museum of Art (New York). Victor Binet, Le Pont \n",
              "des arts, musée Alfred-Canel; Ferdinand Desnos, La Cène sur la Seine, musée national d'Art moderne (Paris).\n",
              "\n",
              "[Pont des Arts, Paris, France - LatLong](https://www.latlong.net/place/pont-des-arts-paris-france-32945.html)\n",
              "Pont des Arts, Paris, France. Latitude and longitude coordinates are: 48.858334, 2.337369. Pont des Arts is a \n",
              "scenic pedestrian bridge over the Seine River in Paris, France. The bridge connects the Musée du Louvre and the \n",
              "Institut de France. It has a length of 155 m (509 ft) and a width of 11 m (36 ft). It was constructed between 1981 \n",
              "and 1984.\n",
              "\n",
              "[The Unique Architecture of the Pont des Arts in \n",
              "Paris](https://www.parisartstravel.com/the-unique-architecture-of-the-pont-des-arts-in-paris/)\n",
              "The bridge is composed of nine arches and spans approximately 155 meters in length. Its iron structure, adorned \n",
              "with numerous decorative details, gives it an elegant and romantic appearance. ... The Pont des Arts was first \n",
              "opened in 1804; however, the current bridge that we see today is a reconstruction dating back to 1984. During the \n",
              "1980s ...\n",
              "\n",
              "[Exploring the History of the Pont des Arts in \n",
              "Paris](https://www.parisartstravel.com/exploring-the-history-of-the-pont-des-arts-in-paris/)\n",
              "This new bridge featured seven arches and a length of 155 meters. The Pont des Arts continued to evolve with the \n",
              "city's changing needs. In the 20th century, the bridge faced increased traffic demands, leading to rising concerns \n",
              "about its stability. As a result, it underwent another major renovation in 1984, modernizing the structure and ...\n",
              "\n",
              "[Pont des Arts - The Love Bridge](https://www.tourdefrance-bridges.com/post/pont-des-arts-the-love-bridge)\n",
              "Context:The Pont des Arts, originally a cast iron arch bridge, known more often as the \"Love Bridge,\" crosses the \n",
              "Seine river in Paris, connecting the Louvre to the Institut de Paris, formerly the Collèges des quatre Nations. ...\n",
              "The original bridge had 9 arches, was 155m in length and 11m wide and was made using cast iron. The number of ...\n",
              "\n",
              "[10 things to know about the Pont des Arts in Paris. - \n",
              "ParisCadenas.com](https://www.pariscadenas.com/articles/read/art-bridge)\n",
              "The Pont des Arts is the first wooden bridge to be built over the Seine in Paris. It was closed to the public in \n",
              "2015 due to the weight overload caused by the love locks. Since June 2015, the Pont des Arts has become a steel and\n",
              "glass bridge for pedestrians. The Pont des Arts is a popular walkway for Parisians and tourists.\n",
              "\n",
              "[Learn everything about the history of Pont des Arts in Paris](https://sainte-chapelle.co/pont-des-arts/)\n",
              "Built-in 1801 and 1804, Pont Des Arts, previously known as Passerelle des Arts, was the first iron bridge in Paris.\n",
              "Now a pedestrian bridge, it is used by locals and tourists to have a pleasant stroll with friends and family and \n",
              "get stunning views of the River Seine.\n",
              "\n",
              "[Pont des Arts Map - Bridge - Paris, Île-de-France, France - Mapcarta](https://mapcarta.com/29198828)\n",
              "The Pont Neuf is the oldest standing bridge across the river Seine in Paris, France. Pont Neuf is situated 330 \n",
              "metres east of Pont des Arts.\n",
              "Leopard speed: ## Search Results\n",
              "\n",
              "[How Fast Are Leopards? Top Speeds and How It Compares ... - AZ \n",
              "Animals](https://a-z-animals.com/blog/how-fast-leopards-top-speeds-how-compares-other-big-cats/)\n",
              "Cats of all sizes can reach impressive speeds. These are all attributes of their anatomy. Much like how the \n",
              "strength of an animal's bite is due to their jaw and skull anatomy, a cat's speed is determined by their \n",
              "musculature and skeletal composition. However, you can boil down their speed and agility to a very important \n",
              "structure: the spine.\n",
              "\n",
              "[16 Leopard Facts You Should Know - Fact Animal](https://factanimal.com/leopard/)\n",
              "The Leopard is the smallest of the big cat family, but are one of the most daring. They are known for their spots \n",
              "on their fur and live in a wide range of countries across Africa, Asia and Southern Russia. ... Top Speed: 58 km/h \n",
              "(36 mph) No. of Species: 1: Conservation Status:\n",
              "\n",
              "[Unraveling the Speed of Leopards: How Fast Can They Actually \n",
              "Run?](https://startingrun.com/unraveling-the-speed-of-leopards-how-fast-can-they-actually-run/)\n",
              "The leopard, famously fast, clocks an impressive top speed of 58 km/h (36 mph). But how does this fare against \n",
              "other members of the big cat family? The lion , larger and heavier, reaches a peak speed of 81 km/h (50 mph).\n",
              "\n",
              "[Leopard - Facts, Diet, Habitat & Pictures on Animalia.bio](https://animalia.bio/leopard)\n",
              "Basic facts about Leopard: lifespan, distribution and habitat map, lifestyle and social behavior, mating habits, \n",
              "diet and nutrition, population size and status. ... Top speed. 58. 36. km/h mph. km/h mph . Weight. 28-90. \n",
              "61.6-198. kg lbs. kg lbs . Height. 57-70. 22.4-27.6. cm inch. cm inch . Length. 90-190. ... Leopards are top \n",
              "predators of ...\n",
              "\n",
              "[How fast can leopards run and how agile are they? - \n",
              "WildLifeFAQs](https://wildlifefaq.com/leopard-speed-and-agility/)\n",
              "In this section, we will explore the leopard's running speed, top speed, and overall agility. Get ready to be \n",
              "amazed by these agile big cats! Leopards are known for their remarkable speed and agility. They can reach speeds of\n",
              "up to 58 kilometers per hour (36 mph) during short bursts of running. Their long, muscular legs allow them to cover\n",
              "...\n",
              "\n",
              "[How Fast Can a Leopard Run? It's Slower than Most of Its Prey](https://africafreak.com/how-fast-can-a-leopard-run)\n",
              "African leopards have a top speed of 56-60 km/h (35-37 mph). That's significantly slower than cheetahs and lions, \n",
              "as well as being slower than a lot of its prey. But this top speed isn't the full story. The leopard is one of the \n",
              "greatest predators on the African savannah and speed is just one piece of its hunting prowess.\n",
              "\n",
              "[Leopard's Peak Speed: Unleashing Their Running \n",
              "Power](https://startingrun.com/unleashing-the-speed-how-fast-can-a-leopard-run/)\n",
              "Top Speed (in mph) Leopard: 58: Lion: 50: Tiger: 40: Cheetah: 60-70: Clearly, leopards demonstrate remarkable \n",
              "speed, but their main advantage lies in their agility and adaptability.\n",
              "\n",
              "[Leopard Facts - Animal Facts Encyclopedia](https://www.animalfactsencyclopedia.com/Leopard-facts.html)\n",
              "The leopard's climbing ability sets it apart from the other big cats and has made it the most successful of the \n",
              "group as far as its numbers in the wild. ... TOP SPEED: GENDER DIFFERENCE: COLOR VARIATION: 36 mph: male 20% larger\n",
              "\"black panther\" is melanistic leopard - spots are faintly visible: ESTRUS: GESTATION: BIRTH LENGTH: BIRTH WEIGHT\n",
              "\n",
              "[Leopard - A-Z Animals](https://a-z-animals.com/animals/leopard/)\n",
              "Top Speed 36 mph Lifespan 10 - 15 years Weight 30kg - 90kg (66lbs - 198lbs) Length 100cm - 190cm (40in - 75in) Age \n",
              "of Sexual Maturity 2 - 2.5 years ... The leopard is a medium-sized wildcat that lives in a variety of different \n",
              "habitats across sub-Saharan Africa and southern Asia. Distinguished by their uniquely beautiful \"spotted\" coat ...\n",
              "\n",
              "[How Fast Can a Leopard Run? - \n",
              "Reference.com](https://www.reference.com/pets-animals/fast-can-leopard-run-deeb9e03f5aa98b5)\n",
              "A leopard can reach a top speed of 30 miles per hour. The fastest animal on Earth is said to be a cheetah, which \n",
              "can reach a top speed of 70 miles per hour. Leopards do not waste energy. Instead of chasing, the cats stalk and \n",
              "ambush prey. A leopard's prey consists primarily of deer, rodents and warthogs. Predators of the leopard include \n",
              "lions ...\n",
              "\n",
              "Out: None\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Execution logs:</span>\n",
              "Pont des Arts: ## Search Results\n",
              "\n",
              "[Pont des Arts - Wikipedia](https://en.wikipedia.org/wiki/Pont_des_Arts)\n",
              "The Pont des Arts (French pronunciation: [pɔ̃ dez‿aʁ]) or Passerelle des Arts ([pasʁɛl-]) is a pedestrian bridge in\n",
              "Paris which crosses the River Seine. It links the Institut de France and the central square ( cour carrée ) of the \n",
              "Palais du Louvre , (which had been termed the \"Palais des Arts\" under the First French Empire ).\n",
              "\n",
              "[10 Picturesque Facts About The Pont Des Arts](https://art-facts.com/pont-des-arts-facts/)\n",
              "Today, the number of arches has been reduced to just 7 in order to align it with the Pont Neuf, the oldest standing\n",
              "bridge in Paris, which is located just east of the Pont des Arts at the western tip of the \"Île de la Cité.\". Even \n",
              "though the bridge doesn't look that big at first sight, it still has a length of 155 meters (509 feet) and a width \n",
              "of 11 meters (36 feet).\n",
              "\n",
              "[Pont des Arts — Wikipédia](https://fr.wikipedia.org/wiki/Pont_des_Arts)\n",
              "Jean Béraud, Un jour de vent sur le pont des Arts (vers 1880-1881), Metropolitan Museum of Art . Jean Béraud, Un \n",
              "jour de vent sur le pont des Arts (vers 1880-1881), Metropolitan Museum of Art (New York). Victor Binet, Le Pont \n",
              "des arts, musée Alfred-Canel; Ferdinand Desnos, La Cène sur la Seine, musée national d'Art moderne (Paris).\n",
              "\n",
              "[Pont des Arts, Paris, France - LatLong](https://www.latlong.net/place/pont-des-arts-paris-france-32945.html)\n",
              "Pont des Arts, Paris, France. Latitude and longitude coordinates are: 48.858334, 2.337369. Pont des Arts is a \n",
              "scenic pedestrian bridge over the Seine River in Paris, France. The bridge connects the Musée du Louvre and the \n",
              "Institut de France. It has a length of 155 m (509 ft) and a width of 11 m (36 ft). It was constructed between 1981 \n",
              "and 1984.\n",
              "\n",
              "[The Unique Architecture of the Pont des Arts in \n",
              "Paris](https://www.parisartstravel.com/the-unique-architecture-of-the-pont-des-arts-in-paris/)\n",
              "The bridge is composed of nine arches and spans approximately 155 meters in length. Its iron structure, adorned \n",
              "with numerous decorative details, gives it an elegant and romantic appearance. ... The Pont des Arts was first \n",
              "opened in 1804; however, the current bridge that we see today is a reconstruction dating back to 1984. During the \n",
              "1980s ...\n",
              "\n",
              "[Exploring the History of the Pont des Arts in \n",
              "Paris](https://www.parisartstravel.com/exploring-the-history-of-the-pont-des-arts-in-paris/)\n",
              "This new bridge featured seven arches and a length of 155 meters. The Pont des Arts continued to evolve with the \n",
              "city's changing needs. In the 20th century, the bridge faced increased traffic demands, leading to rising concerns \n",
              "about its stability. As a result, it underwent another major renovation in 1984, modernizing the structure and ...\n",
              "\n",
              "[Pont des Arts - The Love Bridge](https://www.tourdefrance-bridges.com/post/pont-des-arts-the-love-bridge)\n",
              "Context:The Pont des Arts, originally a cast iron arch bridge, known more often as the \"Love Bridge,\" crosses the \n",
              "Seine river in Paris, connecting the Louvre to the Institut de Paris, formerly the Collèges des quatre Nations. ...\n",
              "The original bridge had 9 arches, was 155m in length and 11m wide and was made using cast iron. The number of ...\n",
              "\n",
              "[10 things to know about the Pont des Arts in Paris. - \n",
              "ParisCadenas.com](https://www.pariscadenas.com/articles/read/art-bridge)\n",
              "The Pont des Arts is the first wooden bridge to be built over the Seine in Paris. It was closed to the public in \n",
              "2015 due to the weight overload caused by the love locks. Since June 2015, the Pont des Arts has become a steel and\n",
              "glass bridge for pedestrians. The Pont des Arts is a popular walkway for Parisians and tourists.\n",
              "\n",
              "[Learn everything about the history of Pont des Arts in Paris](https://sainte-chapelle.co/pont-des-arts/)\n",
              "Built-in 1801 and 1804, Pont Des Arts, previously known as Passerelle des Arts, was the first iron bridge in Paris.\n",
              "Now a pedestrian bridge, it is used by locals and tourists to have a pleasant stroll with friends and family and \n",
              "get stunning views of the River Seine.\n",
              "\n",
              "[Pont des Arts Map - Bridge - Paris, Île-de-France, France - Mapcarta](https://mapcarta.com/29198828)\n",
              "The Pont Neuf is the oldest standing bridge across the river Seine in Paris, France. Pont Neuf is situated 330 \n",
              "metres east of Pont des Arts.\n",
              "Leopard speed: ## Search Results\n",
              "\n",
              "[How Fast Are Leopards? Top Speeds and How It Compares ... - AZ \n",
              "Animals](https://a-z-animals.com/blog/how-fast-leopards-top-speeds-how-compares-other-big-cats/)\n",
              "Cats of all sizes can reach impressive speeds. These are all attributes of their anatomy. Much like how the \n",
              "strength of an animal's bite is due to their jaw and skull anatomy, a cat's speed is determined by their \n",
              "musculature and skeletal composition. However, you can boil down their speed and agility to a very important \n",
              "structure: the spine.\n",
              "\n",
              "[16 Leopard Facts You Should Know - Fact Animal](https://factanimal.com/leopard/)\n",
              "The Leopard is the smallest of the big cat family, but are one of the most daring. They are known for their spots \n",
              "on their fur and live in a wide range of countries across Africa, Asia and Southern Russia. ... Top Speed: 58 km/h \n",
              "(36 mph) No. of Species: 1: Conservation Status:\n",
              "\n",
              "[Unraveling the Speed of Leopards: How Fast Can They Actually \n",
              "Run?](https://startingrun.com/unraveling-the-speed-of-leopards-how-fast-can-they-actually-run/)\n",
              "The leopard, famously fast, clocks an impressive top speed of 58 km/h (36 mph). But how does this fare against \n",
              "other members of the big cat family? The lion , larger and heavier, reaches a peak speed of 81 km/h (50 mph).\n",
              "\n",
              "[Leopard - Facts, Diet, Habitat &amp; Pictures on Animalia.bio](https://animalia.bio/leopard)\n",
              "Basic facts about Leopard: lifespan, distribution and habitat map, lifestyle and social behavior, mating habits, \n",
              "diet and nutrition, population size and status. ... Top speed. 58. 36. km/h mph. km/h mph . Weight. 28-90. \n",
              "61.6-198. kg lbs. kg lbs . Height. 57-70. 22.4-27.6. cm inch. cm inch . Length. 90-190. ... Leopards are top \n",
              "predators of ...\n",
              "\n",
              "[How fast can leopards run and how agile are they? - \n",
              "WildLifeFAQs](https://wildlifefaq.com/leopard-speed-and-agility/)\n",
              "In this section, we will explore the leopard's running speed, top speed, and overall agility. Get ready to be \n",
              "amazed by these agile big cats! Leopards are known for their remarkable speed and agility. They can reach speeds of\n",
              "up to 58 kilometers per hour (36 mph) during short bursts of running. Their long, muscular legs allow them to cover\n",
              "...\n",
              "\n",
              "[How Fast Can a Leopard Run? It's Slower than Most of Its Prey](https://africafreak.com/how-fast-can-a-leopard-run)\n",
              "African leopards have a top speed of 56-60 km/h (35-37 mph). That's significantly slower than cheetahs and lions, \n",
              "as well as being slower than a lot of its prey. But this top speed isn't the full story. The leopard is one of the \n",
              "greatest predators on the African savannah and speed is just one piece of its hunting prowess.\n",
              "\n",
              "[Leopard's Peak Speed: Unleashing Their Running \n",
              "Power](https://startingrun.com/unleashing-the-speed-how-fast-can-a-leopard-run/)\n",
              "Top Speed (in mph) Leopard: 58: Lion: 50: Tiger: 40: Cheetah: 60-70: Clearly, leopards demonstrate remarkable \n",
              "speed, but their main advantage lies in their agility and adaptability.\n",
              "\n",
              "[Leopard Facts - Animal Facts Encyclopedia](https://www.animalfactsencyclopedia.com/Leopard-facts.html)\n",
              "The leopard's climbing ability sets it apart from the other big cats and has made it the most successful of the \n",
              "group as far as its numbers in the wild. ... TOP SPEED: GENDER DIFFERENCE: COLOR VARIATION: 36 mph: male 20% larger\n",
              "\"black panther\" is melanistic leopard - spots are faintly visible: ESTRUS: GESTATION: BIRTH LENGTH: BIRTH WEIGHT\n",
              "\n",
              "[Leopard - A-Z Animals](https://a-z-animals.com/animals/leopard/)\n",
              "Top Speed 36 mph Lifespan 10 - 15 years Weight 30kg - 90kg (66lbs - 198lbs) Length 100cm - 190cm (40in - 75in) Age \n",
              "of Sexual Maturity 2 - 2.5 years ... The leopard is a medium-sized wildcat that lives in a variety of different \n",
              "habitats across sub-Saharan Africa and southern Asia. Distinguished by their uniquely beautiful \"spotted\" coat ...\n",
              "\n",
              "[How Fast Can a Leopard Run? - \n",
              "Reference.com](https://www.reference.com/pets-animals/fast-can-leopard-run-deeb9e03f5aa98b5)\n",
              "A leopard can reach a top speed of 30 miles per hour. The fastest animal on Earth is said to be a cheetah, which \n",
              "can reach a top speed of 70 miles per hour. Leopards do not waste energy. Instead of chasing, the cats stalk and \n",
              "ambush prey. A leopard's prey consists primarily of deer, rodents and warthogs. Predators of the leopard include \n",
              "lions ...\n",
              "\n",
              "Out: None\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2m[Step 0: Duration 3.61 seconds| Input tokens: 2,076 | Output tokens: 109]\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 0: Duration 3.61 seconds| Input tokens: 2,076 | Output tokens: 109]</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "╭─ \u001b[1mExecuting this code:\u001b[0m ──────────────────────────────────────────────────────────────────────────────────────────╮\n",
              "│ \u001b[1;38;2;227;227;221;48;2;39;40;34m  \u001b[0m\u001b[38;2;101;102;96;48;2;39;40;34m1 \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mpont_des_arts_length_meters\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m155\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m  \u001b[0m\u001b[38;2;149;144;119;48;2;39;40;34m# length of the bridge in meters\u001b[0m\u001b[48;2;39;40;34m                                        \u001b[0m │\n",
              "│ \u001b[1;38;2;227;227;221;48;2;39;40;34m  \u001b[0m\u001b[38;2;101;102;96;48;2;39;40;34m2 \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mleopard_speed_m_per_s\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m58\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m/\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m3.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m6\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m  \u001b[0m\u001b[38;2;149;144;119;48;2;39;40;34m# leopard's top speed in meters per second\u001b[0m\u001b[48;2;39;40;34m                              \u001b[0m │\n",
              "│ \u001b[1;38;2;227;227;221;48;2;39;40;34m  \u001b[0m\u001b[38;2;101;102;96;48;2;39;40;34m3 \u001b[0m\u001b[48;2;39;40;34m                                                                                                           \u001b[0m │\n",
              "│ \u001b[1;38;2;227;227;221;48;2;39;40;34m  \u001b[0m\u001b[38;2;101;102;96;48;2;39;40;34m4 \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mtime_to_run\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mpont_des_arts_length_meters\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m/\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mleopard_speed_m_per_s\u001b[0m\u001b[48;2;39;40;34m                                          \u001b[0m │\n",
              "│ \u001b[1;38;2;227;227;221;48;2;39;40;34m  \u001b[0m\u001b[38;2;101;102;96;48;2;39;40;34m5 \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mfinal_answer\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mtime_to_run\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                  \u001b[0m │\n",
              "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─ <span style=\"font-weight: bold\">Executing this code:</span> ──────────────────────────────────────────────────────────────────────────────────────────╮\n",
              "│ <span style=\"color: #e3e3dd; text-decoration-color: #e3e3dd; background-color: #272822; font-weight: bold\">  </span><span style=\"color: #656660; text-decoration-color: #656660; background-color: #272822\">1 </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">pont_des_arts_length_meters </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> </span><span style=\"color: #ae81ff; text-decoration-color: #ae81ff; background-color: #272822\">155</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">  </span><span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># length of the bridge in meters</span><span style=\"background-color: #272822\">                                        </span> │\n",
              "│ <span style=\"color: #e3e3dd; text-decoration-color: #e3e3dd; background-color: #272822; font-weight: bold\">  </span><span style=\"color: #656660; text-decoration-color: #656660; background-color: #272822\">2 </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">leopard_speed_m_per_s </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> </span><span style=\"color: #ae81ff; text-decoration-color: #ae81ff; background-color: #272822\">58</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">/</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> </span><span style=\"color: #ae81ff; text-decoration-color: #ae81ff; background-color: #272822\">3.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> </span><span style=\"color: #ae81ff; text-decoration-color: #ae81ff; background-color: #272822\">6</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">  </span><span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># leopard's top speed in meters per second</span><span style=\"background-color: #272822\">                              </span> │\n",
              "│ <span style=\"color: #e3e3dd; text-decoration-color: #e3e3dd; background-color: #272822; font-weight: bold\">  </span><span style=\"color: #656660; text-decoration-color: #656660; background-color: #272822\">3 </span><span style=\"background-color: #272822\">                                                                                                           </span> │\n",
              "│ <span style=\"color: #e3e3dd; text-decoration-color: #e3e3dd; background-color: #272822; font-weight: bold\">  </span><span style=\"color: #656660; text-decoration-color: #656660; background-color: #272822\">4 </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">time_to_run </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> pont_des_arts_length_meters </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">/</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> leopard_speed_m_per_s</span><span style=\"background-color: #272822\">                                          </span> │\n",
              "│ <span style=\"color: #e3e3dd; text-decoration-color: #e3e3dd; background-color: #272822; font-weight: bold\">  </span><span style=\"color: #656660; text-decoration-color: #656660; background-color: #272822\">5 </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">final_answer(time_to_run)</span><span style=\"background-color: #272822\">                                                                                  </span> │\n",
              "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;31mCode execution failed on line \u001b[0m\u001b[1;31m2\u001b[0m\u001b[1;31m due to: SyntaxError\u001b[0m\n",
              "\u001b[1;31mleopard_speed_m_per_s = \u001b[0m\u001b[1;31m58\u001b[0m\u001b[1;31m \u001b[0m\u001b[1;31m/\u001b[0m\u001b[1;31m \u001b[0m\u001b[1;31m3\u001b[0m\u001b[1;31m. \u001b[0m\u001b[1;31m6\u001b[0m\u001b[1;31m  # leopard's top speed in meters per second\u001b[0m\n",
              "\u001b[1;31m                                 ^\u001b[0m\n",
              "\u001b[1;31mError: invalid syntax \u001b[0m\u001b[1;31m(\u001b[0m\u001b[1;31m<\u001b[0m\u001b[1;31munknown\u001b[0m\u001b[1;31m>\u001b[0m\u001b[1;31m, line \u001b[0m\u001b[1;31m2\u001b[0m\u001b[1;31m)\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Code execution failed on line </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">2</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\"> due to: SyntaxError</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">leopard_speed_m_per_s = </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">58</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\"> / </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">3</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">. </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">6</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">  # leopard's top speed in meters per second</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">                                 ^</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Error: invalid syntax (&lt;unknown&gt;, line </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">2</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">)</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2m[Step 1: Duration 5.23 seconds| Input tokens: 6,672 | Output tokens: 265]\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 5.23 seconds| Input tokens: 6,672 | Output tokens: 265]</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m2\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "╭─ \u001b[1mExecuting this code:\u001b[0m ──────────────────────────────────────────────────────────────────────────────────────────╮\n",
              "│ \u001b[1;38;2;227;227;221;48;2;39;40;34m  \u001b[0m\u001b[38;2;101;102;96;48;2;39;40;34m1 \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mpont_des_arts_length_meters\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m155\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m  \u001b[0m\u001b[38;2;149;144;119;48;2;39;40;34m# length of the bridge in meters\u001b[0m\u001b[48;2;39;40;34m                                        \u001b[0m │\n",
              "│ \u001b[1;38;2;227;227;221;48;2;39;40;34m  \u001b[0m\u001b[38;2;101;102;96;48;2;39;40;34m2 \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mleopard_speed_m_per_s\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m58\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m/\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m3.6\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m   \u001b[0m\u001b[38;2;149;144;119;48;2;39;40;34m# leopard's top speed in meters per second\u001b[0m\u001b[48;2;39;40;34m                              \u001b[0m │\n",
              "│ \u001b[1;38;2;227;227;221;48;2;39;40;34m  \u001b[0m\u001b[38;2;101;102;96;48;2;39;40;34m3 \u001b[0m\u001b[48;2;39;40;34m                                                                                                           \u001b[0m │\n",
              "│ \u001b[1;38;2;227;227;221;48;2;39;40;34m  \u001b[0m\u001b[38;2;101;102;96;48;2;39;40;34m4 \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mtime_to_run\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mpont_des_arts_length_meters\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m/\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mleopard_speed_m_per_s\u001b[0m\u001b[48;2;39;40;34m                                          \u001b[0m │\n",
              "│ \u001b[1;38;2;227;227;221;48;2;39;40;34m  \u001b[0m\u001b[38;2;101;102;96;48;2;39;40;34m5 \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mfinal_answer\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mtime_to_run\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                  \u001b[0m │\n",
              "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─ <span style=\"font-weight: bold\">Executing this code:</span> ──────────────────────────────────────────────────────────────────────────────────────────╮\n",
              "│ <span style=\"color: #e3e3dd; text-decoration-color: #e3e3dd; background-color: #272822; font-weight: bold\">  </span><span style=\"color: #656660; text-decoration-color: #656660; background-color: #272822\">1 </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">pont_des_arts_length_meters </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> </span><span style=\"color: #ae81ff; text-decoration-color: #ae81ff; background-color: #272822\">155</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">  </span><span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># length of the bridge in meters</span><span style=\"background-color: #272822\">                                        </span> │\n",
              "│ <span style=\"color: #e3e3dd; text-decoration-color: #e3e3dd; background-color: #272822; font-weight: bold\">  </span><span style=\"color: #656660; text-decoration-color: #656660; background-color: #272822\">2 </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">leopard_speed_m_per_s </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> </span><span style=\"color: #ae81ff; text-decoration-color: #ae81ff; background-color: #272822\">58</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">/</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> </span><span style=\"color: #ae81ff; text-decoration-color: #ae81ff; background-color: #272822\">3.6</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">   </span><span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># leopard's top speed in meters per second</span><span style=\"background-color: #272822\">                              </span> │\n",
              "│ <span style=\"color: #e3e3dd; text-decoration-color: #e3e3dd; background-color: #272822; font-weight: bold\">  </span><span style=\"color: #656660; text-decoration-color: #656660; background-color: #272822\">3 </span><span style=\"background-color: #272822\">                                                                                                           </span> │\n",
              "│ <span style=\"color: #e3e3dd; text-decoration-color: #e3e3dd; background-color: #272822; font-weight: bold\">  </span><span style=\"color: #656660; text-decoration-color: #656660; background-color: #272822\">4 </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">time_to_run </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> pont_des_arts_length_meters </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">/</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> leopard_speed_m_per_s</span><span style=\"background-color: #272822\">                                          </span> │\n",
              "│ <span style=\"color: #e3e3dd; text-decoration-color: #e3e3dd; background-color: #272822; font-weight: bold\">  </span><span style=\"color: #656660; text-decoration-color: #656660; background-color: #272822\">5 </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">final_answer(time_to_run)</span><span style=\"background-color: #272822\">                                                                                  </span> │\n",
              "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;2;212;183;2mOut - Final answer: 9.620689655172415\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Out - Final answer: 9.620689655172415</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2m[Step 2: Duration 4.99 seconds| Input tokens: 11,630 | Output tokens: 389]\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 2: Duration 4.99 seconds| Input tokens: 11,630 | Output tokens: 389]</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9.620689655172415"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent.run(\"How much is MODI better than KCR in terms developement?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Ov0Kpwoi1vSs",
        "outputId": "616800cc-d1fb-4fa4-a485-95b0a3368653"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[38;2;212;183;2m╭─\u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╮\u001b[0m\n",
              "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
              "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mHow much is MODI better than KCR in terms developement?\u001b[0m                                                         \u001b[38;2;212;183;2m│\u001b[0m\n",
              "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
              "\u001b[38;2;212;183;2m╰─\u001b[0m\u001b[38;2;212;183;2m HfApiModel - Qwen/Qwen2.5-Coder-32B-Instruct \u001b[0m\u001b[38;2;212;183;2m─────────────────────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╯\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">╭──────────────────────────────────────────────────── </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ────────────────────────────────────────────────────╮</span>\n",
              "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
              "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">How much is MODI better than KCR in terms developement?</span>                                                         <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
              "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
              "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">╰─ HfApiModel - Qwen/Qwen2.5-Coder-32B-Instruct ──────────────────────────────────────────────────────────────────╯</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m0\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "╭─ \u001b[1mExecuting this code:\u001b[0m ──────────────────────────────────────────────────────────────────────────────────────────╮\n",
              "│ \u001b[1;38;2;227;227;221;48;2;39;40;34m  \u001b[0m\u001b[38;2;101;102;96;48;2;39;40;34m1 \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mresults\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mweb_search\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mquery\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mModi vs KCR development\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                      \u001b[0m │\n",
              "│ \u001b[1;38;2;227;227;221;48;2;39;40;34m  \u001b[0m\u001b[38;2;101;102;96;48;2;39;40;34m2 \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mprint\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mresults\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                             \u001b[0m │\n",
              "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─ <span style=\"font-weight: bold\">Executing this code:</span> ──────────────────────────────────────────────────────────────────────────────────────────╮\n",
              "│ <span style=\"color: #e3e3dd; text-decoration-color: #e3e3dd; background-color: #272822; font-weight: bold\">  </span><span style=\"color: #656660; text-decoration-color: #656660; background-color: #272822\">1 </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">results </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> web_search(query</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"Modi vs KCR development\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">                                                      </span> │\n",
              "│ <span style=\"color: #e3e3dd; text-decoration-color: #e3e3dd; background-color: #272822; font-weight: bold\">  </span><span style=\"color: #656660; text-decoration-color: #656660; background-color: #272822\">2 </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">print(results)</span><span style=\"background-color: #272822\">                                                                                             </span> │\n",
              "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mExecution logs:\u001b[0m\n",
              "## Search Results\n",
              "\n",
              "[As PM Modi turns up the heat on KCR, the long and short of BJP's \n",
              "...](https://indianexpress.com/article/political-pulse/as-pm-modi-turns-up-the-heat-on-kcr-the-long-and-short-of-bj\n",
              "ps-strategy-9044708/)\n",
              "In October, Modi said KCR wanted to join the BJP-led National Democratic Alliance (NDA) in December 2020 after the \n",
              "BJP's impressive performance in the Hyderabad civic polls, a claim that the Telangana CM denied. The PM repeated \n",
              "the allegation at the rally on Monday. \"KCR realised the increasing power of BJP much earlier.\n",
              "\n",
              "[Opinion: How is KCR different from Modi? - The Siasat \n",
              "Daily](https://www.siasat.com/opinion-how-is-kcr-different-from-modi-2370733/)\n",
              "KCR has accused Modi of destabilising the opposition parties and toppling the opposition-led government in the \n",
              "states. Well, what Modi and his team are doing is highly reprehensible, the TRS president, too, has to see the \n",
              "accusing finger pointing toward him. \"Like Narendra Modi, the Telangana chief minister, too, doesn't respect the \n",
              "...\n",
              "\n",
              "[Progress stalled under KCR, thrived under Modi: Rajnath - Times of \n",
              "India](https://timesofindia.indiatimes.com/city/hyderabad/progress-stalled-under-kcr/articleshow/105486657.cms)\n",
              "Defence minister Rajnath Singh compares the progress under Prime Minister Narendra Modi's leadership with the \n",
              "stalled growth in Telangana under KCR's governance. Learn more about the impact of BJP ...\n",
              "\n",
              "[On KCR's 'Modi good friend' remark, BJP says 'part of his \n",
              "conspiracy'](https://www.hindustantimes.com/india-news/on-kcrs-modi-good-friend-remark-bjp-says-part-of-his-conspir\n",
              "acy-101686964856326.html)\n",
              "\"Telangana is totally destroyed under the rule of KCR, Development is possible only with the Double Engine \n",
              "government of BJP,\" he added. On Thursday, Rao, popularly known as 'KCR', called PM ...\n",
              "\n",
              "[KCR vs Modi | Modi Is Mad About Privatization, And I Stand For The \n",
              "...](https://telanganatoday.com/kcr-vs-modi-modi-is-mad-about-privatization-and-i-stand-for-the-public-kcr)\n",
              "Prime Minister Narendra Modi warned me to add meters to the water supply for farmers, but I risked my life and \n",
              "stood against him for the welfare of Telangana farmers, says KCR at a public meeting in the Telangana elections \n",
              "campaign 2023.\n",
              "\n",
              "[KCR Vs Modi-Are the battle lines drawn for 2023? - The Siasat \n",
              "Daily](https://www.siasat.com/kcr-vs-modi-are-the-battle-lines-drawn-for-2023-2367458/)\n",
              "Though, KCR started his political moves to build a coalition of opposition parties against the NDA Government led \n",
              "by Modi a few months ago, his criticism of the BJP attained 'sharp and direct' tone on the PM and the Union \n",
              "government's let down of Telangana State on the eve of the BJP National Executive on July 2-3 in Hyderabad.\n",
              "\n",
              "[\"No Cooperation From State Government\": PM Attacks KCR In \n",
              "Hyderabad](https://www.ndtv.com/india-news/no-cooperation-from-state-government-so-development-suffers-pm-modi-in-t\n",
              "elanganas-hyderabad-3930837)\n",
              "Prime Minister Narendra Modi launched a scathing attack on Telangana's ruling Bharat Rashtra Samithi (BRS) and its \n",
              "chief K Chandrashekar Rao, popularly known as KCR, on Saturday, accusing them of ...\n",
              "\n",
              "[Telangana Chief Minister KCR To Skip PM Modi's Programme In The State \n",
              "Today](https://www.ndtv.com/india-news/telangana-chief-minister-kcr-to-skip-pm-modis-programme-in-the-state-today-3\n",
              "930686)\n",
              "In a major thrust to infrastructure and connectivity, Prime Minister Narendra Modi will inaugurate and lay the \n",
              "foundation stone of development projects worth over Rs 11,300 crore in Telangana on ...\n",
              "\n",
              "[Modi Vs KCR - What s Cooking Out now? - India Herald](https://www.indiaherald.com/Breaking/Read/994515001/)\n",
              "breaking : In his speech, Modi asserted that since the BJP won four seats in the 2019 Lok Sabha elections, \n",
              "Telangana residents support for the party\n",
              "\n",
              "[Kcr Vs Modi: Latest News, Videos and Photos of Kcr Vs Modi - Times of \n",
              "India](https://timesofindia.indiatimes.com/topic/kcr-vs-modi)\n",
              "kcr vs modi News: Latest and Breaking News on kcr vs modi. Explore kcr vs modi profile at Times of India for \n",
              "photos, videos and latest news of kcr vs modi. Also find news, photos and videos on kcr ...\n",
              "\n",
              "Out: None\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Execution logs:</span>\n",
              "## Search Results\n",
              "\n",
              "[As PM Modi turns up the heat on KCR, the long and short of BJP's \n",
              "...](https://indianexpress.com/article/political-pulse/as-pm-modi-turns-up-the-heat-on-kcr-the-long-and-short-of-bj\n",
              "ps-strategy-9044708/)\n",
              "In October, Modi said KCR wanted to join the BJP-led National Democratic Alliance (NDA) in December 2020 after the \n",
              "BJP's impressive performance in the Hyderabad civic polls, a claim that the Telangana CM denied. The PM repeated \n",
              "the allegation at the rally on Monday. \"KCR realised the increasing power of BJP much earlier.\n",
              "\n",
              "[Opinion: How is KCR different from Modi? - The Siasat \n",
              "Daily](https://www.siasat.com/opinion-how-is-kcr-different-from-modi-2370733/)\n",
              "KCR has accused Modi of destabilising the opposition parties and toppling the opposition-led government in the \n",
              "states. Well, what Modi and his team are doing is highly reprehensible, the TRS president, too, has to see the \n",
              "accusing finger pointing toward him. \"Like Narendra Modi, the Telangana chief minister, too, doesn't respect the \n",
              "...\n",
              "\n",
              "[Progress stalled under KCR, thrived under Modi: Rajnath - Times of \n",
              "India](https://timesofindia.indiatimes.com/city/hyderabad/progress-stalled-under-kcr/articleshow/105486657.cms)\n",
              "Defence minister Rajnath Singh compares the progress under Prime Minister Narendra Modi's leadership with the \n",
              "stalled growth in Telangana under KCR's governance. Learn more about the impact of BJP ...\n",
              "\n",
              "[On KCR's 'Modi good friend' remark, BJP says 'part of his \n",
              "conspiracy'](https://www.hindustantimes.com/india-news/on-kcrs-modi-good-friend-remark-bjp-says-part-of-his-conspir\n",
              "acy-101686964856326.html)\n",
              "\"Telangana is totally destroyed under the rule of KCR, Development is possible only with the Double Engine \n",
              "government of BJP,\" he added. On Thursday, Rao, popularly known as 'KCR', called PM ...\n",
              "\n",
              "[KCR vs Modi | Modi Is Mad About Privatization, And I Stand For The \n",
              "...](https://telanganatoday.com/kcr-vs-modi-modi-is-mad-about-privatization-and-i-stand-for-the-public-kcr)\n",
              "Prime Minister Narendra Modi warned me to add meters to the water supply for farmers, but I risked my life and \n",
              "stood against him for the welfare of Telangana farmers, says KCR at a public meeting in the Telangana elections \n",
              "campaign 2023.\n",
              "\n",
              "[KCR Vs Modi-Are the battle lines drawn for 2023? - The Siasat \n",
              "Daily](https://www.siasat.com/kcr-vs-modi-are-the-battle-lines-drawn-for-2023-2367458/)\n",
              "Though, KCR started his political moves to build a coalition of opposition parties against the NDA Government led \n",
              "by Modi a few months ago, his criticism of the BJP attained 'sharp and direct' tone on the PM and the Union \n",
              "government's let down of Telangana State on the eve of the BJP National Executive on July 2-3 in Hyderabad.\n",
              "\n",
              "[\"No Cooperation From State Government\": PM Attacks KCR In \n",
              "Hyderabad](https://www.ndtv.com/india-news/no-cooperation-from-state-government-so-development-suffers-pm-modi-in-t\n",
              "elanganas-hyderabad-3930837)\n",
              "Prime Minister Narendra Modi launched a scathing attack on Telangana's ruling Bharat Rashtra Samithi (BRS) and its \n",
              "chief K Chandrashekar Rao, popularly known as KCR, on Saturday, accusing them of ...\n",
              "\n",
              "[Telangana Chief Minister KCR To Skip PM Modi's Programme In The State \n",
              "Today](https://www.ndtv.com/india-news/telangana-chief-minister-kcr-to-skip-pm-modis-programme-in-the-state-today-3\n",
              "930686)\n",
              "In a major thrust to infrastructure and connectivity, Prime Minister Narendra Modi will inaugurate and lay the \n",
              "foundation stone of development projects worth over Rs 11,300 crore in Telangana on ...\n",
              "\n",
              "[Modi Vs KCR - What s Cooking Out now? - India Herald](https://www.indiaherald.com/Breaking/Read/994515001/)\n",
              "breaking : In his speech, Modi asserted that since the BJP won four seats in the 2019 Lok Sabha elections, \n",
              "Telangana residents support for the party\n",
              "\n",
              "[Kcr Vs Modi: Latest News, Videos and Photos of Kcr Vs Modi - Times of \n",
              "India](https://timesofindia.indiatimes.com/topic/kcr-vs-modi)\n",
              "kcr vs modi News: Latest and Breaking News on kcr vs modi. Explore kcr vs modi profile at Times of India for \n",
              "photos, videos and latest news of kcr vs modi. Also find news, photos and videos on kcr ...\n",
              "\n",
              "Out: None\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2m[Step 0: Duration 3.47 seconds| Input tokens: 2,071 | Output tokens: 74]\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 0: Duration 3.47 seconds| Input tokens: 2,071 | Output tokens: 74]</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "╭─ \u001b[1mExecuting this code:\u001b[0m ──────────────────────────────────────────────────────────────────────────────────────────╮\n",
              "│ \u001b[1;38;2;227;227;221;48;2;39;40;34m  \u001b[0m\u001b[38;2;101;102;96;48;2;39;40;34m1 \u001b[0m\u001b[38;2;149;144;119;48;2;39;40;34m# Extracting relevant information from the search results\u001b[0m\u001b[48;2;39;40;34m                                                  \u001b[0m │\n",
              "│ \u001b[1;38;2;227;227;221;48;2;39;40;34m  \u001b[0m\u001b[38;2;101;102;96;48;2;39;40;34m2 \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mdevelopment_comparison\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m[\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m]\u001b[0m\u001b[48;2;39;40;34m                                                                                \u001b[0m │\n",
              "│ \u001b[1;38;2;227;227;221;48;2;39;40;34m  \u001b[0m\u001b[38;2;101;102;96;48;2;39;40;34m3 \u001b[0m\u001b[38;2;102;217;239;48;2;39;40;34mfor\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mresult\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34min\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mresults\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m:\u001b[0m\u001b[48;2;39;40;34m                                                                                     \u001b[0m │\n",
              "│ \u001b[1;38;2;227;227;221;48;2;39;40;34m  \u001b[0m\u001b[38;2;101;102;96;48;2;39;40;34m4 \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m    \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mbody\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mresult\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mget\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m'\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mbody\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m'\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m'\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m'\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                          \u001b[0m │\n",
              "│ \u001b[1;38;2;227;227;221;48;2;39;40;34m  \u001b[0m\u001b[38;2;101;102;96;48;2;39;40;34m5 \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m    \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mdevelopment_words\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m[\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m'\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mdevelopment\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m'\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m'\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mprogress\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m'\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m'\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34minfrastructure\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m'\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m'\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mgrowth\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m'\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m]\u001b[0m\u001b[48;2;39;40;34m                            \u001b[0m │\n",
              "│ \u001b[1;38;2;227;227;221;48;2;39;40;34m  \u001b[0m\u001b[38;2;101;102;96;48;2;39;40;34m6 \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m    \u001b[0m\u001b[38;2;102;217;239;48;2;39;40;34mif\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34many\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mword\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34min\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mbody\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mlower\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;102;217;239;48;2;39;40;34mfor\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mword\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34min\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mdevelopment_words\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m:\u001b[0m\u001b[48;2;39;40;34m                                            \u001b[0m │\n",
              "│ \u001b[1;38;2;227;227;221;48;2;39;40;34m  \u001b[0m\u001b[38;2;101;102;96;48;2;39;40;34m7 \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m        \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mdevelopment_comparison\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mappend\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mresult\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m[\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m'\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mtitle\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m'\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m]\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mbody\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                             \u001b[0m │\n",
              "│ \u001b[1;38;2;227;227;221;48;2;39;40;34m  \u001b[0m\u001b[38;2;101;102;96;48;2;39;40;34m8 \u001b[0m\u001b[48;2;39;40;34m                                                                                                           \u001b[0m │\n",
              "│ \u001b[1;38;2;227;227;221;48;2;39;40;34m  \u001b[0m\u001b[38;2;101;102;96;48;2;39;40;34m9 \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mprint\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mdevelopment_comparison\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                              \u001b[0m │\n",
              "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─ <span style=\"font-weight: bold\">Executing this code:</span> ──────────────────────────────────────────────────────────────────────────────────────────╮\n",
              "│ <span style=\"color: #e3e3dd; text-decoration-color: #e3e3dd; background-color: #272822; font-weight: bold\">  </span><span style=\"color: #656660; text-decoration-color: #656660; background-color: #272822\">1 </span><span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># Extracting relevant information from the search results</span><span style=\"background-color: #272822\">                                                  </span> │\n",
              "│ <span style=\"color: #e3e3dd; text-decoration-color: #e3e3dd; background-color: #272822; font-weight: bold\">  </span><span style=\"color: #656660; text-decoration-color: #656660; background-color: #272822\">2 </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">development_comparison </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> []</span><span style=\"background-color: #272822\">                                                                                </span> │\n",
              "│ <span style=\"color: #e3e3dd; text-decoration-color: #e3e3dd; background-color: #272822; font-weight: bold\">  </span><span style=\"color: #656660; text-decoration-color: #656660; background-color: #272822\">3 </span><span style=\"color: #66d9ef; text-decoration-color: #66d9ef; background-color: #272822\">for</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> result </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">in</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> results:</span><span style=\"background-color: #272822\">                                                                                     </span> │\n",
              "│ <span style=\"color: #e3e3dd; text-decoration-color: #e3e3dd; background-color: #272822; font-weight: bold\">  </span><span style=\"color: #656660; text-decoration-color: #656660; background-color: #272822\">4 </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    body </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> result</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">get(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">'body'</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">, </span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">''</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">                                                                          </span> │\n",
              "│ <span style=\"color: #e3e3dd; text-decoration-color: #e3e3dd; background-color: #272822; font-weight: bold\">  </span><span style=\"color: #656660; text-decoration-color: #656660; background-color: #272822\">5 </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    development_words </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> [</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">'development'</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">, </span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">'progress'</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">, </span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">'infrastructure'</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">, </span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">'growth'</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">]</span><span style=\"background-color: #272822\">                            </span> │\n",
              "│ <span style=\"color: #e3e3dd; text-decoration-color: #e3e3dd; background-color: #272822; font-weight: bold\">  </span><span style=\"color: #656660; text-decoration-color: #656660; background-color: #272822\">6 </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    </span><span style=\"color: #66d9ef; text-decoration-color: #66d9ef; background-color: #272822\">if</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> any(word </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">in</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> body</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">lower() </span><span style=\"color: #66d9ef; text-decoration-color: #66d9ef; background-color: #272822\">for</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> word </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">in</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> development_words):</span><span style=\"background-color: #272822\">                                            </span> │\n",
              "│ <span style=\"color: #e3e3dd; text-decoration-color: #e3e3dd; background-color: #272822; font-weight: bold\">  </span><span style=\"color: #656660; text-decoration-color: #656660; background-color: #272822\">7 </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">        development_comparison</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">append((result[</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">'title'</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">], body))</span><span style=\"background-color: #272822\">                                             </span> │\n",
              "│ <span style=\"color: #e3e3dd; text-decoration-color: #e3e3dd; background-color: #272822; font-weight: bold\">  </span><span style=\"color: #656660; text-decoration-color: #656660; background-color: #272822\">8 </span><span style=\"background-color: #272822\">                                                                                                           </span> │\n",
              "│ <span style=\"color: #e3e3dd; text-decoration-color: #e3e3dd; background-color: #272822; font-weight: bold\">  </span><span style=\"color: #656660; text-decoration-color: #656660; background-color: #272822\">9 </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">print(development_comparison)</span><span style=\"background-color: #272822\">                                                                              </span> │\n",
              "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;31mCode execution failed: Code execution failed at line 'for result in results:\u001b[0m\n",
              "\u001b[1;31m    body = \u001b[0m\u001b[1;31mresult.get\u001b[0m\u001b[1;31m(\u001b[0m\u001b[1;31m'body'\u001b[0m\u001b[1;31m, \u001b[0m\u001b[1;31m''\u001b[0m\u001b[1;31m)\u001b[0m\n",
              "\u001b[1;31m    development_words = \u001b[0m\u001b[1;31m[\u001b[0m\u001b[1;31m'development'\u001b[0m\u001b[1;31m, \u001b[0m\u001b[1;31m'progress'\u001b[0m\u001b[1;31m, \u001b[0m\u001b[1;31m'infrastructure'\u001b[0m\u001b[1;31m, \u001b[0m\u001b[1;31m'growth'\u001b[0m\u001b[1;31m]\u001b[0m\n",
              "\u001b[1;31m    if \u001b[0m\u001b[1;31many\u001b[0m\u001b[1;31m(\u001b[0m\u001b[1;31mword in \u001b[0m\u001b[1;31mbody.lower\u001b[0m\u001b[1;31m(\u001b[0m\u001b[1;31m)\u001b[0m\u001b[1;31m for word in development_words\u001b[0m\u001b[1;31m)\u001b[0m\u001b[1;31m:\u001b[0m\n",
              "\u001b[1;31m        \u001b[0m\u001b[1;31mdevelopment_comparison.append\u001b[0m\u001b[1;31m(\u001b[0m\u001b[1;31m(\u001b[0m\u001b[1;31mresult\u001b[0m\u001b[1;31m[\u001b[0m\u001b[1;31m'title'\u001b[0m\u001b[1;31m]\u001b[0m\u001b[1;31m, body\u001b[0m\u001b[1;31m)\u001b[0m\u001b[1;31m)\u001b[0m\u001b[1;31m' because of the following error:\u001b[0m\n",
              "\u001b[1;31mObject # has no attribute get\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Code execution failed: Code execution failed at line 'for result in results:</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">    body = result.get(</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">'body'</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">, </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">''</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">)</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">    development_words = [</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">'development'</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">, </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">'progress'</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">, </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">'infrastructure'</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">, </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">'growth'</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">]</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">    if any(word in body.lower() for word in development_words):</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">        development_comparison.append((result[</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">'title'</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">], body))' because of the following error:</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Object # has no attribute get</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2m[Step 1: Duration 4.58 seconds| Input tokens: 5,381 | Output tokens: 228]\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 4.58 seconds| Input tokens: 5,381 | Output tokens: 228]</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m2\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "╭─ \u001b[1mExecuting this code:\u001b[0m ──────────────────────────────────────────────────────────────────────────────────────────╮\n",
              "│ \u001b[1;38;2;227;227;221;48;2;39;40;34m  \u001b[0m\u001b[38;2;101;102;96;48;2;39;40;34m1 \u001b[0m\u001b[38;2;149;144;119;48;2;39;40;34m# Print the first few results to understand their format\u001b[0m\u001b[48;2;39;40;34m                                                   \u001b[0m │\n",
              "│ \u001b[1;38;2;227;227;221;48;2;39;40;34m  \u001b[0m\u001b[38;2;101;102;96;48;2;39;40;34m2 \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mprint\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mresults\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m[\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m:\u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m5\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m]\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                         \u001b[0m │\n",
              "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─ <span style=\"font-weight: bold\">Executing this code:</span> ──────────────────────────────────────────────────────────────────────────────────────────╮\n",
              "│ <span style=\"color: #e3e3dd; text-decoration-color: #e3e3dd; background-color: #272822; font-weight: bold\">  </span><span style=\"color: #656660; text-decoration-color: #656660; background-color: #272822\">1 </span><span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># Print the first few results to understand their format</span><span style=\"background-color: #272822\">                                                   </span> │\n",
              "│ <span style=\"color: #e3e3dd; text-decoration-color: #e3e3dd; background-color: #272822; font-weight: bold\">  </span><span style=\"color: #656660; text-decoration-color: #656660; background-color: #272822\">2 </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">print(results[:</span><span style=\"color: #ae81ff; text-decoration-color: #ae81ff; background-color: #272822\">5</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">])</span><span style=\"background-color: #272822\">                                                                                         </span> │\n",
              "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mExecution logs:\u001b[0m\n",
              "## Se\n",
              "\n",
              "Out: None\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Execution logs:</span>\n",
              "## Se\n",
              "\n",
              "Out: None\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2m[Step 2: Duration 2.74 seconds| Input tokens: 9,089 | Output tokens: 318]\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 2: Duration 2.74 seconds| Input tokens: 9,089 | Output tokens: 318]</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m3\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "╭─ \u001b[1mExecuting this code:\u001b[0m ──────────────────────────────────────────────────────────────────────────────────────────╮\n",
              "│ \u001b[1;38;2;227;227;221;48;2;39;40;34m  \u001b[0m\u001b[38;2;101;102;96;48;2;39;40;34m1 \u001b[0m\u001b[38;2;149;144;119;48;2;39;40;34m# Print the type of results and the type of the first element within it\u001b[0m\u001b[48;2;39;40;34m                                    \u001b[0m │\n",
              "│ \u001b[1;38;2;227;227;221;48;2;39;40;34m  \u001b[0m\u001b[38;2;101;102;96;48;2;39;40;34m2 \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mprint\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mtype\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mresults\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                       \u001b[0m │\n",
              "│ \u001b[1;38;2;227;227;221;48;2;39;40;34m  \u001b[0m\u001b[38;2;101;102;96;48;2;39;40;34m3 \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mprint\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mtype\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mresults\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m[\u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m0\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m]\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                    \u001b[0m │\n",
              "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─ <span style=\"font-weight: bold\">Executing this code:</span> ──────────────────────────────────────────────────────────────────────────────────────────╮\n",
              "│ <span style=\"color: #e3e3dd; text-decoration-color: #e3e3dd; background-color: #272822; font-weight: bold\">  </span><span style=\"color: #656660; text-decoration-color: #656660; background-color: #272822\">1 </span><span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># Print the type of results and the type of the first element within it</span><span style=\"background-color: #272822\">                                    </span> │\n",
              "│ <span style=\"color: #e3e3dd; text-decoration-color: #e3e3dd; background-color: #272822; font-weight: bold\">  </span><span style=\"color: #656660; text-decoration-color: #656660; background-color: #272822\">2 </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">print(type(results))</span><span style=\"background-color: #272822\">                                                                                       </span> │\n",
              "│ <span style=\"color: #e3e3dd; text-decoration-color: #e3e3dd; background-color: #272822; font-weight: bold\">  </span><span style=\"color: #656660; text-decoration-color: #656660; background-color: #272822\">3 </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">print(type(results[</span><span style=\"color: #ae81ff; text-decoration-color: #ae81ff; background-color: #272822\">0</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">]))</span><span style=\"background-color: #272822\">                                                                                    </span> │\n",
              "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mExecution logs:\u001b[0m\n",
              "<class 'str'>\n",
              "<class 'str'>\n",
              "\n",
              "Out: None\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Execution logs:</span>\n",
              "&lt;class 'str'&gt;\n",
              "&lt;class 'str'&gt;\n",
              "\n",
              "Out: None\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2m[Step 3: Duration 2.89 seconds| Input tokens: 12,967 | Output tokens: 422]\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 3: Duration 2.89 seconds| Input tokens: 12,967 | Output tokens: 422]</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m4\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "╭─ \u001b[1mExecuting this code:\u001b[0m ──────────────────────────────────────────────────────────────────────────────────────────╮\n",
              "│ \u001b[1;38;2;227;227;221;48;2;39;40;34m  \u001b[0m\u001b[38;2;101;102;96;48;2;39;40;34m 1 \u001b[0m\u001b[38;2;149;144;119;48;2;39;40;34m# Split the results into individual search results\u001b[0m\u001b[48;2;39;40;34m                                                        \u001b[0m │\n",
              "│ \u001b[1;38;2;227;227;221;48;2;39;40;34m  \u001b[0m\u001b[38;2;101;102;96;48;2;39;40;34m 2 \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mindividual_results\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mresults\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34msplit\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m## Search Results\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m[\u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m1\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m]\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34msplit\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m\\n\u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m\\n\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                  \u001b[0m │\n",
              "│ \u001b[1;38;2;227;227;221;48;2;39;40;34m  \u001b[0m\u001b[38;2;101;102;96;48;2;39;40;34m 3 \u001b[0m\u001b[48;2;39;40;34m                                                                                                          \u001b[0m │\n",
              "│ \u001b[1;38;2;227;227;221;48;2;39;40;34m  \u001b[0m\u001b[38;2;101;102;96;48;2;39;40;34m 4 \u001b[0m\u001b[38;2;149;144;119;48;2;39;40;34m# Extracting relevant information from the individual results\u001b[0m\u001b[48;2;39;40;34m                                             \u001b[0m │\n",
              "│ \u001b[1;38;2;227;227;221;48;2;39;40;34m  \u001b[0m\u001b[38;2;101;102;96;48;2;39;40;34m 5 \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mdevelopment_comparison\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m[\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m]\u001b[0m\u001b[48;2;39;40;34m                                                                               \u001b[0m │\n",
              "│ \u001b[1;38;2;227;227;221;48;2;39;40;34m  \u001b[0m\u001b[38;2;101;102;96;48;2;39;40;34m 6 \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mdevelopment_words\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m[\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m'\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mdevelopment\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m'\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m'\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mprogress\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m'\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m'\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34minfrastructure\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m'\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m'\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mgrowth\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m'\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m]\u001b[0m\u001b[48;2;39;40;34m                               \u001b[0m │\n",
              "│ \u001b[1;38;2;227;227;221;48;2;39;40;34m  \u001b[0m\u001b[38;2;101;102;96;48;2;39;40;34m 7 \u001b[0m\u001b[38;2;102;217;239;48;2;39;40;34mfor\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mresult\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34min\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mindividual_results\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m:\u001b[0m\u001b[48;2;39;40;34m                                                                         \u001b[0m │\n",
              "│ \u001b[1;38;2;227;227;221;48;2;39;40;34m  \u001b[0m\u001b[38;2;101;102;96;48;2;39;40;34m 8 \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m    \u001b[0m\u001b[38;2;102;217;239;48;2;39;40;34mif\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34mnot\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mresult\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mstrip\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m:\u001b[0m\u001b[48;2;39;40;34m                                                                                \u001b[0m │\n",
              "│ \u001b[1;38;2;227;227;221;48;2;39;40;34m  \u001b[0m\u001b[38;2;101;102;96;48;2;39;40;34m 9 \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m        \u001b[0m\u001b[38;2;102;217;239;48;2;39;40;34mcontinue\u001b[0m\u001b[48;2;39;40;34m                                                                                          \u001b[0m │\n",
              "│ \u001b[1;38;2;227;227;221;48;2;39;40;34m  \u001b[0m\u001b[38;2;101;102;96;48;2;39;40;34m10 \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m    \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mtitle\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mbody\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mresult\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34msplit\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m'\u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m\\n\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m'\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m1\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                   \u001b[0m │\n",
              "│ \u001b[1;38;2;227;227;221;48;2;39;40;34m  \u001b[0m\u001b[38;2;101;102;96;48;2;39;40;34m11 \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m    \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mtitle\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mtitle\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mstrip\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                 \u001b[0m │\n",
              "│ \u001b[1;38;2;227;227;221;48;2;39;40;34m  \u001b[0m\u001b[38;2;101;102;96;48;2;39;40;34m12 \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m    \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mbody\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mbody\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mstrip\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                   \u001b[0m │\n",
              "│ \u001b[1;38;2;227;227;221;48;2;39;40;34m  \u001b[0m\u001b[38;2;101;102;96;48;2;39;40;34m13 \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m    \u001b[0m\u001b[38;2;102;217;239;48;2;39;40;34mif\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34many\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mword\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34min\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mbody\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mlower\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;102;217;239;48;2;39;40;34mfor\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mword\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34min\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mdevelopment_words\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m:\u001b[0m\u001b[48;2;39;40;34m                                           \u001b[0m │\n",
              "│ \u001b[1;38;2;227;227;221;48;2;39;40;34m  \u001b[0m\u001b[38;2;101;102;96;48;2;39;40;34m14 \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m        \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mdevelopment_comparison\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mappend\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mtitle\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mbody\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                      \u001b[0m │\n",
              "│ \u001b[1;38;2;227;227;221;48;2;39;40;34m  \u001b[0m\u001b[38;2;101;102;96;48;2;39;40;34m15 \u001b[0m\u001b[48;2;39;40;34m                                                                                                          \u001b[0m │\n",
              "│ \u001b[1;38;2;227;227;221;48;2;39;40;34m  \u001b[0m\u001b[38;2;101;102;96;48;2;39;40;34m16 \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mprint\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mdevelopment_comparison\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                             \u001b[0m │\n",
              "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─ <span style=\"font-weight: bold\">Executing this code:</span> ──────────────────────────────────────────────────────────────────────────────────────────╮\n",
              "│ <span style=\"color: #e3e3dd; text-decoration-color: #e3e3dd; background-color: #272822; font-weight: bold\">  </span><span style=\"color: #656660; text-decoration-color: #656660; background-color: #272822\"> 1 </span><span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># Split the results into individual search results</span><span style=\"background-color: #272822\">                                                        </span> │\n",
              "│ <span style=\"color: #e3e3dd; text-decoration-color: #e3e3dd; background-color: #272822; font-weight: bold\">  </span><span style=\"color: #656660; text-decoration-color: #656660; background-color: #272822\"> 2 </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">individual_results </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> results</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">split(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"## Search Results\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)[</span><span style=\"color: #ae81ff; text-decoration-color: #ae81ff; background-color: #272822\">1</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">]</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">split(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"</span><span style=\"color: #ae81ff; text-decoration-color: #ae81ff; background-color: #272822\">\\n\\n</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">                                  </span> │\n",
              "│ <span style=\"color: #e3e3dd; text-decoration-color: #e3e3dd; background-color: #272822; font-weight: bold\">  </span><span style=\"color: #656660; text-decoration-color: #656660; background-color: #272822\"> 3 </span><span style=\"background-color: #272822\">                                                                                                          </span> │\n",
              "│ <span style=\"color: #e3e3dd; text-decoration-color: #e3e3dd; background-color: #272822; font-weight: bold\">  </span><span style=\"color: #656660; text-decoration-color: #656660; background-color: #272822\"> 4 </span><span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># Extracting relevant information from the individual results</span><span style=\"background-color: #272822\">                                             </span> │\n",
              "│ <span style=\"color: #e3e3dd; text-decoration-color: #e3e3dd; background-color: #272822; font-weight: bold\">  </span><span style=\"color: #656660; text-decoration-color: #656660; background-color: #272822\"> 5 </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">development_comparison </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> []</span><span style=\"background-color: #272822\">                                                                               </span> │\n",
              "│ <span style=\"color: #e3e3dd; text-decoration-color: #e3e3dd; background-color: #272822; font-weight: bold\">  </span><span style=\"color: #656660; text-decoration-color: #656660; background-color: #272822\"> 6 </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">development_words </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> [</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">'development'</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">, </span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">'progress'</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">, </span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">'infrastructure'</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">, </span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">'growth'</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">]</span><span style=\"background-color: #272822\">                               </span> │\n",
              "│ <span style=\"color: #e3e3dd; text-decoration-color: #e3e3dd; background-color: #272822; font-weight: bold\">  </span><span style=\"color: #656660; text-decoration-color: #656660; background-color: #272822\"> 7 </span><span style=\"color: #66d9ef; text-decoration-color: #66d9ef; background-color: #272822\">for</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> result </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">in</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> individual_results:</span><span style=\"background-color: #272822\">                                                                         </span> │\n",
              "│ <span style=\"color: #e3e3dd; text-decoration-color: #e3e3dd; background-color: #272822; font-weight: bold\">  </span><span style=\"color: #656660; text-decoration-color: #656660; background-color: #272822\"> 8 </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    </span><span style=\"color: #66d9ef; text-decoration-color: #66d9ef; background-color: #272822\">if</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">not</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> result</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">strip():</span><span style=\"background-color: #272822\">                                                                                </span> │\n",
              "│ <span style=\"color: #e3e3dd; text-decoration-color: #e3e3dd; background-color: #272822; font-weight: bold\">  </span><span style=\"color: #656660; text-decoration-color: #656660; background-color: #272822\"> 9 </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">        </span><span style=\"color: #66d9ef; text-decoration-color: #66d9ef; background-color: #272822\">continue</span><span style=\"background-color: #272822\">                                                                                          </span> │\n",
              "│ <span style=\"color: #e3e3dd; text-decoration-color: #e3e3dd; background-color: #272822; font-weight: bold\">  </span><span style=\"color: #656660; text-decoration-color: #656660; background-color: #272822\">10 </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    title, body </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> result</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">split(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">'</span><span style=\"color: #ae81ff; text-decoration-color: #ae81ff; background-color: #272822\">\\n</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">'</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">, </span><span style=\"color: #ae81ff; text-decoration-color: #ae81ff; background-color: #272822\">1</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">                                                                   </span> │\n",
              "│ <span style=\"color: #e3e3dd; text-decoration-color: #e3e3dd; background-color: #272822; font-weight: bold\">  </span><span style=\"color: #656660; text-decoration-color: #656660; background-color: #272822\">11 </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    title </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> title</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">strip()</span><span style=\"background-color: #272822\">                                                                                 </span> │\n",
              "│ <span style=\"color: #e3e3dd; text-decoration-color: #e3e3dd; background-color: #272822; font-weight: bold\">  </span><span style=\"color: #656660; text-decoration-color: #656660; background-color: #272822\">12 </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    body </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> body</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">strip()</span><span style=\"background-color: #272822\">                                                                                   </span> │\n",
              "│ <span style=\"color: #e3e3dd; text-decoration-color: #e3e3dd; background-color: #272822; font-weight: bold\">  </span><span style=\"color: #656660; text-decoration-color: #656660; background-color: #272822\">13 </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    </span><span style=\"color: #66d9ef; text-decoration-color: #66d9ef; background-color: #272822\">if</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> any(word </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">in</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> body</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">lower() </span><span style=\"color: #66d9ef; text-decoration-color: #66d9ef; background-color: #272822\">for</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> word </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">in</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> development_words):</span><span style=\"background-color: #272822\">                                           </span> │\n",
              "│ <span style=\"color: #e3e3dd; text-decoration-color: #e3e3dd; background-color: #272822; font-weight: bold\">  </span><span style=\"color: #656660; text-decoration-color: #656660; background-color: #272822\">14 </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">        development_comparison</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">append((title, body))</span><span style=\"background-color: #272822\">                                                      </span> │\n",
              "│ <span style=\"color: #e3e3dd; text-decoration-color: #e3e3dd; background-color: #272822; font-weight: bold\">  </span><span style=\"color: #656660; text-decoration-color: #656660; background-color: #272822\">15 </span><span style=\"background-color: #272822\">                                                                                                          </span> │\n",
              "│ <span style=\"color: #e3e3dd; text-decoration-color: #e3e3dd; background-color: #272822; font-weight: bold\">  </span><span style=\"color: #656660; text-decoration-color: #656660; background-color: #272822\">16 </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">print(development_comparison)</span><span style=\"background-color: #272822\">                                                                             </span> │\n",
              "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;31mCode execution failed: Code execution failed at line 'for result in individual_results:\u001b[0m\n",
              "\u001b[1;31m    if not \u001b[0m\u001b[1;31mresult.strip\u001b[0m\u001b[1;31m(\u001b[0m\u001b[1;31m)\u001b[0m\u001b[1;31m:\u001b[0m\n",
              "\u001b[1;31m        continue\u001b[0m\n",
              "\u001b[1;31m    title, body = \u001b[0m\u001b[1;31mresult.split\u001b[0m\u001b[1;31m(\u001b[0m\u001b[1;31m'\\n'\u001b[0m\u001b[1;31m, \u001b[0m\u001b[1;31m1\u001b[0m\u001b[1;31m)\u001b[0m\n",
              "\u001b[1;31m    title = \u001b[0m\u001b[1;31mtitle.strip\u001b[0m\u001b[1;31m(\u001b[0m\u001b[1;31m)\u001b[0m\n",
              "\u001b[1;31m    body = \u001b[0m\u001b[1;31mbody.strip\u001b[0m\u001b[1;31m(\u001b[0m\u001b[1;31m)\u001b[0m\n",
              "\u001b[1;31m    if \u001b[0m\u001b[1;31many\u001b[0m\u001b[1;31m(\u001b[0m\u001b[1;31mword in \u001b[0m\u001b[1;31mbody.lower\u001b[0m\u001b[1;31m(\u001b[0m\u001b[1;31m)\u001b[0m\u001b[1;31m for word in development_words\u001b[0m\u001b[1;31m)\u001b[0m\u001b[1;31m:\u001b[0m\n",
              "\u001b[1;31m        \u001b[0m\u001b[1;31mdevelopment_comparison.append\u001b[0m\u001b[1;31m(\u001b[0m\u001b[1;31m(\u001b[0m\u001b[1;31mtitle, body\u001b[0m\u001b[1;31m)\u001b[0m\u001b[1;31m)\u001b[0m\u001b[1;31m' because of the following error:\u001b[0m\n",
              "\u001b[1;31mCannot unpack tuple of wrong size\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Code execution failed: Code execution failed at line 'for result in individual_results:</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">    if not result.strip():</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">        continue</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">    title, body = result.split(</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">'\\n'</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">, </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">1</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">)</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">    title = title.strip()</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">    body = body.strip()</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">    if any(word in body.lower() for word in development_words):</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">        development_comparison.append((title, body))' because of the following error:</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Cannot unpack tuple of wrong size</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2m[Step 4: Duration 7.86 seconds| Input tokens: 17,046 | Output tokens: 781]\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 4: Duration 7.86 seconds| Input tokens: 17,046 | Output tokens: 781]</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m5\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "╭─ \u001b[1mExecuting this code:\u001b[0m ──────────────────────────────────────────────────────────────────────────────────────────╮\n",
              "│ \u001b[1;38;2;227;227;221;48;2;39;40;34m  \u001b[0m\u001b[38;2;101;102;96;48;2;39;40;34m 1 \u001b[0m\u001b[38;2;149;144;119;48;2;39;40;34m# Split the results into individual search results\u001b[0m\u001b[48;2;39;40;34m                                                        \u001b[0m │\n",
              "│ \u001b[1;38;2;227;227;221;48;2;39;40;34m  \u001b[0m\u001b[38;2;101;102;96;48;2;39;40;34m 2 \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mindividual_results\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mresults\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34msplit\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m## Search Results\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m[\u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m1\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m]\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mstrip\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34msplit\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m\\n\u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m\\n\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                          \u001b[0m │\n",
              "│ \u001b[1;38;2;227;227;221;48;2;39;40;34m  \u001b[0m\u001b[38;2;101;102;96;48;2;39;40;34m 3 \u001b[0m\u001b[48;2;39;40;34m                                                                                                          \u001b[0m │\n",
              "│ \u001b[1;38;2;227;227;221;48;2;39;40;34m  \u001b[0m\u001b[38;2;101;102;96;48;2;39;40;34m 4 \u001b[0m\u001b[38;2;149;144;119;48;2;39;40;34m# Extracting relevant information from the individual results\u001b[0m\u001b[48;2;39;40;34m                                             \u001b[0m │\n",
              "│ \u001b[1;38;2;227;227;221;48;2;39;40;34m  \u001b[0m\u001b[38;2;101;102;96;48;2;39;40;34m 5 \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mdevelopment_comparison\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m[\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m]\u001b[0m\u001b[48;2;39;40;34m                                                                               \u001b[0m │\n",
              "│ \u001b[1;38;2;227;227;221;48;2;39;40;34m  \u001b[0m\u001b[38;2;101;102;96;48;2;39;40;34m 6 \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mdevelopment_words\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m[\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m'\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mdevelopment\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m'\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m'\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mprogress\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m'\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m'\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34minfrastructure\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m'\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m'\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mgrowth\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m'\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m]\u001b[0m\u001b[48;2;39;40;34m                               \u001b[0m │\n",
              "│ \u001b[1;38;2;227;227;221;48;2;39;40;34m  \u001b[0m\u001b[38;2;101;102;96;48;2;39;40;34m 7 \u001b[0m\u001b[48;2;39;40;34m                                                                                                          \u001b[0m │\n",
              "│ \u001b[1;38;2;227;227;221;48;2;39;40;34m  \u001b[0m\u001b[38;2;101;102;96;48;2;39;40;34m 8 \u001b[0m\u001b[38;2;102;217;239;48;2;39;40;34mfor\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mresult\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34min\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mindividual_results\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m:\u001b[0m\u001b[48;2;39;40;34m                                                                         \u001b[0m │\n",
              "│ \u001b[1;38;2;227;227;221;48;2;39;40;34m  \u001b[0m\u001b[38;2;101;102;96;48;2;39;40;34m 9 \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m    \u001b[0m\u001b[38;2;102;217;239;48;2;39;40;34mif\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34mnot\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mresult\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mstrip\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m:\u001b[0m\u001b[48;2;39;40;34m                                                                                \u001b[0m │\n",
              "│ \u001b[1;38;2;227;227;221;48;2;39;40;34m  \u001b[0m\u001b[38;2;101;102;96;48;2;39;40;34m10 \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m        \u001b[0m\u001b[38;2;102;217;239;48;2;39;40;34mcontinue\u001b[0m\u001b[48;2;39;40;34m                                                                                          \u001b[0m │\n",
              "│ \u001b[1;38;2;227;227;221;48;2;39;40;34m  \u001b[0m\u001b[38;2;101;102;96;48;2;39;40;34m11 \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m    \u001b[0m\u001b[38;2;149;144;119;48;2;39;40;34m# Splitting the title and body based on the first \\n\u001b[0m\u001b[48;2;39;40;34m                                                  \u001b[0m │\n",
              "│ \u001b[1;38;2;227;227;221;48;2;39;40;34m  \u001b[0m\u001b[38;2;101;102;96;48;2;39;40;34m12 \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m    \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mparts\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mresult\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34msplit\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m'\u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m\\n\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m'\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m1\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                         \u001b[0m │\n",
              "│ \u001b[1;38;2;227;227;221;48;2;39;40;34m  \u001b[0m\u001b[38;2;101;102;96;48;2;39;40;34m13 \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m    \u001b[0m\u001b[38;2;102;217;239;48;2;39;40;34mif\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mlen\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mparts\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m==\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m2\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m:\u001b[0m\u001b[48;2;39;40;34m                                                                                   \u001b[0m │\n",
              "│ \u001b[1;38;2;227;227;221;48;2;39;40;34m  \u001b[0m\u001b[38;2;101;102;96;48;2;39;40;34m14 \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m        \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mtitle\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mbody\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mparts\u001b[0m\u001b[48;2;39;40;34m                                                                               \u001b[0m │\n",
              "│ \u001b[1;38;2;227;227;221;48;2;39;40;34m  \u001b[0m\u001b[38;2;101;102;96;48;2;39;40;34m15 \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m        \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mtitle\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mtitle\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mstrip\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                             \u001b[0m │\n",
              "│ \u001b[1;38;2;227;227;221;48;2;39;40;34m  \u001b[0m\u001b[38;2;101;102;96;48;2;39;40;34m16 \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m        \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mbody\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mbody\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mstrip\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                               \u001b[0m │\n",
              "│ \u001b[1;38;2;227;227;221;48;2;39;40;34m  \u001b[0m\u001b[38;2;101;102;96;48;2;39;40;34m17 \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m        \u001b[0m\u001b[38;2;102;217;239;48;2;39;40;34mif\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34many\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mword\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34min\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mbody\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mlower\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;102;217;239;48;2;39;40;34mfor\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mword\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34min\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mdevelopment_words\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m:\u001b[0m\u001b[48;2;39;40;34m                                       \u001b[0m │\n",
              "│ \u001b[1;38;2;227;227;221;48;2;39;40;34m  \u001b[0m\u001b[38;2;101;102;96;48;2;39;40;34m18 \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m            \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mdevelopment_comparison\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mappend\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mtitle\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mbody\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                  \u001b[0m │\n",
              "│ \u001b[1;38;2;227;227;221;48;2;39;40;34m  \u001b[0m\u001b[38;2;101;102;96;48;2;39;40;34m19 \u001b[0m\u001b[48;2;39;40;34m                                                                                                          \u001b[0m │\n",
              "│ \u001b[1;38;2;227;227;221;48;2;39;40;34m  \u001b[0m\u001b[38;2;101;102;96;48;2;39;40;34m20 \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mprint\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mdevelopment_comparison\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                             \u001b[0m │\n",
              "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─ <span style=\"font-weight: bold\">Executing this code:</span> ──────────────────────────────────────────────────────────────────────────────────────────╮\n",
              "│ <span style=\"color: #e3e3dd; text-decoration-color: #e3e3dd; background-color: #272822; font-weight: bold\">  </span><span style=\"color: #656660; text-decoration-color: #656660; background-color: #272822\"> 1 </span><span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># Split the results into individual search results</span><span style=\"background-color: #272822\">                                                        </span> │\n",
              "│ <span style=\"color: #e3e3dd; text-decoration-color: #e3e3dd; background-color: #272822; font-weight: bold\">  </span><span style=\"color: #656660; text-decoration-color: #656660; background-color: #272822\"> 2 </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">individual_results </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> results</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">split(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"## Search Results\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)[</span><span style=\"color: #ae81ff; text-decoration-color: #ae81ff; background-color: #272822\">1</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">]</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">strip()</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">split(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"</span><span style=\"color: #ae81ff; text-decoration-color: #ae81ff; background-color: #272822\">\\n\\n</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">                          </span> │\n",
              "│ <span style=\"color: #e3e3dd; text-decoration-color: #e3e3dd; background-color: #272822; font-weight: bold\">  </span><span style=\"color: #656660; text-decoration-color: #656660; background-color: #272822\"> 3 </span><span style=\"background-color: #272822\">                                                                                                          </span> │\n",
              "│ <span style=\"color: #e3e3dd; text-decoration-color: #e3e3dd; background-color: #272822; font-weight: bold\">  </span><span style=\"color: #656660; text-decoration-color: #656660; background-color: #272822\"> 4 </span><span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># Extracting relevant information from the individual results</span><span style=\"background-color: #272822\">                                             </span> │\n",
              "│ <span style=\"color: #e3e3dd; text-decoration-color: #e3e3dd; background-color: #272822; font-weight: bold\">  </span><span style=\"color: #656660; text-decoration-color: #656660; background-color: #272822\"> 5 </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">development_comparison </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> []</span><span style=\"background-color: #272822\">                                                                               </span> │\n",
              "│ <span style=\"color: #e3e3dd; text-decoration-color: #e3e3dd; background-color: #272822; font-weight: bold\">  </span><span style=\"color: #656660; text-decoration-color: #656660; background-color: #272822\"> 6 </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">development_words </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> [</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">'development'</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">, </span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">'progress'</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">, </span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">'infrastructure'</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">, </span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">'growth'</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">]</span><span style=\"background-color: #272822\">                               </span> │\n",
              "│ <span style=\"color: #e3e3dd; text-decoration-color: #e3e3dd; background-color: #272822; font-weight: bold\">  </span><span style=\"color: #656660; text-decoration-color: #656660; background-color: #272822\"> 7 </span><span style=\"background-color: #272822\">                                                                                                          </span> │\n",
              "│ <span style=\"color: #e3e3dd; text-decoration-color: #e3e3dd; background-color: #272822; font-weight: bold\">  </span><span style=\"color: #656660; text-decoration-color: #656660; background-color: #272822\"> 8 </span><span style=\"color: #66d9ef; text-decoration-color: #66d9ef; background-color: #272822\">for</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> result </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">in</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> individual_results:</span><span style=\"background-color: #272822\">                                                                         </span> │\n",
              "│ <span style=\"color: #e3e3dd; text-decoration-color: #e3e3dd; background-color: #272822; font-weight: bold\">  </span><span style=\"color: #656660; text-decoration-color: #656660; background-color: #272822\"> 9 </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    </span><span style=\"color: #66d9ef; text-decoration-color: #66d9ef; background-color: #272822\">if</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">not</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> result</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">strip():</span><span style=\"background-color: #272822\">                                                                                </span> │\n",
              "│ <span style=\"color: #e3e3dd; text-decoration-color: #e3e3dd; background-color: #272822; font-weight: bold\">  </span><span style=\"color: #656660; text-decoration-color: #656660; background-color: #272822\">10 </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">        </span><span style=\"color: #66d9ef; text-decoration-color: #66d9ef; background-color: #272822\">continue</span><span style=\"background-color: #272822\">                                                                                          </span> │\n",
              "│ <span style=\"color: #e3e3dd; text-decoration-color: #e3e3dd; background-color: #272822; font-weight: bold\">  </span><span style=\"color: #656660; text-decoration-color: #656660; background-color: #272822\">11 </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    </span><span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># Splitting the title and body based on the first \\n</span><span style=\"background-color: #272822\">                                                  </span> │\n",
              "│ <span style=\"color: #e3e3dd; text-decoration-color: #e3e3dd; background-color: #272822; font-weight: bold\">  </span><span style=\"color: #656660; text-decoration-color: #656660; background-color: #272822\">12 </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    parts </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> result</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">split(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">'</span><span style=\"color: #ae81ff; text-decoration-color: #ae81ff; background-color: #272822\">\\n</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">'</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">, </span><span style=\"color: #ae81ff; text-decoration-color: #ae81ff; background-color: #272822\">1</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">                                                                         </span> │\n",
              "│ <span style=\"color: #e3e3dd; text-decoration-color: #e3e3dd; background-color: #272822; font-weight: bold\">  </span><span style=\"color: #656660; text-decoration-color: #656660; background-color: #272822\">13 </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    </span><span style=\"color: #66d9ef; text-decoration-color: #66d9ef; background-color: #272822\">if</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> len(parts) </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">==</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> </span><span style=\"color: #ae81ff; text-decoration-color: #ae81ff; background-color: #272822\">2</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">:</span><span style=\"background-color: #272822\">                                                                                   </span> │\n",
              "│ <span style=\"color: #e3e3dd; text-decoration-color: #e3e3dd; background-color: #272822; font-weight: bold\">  </span><span style=\"color: #656660; text-decoration-color: #656660; background-color: #272822\">14 </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">        title, body </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> parts</span><span style=\"background-color: #272822\">                                                                               </span> │\n",
              "│ <span style=\"color: #e3e3dd; text-decoration-color: #e3e3dd; background-color: #272822; font-weight: bold\">  </span><span style=\"color: #656660; text-decoration-color: #656660; background-color: #272822\">15 </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">        title </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> title</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">strip()</span><span style=\"background-color: #272822\">                                                                             </span> │\n",
              "│ <span style=\"color: #e3e3dd; text-decoration-color: #e3e3dd; background-color: #272822; font-weight: bold\">  </span><span style=\"color: #656660; text-decoration-color: #656660; background-color: #272822\">16 </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">        body </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> body</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">strip()</span><span style=\"background-color: #272822\">                                                                               </span> │\n",
              "│ <span style=\"color: #e3e3dd; text-decoration-color: #e3e3dd; background-color: #272822; font-weight: bold\">  </span><span style=\"color: #656660; text-decoration-color: #656660; background-color: #272822\">17 </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">        </span><span style=\"color: #66d9ef; text-decoration-color: #66d9ef; background-color: #272822\">if</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> any(word </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">in</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> body</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">lower() </span><span style=\"color: #66d9ef; text-decoration-color: #66d9ef; background-color: #272822\">for</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> word </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">in</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> development_words):</span><span style=\"background-color: #272822\">                                       </span> │\n",
              "│ <span style=\"color: #e3e3dd; text-decoration-color: #e3e3dd; background-color: #272822; font-weight: bold\">  </span><span style=\"color: #656660; text-decoration-color: #656660; background-color: #272822\">18 </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">            development_comparison</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">append((title, body))</span><span style=\"background-color: #272822\">                                                  </span> │\n",
              "│ <span style=\"color: #e3e3dd; text-decoration-color: #e3e3dd; background-color: #272822; font-weight: bold\">  </span><span style=\"color: #656660; text-decoration-color: #656660; background-color: #272822\">19 </span><span style=\"background-color: #272822\">                                                                                                          </span> │\n",
              "│ <span style=\"color: #e3e3dd; text-decoration-color: #e3e3dd; background-color: #272822; font-weight: bold\">  </span><span style=\"color: #656660; text-decoration-color: #656660; background-color: #272822\">20 </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">print(development_comparison)</span><span style=\"background-color: #272822\">                                                                             </span> │\n",
              "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mExecution logs:\u001b[0m\n",
              "[('[Progress stalled under KCR, thrived under Modi: Rajnath - Times of \n",
              "India](https://timesofindia.indiatimes.com/city/hyderabad/progress-stalled-under-kcr/articleshow/105486657.cms)', \n",
              "\"Defence minister Rajnath Singh compares the progress under Prime Minister Narendra Modi's leadership with the \n",
              "stalled growth in Telangana under KCR's governance. Learn more about the impact of BJP ...\"), (\"[On KCR's 'Modi \n",
              "good friend' remark, BJP says 'part of his \n",
              "conspiracy'](https://www.hindustantimes.com/india-news/on-kcrs-modi-good-friend-remark-bjp-says-part-of-his-conspir\n",
              "acy-101686964856326.html)\", '\"Telangana is totally destroyed under the rule of KCR, Development is possible only \n",
              "with the Double Engine government of BJP,\" he added. On Thursday, Rao, popularly known as \\'KCR\\', called PM ...'),\n",
              "(\"[Telangana Chief Minister KCR To Skip PM Modi's Programme In The State \n",
              "Today](https://www.ndtv.com/india-news/telangana-chief-minister-kcr-to-skip-pm-modis-programme-in-the-state-today-3\n",
              "930686)\", 'In a major thrust to infrastructure and connectivity, Prime Minister Narendra Modi will inaugurate and \n",
              "lay the foundation stone of development projects worth over Rs 11,300 crore in Telangana on ...')]\n",
              "\n",
              "Out: None\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Execution logs:</span>\n",
              "[('[Progress stalled under KCR, thrived under Modi: Rajnath - Times of \n",
              "India](https://timesofindia.indiatimes.com/city/hyderabad/progress-stalled-under-kcr/articleshow/105486657.cms)', \n",
              "\"Defence minister Rajnath Singh compares the progress under Prime Minister Narendra Modi's leadership with the \n",
              "stalled growth in Telangana under KCR's governance. Learn more about the impact of BJP ...\"), (\"[On KCR's 'Modi \n",
              "good friend' remark, BJP says 'part of his \n",
              "conspiracy'](https://www.hindustantimes.com/india-news/on-kcrs-modi-good-friend-remark-bjp-says-part-of-his-conspir\n",
              "acy-101686964856326.html)\", '\"Telangana is totally destroyed under the rule of KCR, Development is possible only \n",
              "with the Double Engine government of BJP,\" he added. On Thursday, Rao, popularly known as \\'KCR\\', called PM ...'),\n",
              "(\"[Telangana Chief Minister KCR To Skip PM Modi's Programme In The State \n",
              "Today](https://www.ndtv.com/india-news/telangana-chief-minister-kcr-to-skip-pm-modis-programme-in-the-state-today-3\n",
              "930686)\", 'In a major thrust to infrastructure and connectivity, Prime Minister Narendra Modi will inaugurate and \n",
              "lay the foundation stone of development projects worth over Rs 11,300 crore in Telangana on ...')]\n",
              "\n",
              "Out: None\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2m[Step 5: Duration 28.27 seconds| Input tokens: 21,794 | Output tokens: 2,002]\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 5: Duration 28.27 seconds| Input tokens: 21,794 | Output tokens: 2,002]</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;31mReached max iterations.\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Reached max iterations.</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Final answer: Based on the extracted search results, the comparison between Narendra Modi and K. Chandrashekar Rao \n",
              "(KCR) in terms of development shows a few key points:\n",
              "\n",
              "1. **Progress Stalled Under KCR, Thrived Under Modi**: The statement by Defence Minister Rajnath Singh suggests \n",
              "that while development progressed under Modi's leadership, it stalled under KCR's governance.\n",
              "\n",
              "2. **BJP's Criticism of KCR's Leadership**: The BJP has made strong statements criticizing KCR's leadership, \n",
              "stating that development is not possible under his tenure and that Telangana is \"totally destroyed\" under his rule.\n",
              "\n",
              "3. **Modi Launching Major Development Projects**: Reports indicate that Prime Minister Modi is launching \n",
              "significant infrastructure and connectivity projects worth over Rs 11,300 crore in Telangana, which KCR chose not \n",
              "to participate in.\n",
              "\n",
              "These points collectively suggest that development initiatives and overall progress have been more significant and \n",
              "effective under Modi's leadership compared to KCR's. Therefore, based on the information provided, Modi appears to \n",
              "be better in terms of development.\n",
              "\n",
              "Final Answer: **Modi has shown a stronger record of advancing development compared to KCR based on the information \n",
              "available.**\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Final answer: Based on the extracted search results, the comparison between Narendra Modi and K. Chandrashekar Rao \n",
              "(KCR) in terms of development shows a few key points:\n",
              "\n",
              "1. **Progress Stalled Under KCR, Thrived Under Modi**: The statement by Defence Minister Rajnath Singh suggests \n",
              "that while development progressed under Modi's leadership, it stalled under KCR's governance.\n",
              "\n",
              "2. **BJP's Criticism of KCR's Leadership**: The BJP has made strong statements criticizing KCR's leadership, \n",
              "stating that development is not possible under his tenure and that Telangana is \"totally destroyed\" under his rule.\n",
              "\n",
              "3. **Modi Launching Major Development Projects**: Reports indicate that Prime Minister Modi is launching \n",
              "significant infrastructure and connectivity projects worth over Rs 11,300 crore in Telangana, which KCR chose not \n",
              "to participate in.\n",
              "\n",
              "These points collectively suggest that development initiatives and overall progress have been more significant and \n",
              "effective under Modi's leadership compared to KCR's. Therefore, based on the information provided, Modi appears to \n",
              "be better in terms of development.\n",
              "\n",
              "Final Answer: **Modi has shown a stronger record of advancing development compared to KCR based on the information \n",
              "available.**\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2m[Step 6: Duration 0.00 seconds| Input tokens: 26,348 | Output tokens: 2,247]\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 6: Duration 0.00 seconds| Input tokens: 26,348 | Output tokens: 2,247]</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Based on the extracted search results, the comparison between Narendra Modi and K. Chandrashekar Rao (KCR) in terms of development shows a few key points:\\n\\n1. **Progress Stalled Under KCR, Thrived Under Modi**: The statement by Defence Minister Rajnath Singh suggests that while development progressed under Modi\\'s leadership, it stalled under KCR\\'s governance.\\n\\n2. **BJP\\'s Criticism of KCR\\'s Leadership**: The BJP has made strong statements criticizing KCR\\'s leadership, stating that development is not possible under his tenure and that Telangana is \"totally destroyed\" under his rule.\\n\\n3. **Modi Launching Major Development Projects**: Reports indicate that Prime Minister Modi is launching significant infrastructure and connectivity projects worth over Rs 11,300 crore in Telangana, which KCR chose not to participate in.\\n\\nThese points collectively suggest that development initiatives and overall progress have been more significant and effective under Modi\\'s leadership compared to KCR\\'s. Therefore, based on the information provided, Modi appears to be better in terms of development.\\n\\nFinal Answer: **Modi has shown a stronger record of advancing development compared to KCR based on the information available.**'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# RAG System"
      ],
      "metadata": {
        "id": "9za0nfd92MMR"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets -q\n",
        "!pip install rank_bm25 -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c43-dcDX217h",
        "outputId": "440ae6cf-d3b4-46c6-c1f5-c3da58b59090"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/480.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/116.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/179.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/134.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/194.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.10.1 requires pandas<2.2.3dev0,>=2.0, but you have pandas 2.2.3 which is incompatible.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q4ZfVrjE3WfD",
        "outputId": "5b8c70dd-17bd-43e3-bc40-3420e391c009"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.3.14-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (3.11.10)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting httpx-sse<0.5.0,>=0.4.0 (from langchain_community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting langchain<0.4.0,>=0.3.14 (from langchain_community)\n",
            "  Downloading langchain-0.3.14-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting langchain-core<0.4.0,>=0.3.29 (from langchain_community)\n",
            "  Downloading langchain_core-0.3.29-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: langsmith<0.3,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.2.3)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (1.26.4)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
            "  Downloading pydantic_settings-2.7.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (9.0.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.18.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.23.3-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.14->langchain_community) (0.3.3)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.14->langchain_community) (2.10.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain_community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain_community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain_community) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain_community) (0.27.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain_community) (3.10.12)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2024.12.14)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain_community) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain_community) (1.0.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain_community) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain_community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.29->langchain_community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.14->langchain_community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.14->langchain_community) (2.27.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain_community) (1.2.2)\n",
            "Downloading langchain_community-0.3.14-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading langchain-0.3.14-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m42.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.3.29-py3-none-any.whl (411 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.6/411.6 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.7.1-py3-none-any.whl (29 kB)\n",
            "Downloading marshmallow-3.23.3-py3-none-any.whl (48 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain-core, langchain, langchain_community\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.25\n",
            "    Uninstalling langchain-core-0.3.25:\n",
            "      Successfully uninstalled langchain-core-0.3.25\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.3.12\n",
            "    Uninstalling langchain-0.3.12:\n",
            "      Successfully uninstalled langchain-0.3.12\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-0.3.14 langchain-core-0.3.29 langchain_community-0.3.14 marshmallow-3.23.3 mypy-extensions-1.0.0 pydantic-settings-2.7.1 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.retrievers import BM25Retriever"
      ],
      "metadata": {
        "id": "XObc5ScK3Efn"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "knowledge_base = datasets.load_dataset(\"m-ric/huggingface_doc\", split=\"train\")\n",
        "knowledge_base = knowledge_base.filter(lambda row: row[\"source\"].startswith(\"huggingface/transformers\"))\n",
        "\n",
        "source_docs = [\n",
        "    Document(page_content=doc[\"text\"], metadata={\"source\": doc[\"source\"].split(\"/\")[1]})\n",
        "    for doc in knowledge_base\n",
        "]\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=500,\n",
        "    chunk_overlap=50,\n",
        "    add_start_index=True,\n",
        "    strip_whitespace=True,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \".\",\" \", \"\"]\n",
        "    )\n",
        "\n",
        "docs_processed = text_splitter.split_documents(source_docs)"
      ],
      "metadata": {
        "id": "XAe1-K1Y3UFj"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs_processed"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SyljbxdF4Ige",
        "outputId": "89b15f6c-9fc9-47bb-e011-d2f9f592db33"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': 'transformers', 'start_index': 0}, page_content='!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 232}, page_content='http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\n\\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer.\\n\\n-->\\n\\n# Training on TPU with TensorFlow\\n\\n<Tip>'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 671}, page_content=\"-->\\n\\n# Training on TPU with TensorFlow\\n\\n<Tip>\\n\\nIf you don't need long explanations and just want TPU code samples to get started with, check out [our TPU example notebook!](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/tpu_training-tf.ipynb)\\n\\n</Tip>\\n\\n### What is a TPU?\"),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 950}, page_content='</Tip>\\n\\n### What is a TPU?\\n\\nA TPU is a **Tensor Processing Unit.** They are hardware designed by Google, which are used to greatly speed up the tensor computations within neural networks, much like GPUs. They can be used for both network training and inference. They are generally accessed through Google’s cloud services, but small TPUs can also be accessed directly for free through Google Colab and Kaggle Kernels.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 1369}, page_content='Because [all TensorFlow models in 🤗 Transformers are Keras models](https://huggingface.co/blog/tensorflow-philosophy), most of the methods in this document are generally applicable to TPU training for any Keras model! However, there are a few points that are specific to the HuggingFace ecosystem (hug-o-system?) of Transformers and Datasets, and we’ll make sure to flag them up when we get to them.\\n\\n### What kinds of TPU are available?'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 1770}, page_content='### What kinds of TPU are available?\\n\\nNew users are often very confused by the range of TPUs, and the different ways to access them. The first key distinction to understand is the difference between **TPU Nodes** and **TPU VMs.**'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 2001}, page_content='When you use a **TPU Node**, you are effectively indirectly accessing a remote TPU. You will need a separate VM, which will initialize your network and data pipeline and then forward them to the remote node. When you use a TPU on Google Colab, you are accessing it in the **TPU Node** style.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 2294}, page_content='Using TPU Nodes can have some quite unexpected behaviour for people who aren’t used to them! In particular, because the TPU is located on a physically different system to the machine you’re running your Python code on, your data cannot be local to your machine - any data pipeline that loads from your machine’s internal storage will totally fail! Instead, data must be stored in Google Cloud Storage where your data pipeline can still access it, even when the pipeline is running on the remote TPU'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 2746}, page_content='when the pipeline is running on the remote TPU node'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 2797}, page_content='.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 2800}, page_content='<Tip>\\n\\nIf you can fit all your data in memory as `np.ndarray` or `tf.Tensor`, then you can `fit()` on that data even when using Colab or a TPU Node, without needing to upload it to Google Cloud Storage.\\n\\n</Tip>\\n\\n<Tip>'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 3019}, page_content='**🤗Specific Hugging Face Tip🤗:** The methods `Dataset.to_tf_dataset()` and its higher-level wrapper `model.prepare_tf_dataset()` , which you will see throughout our TF code examples, will both fail on a TPU Node. The reason for this is that even though they create a `tf.data.Dataset` it is not a “pure” `tf.data` pipeline and uses `tf.numpy_function` or `Dataset.from_generator()` to stream data from the underlying HuggingFace `Dataset`'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 3457}, page_content='. This HuggingFace `Dataset` is backed by data that is on a local disc and which the remote TPU Node will not be able to read.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 3585}, page_content='</Tip>\\n\\nThe second way to access a TPU is via a **TPU VM.** When using a TPU VM, you connect directly to the machine that the TPU is attached to, much like training on a GPU VM. TPU VMs are generally easier to work with, particularly when it comes to your data pipeline. All of the above warnings do not apply to TPU VMs!'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 3908}, page_content='This is an opinionated document, so here’s our opinion: **Avoid using TPU Node if possible.** It is more confusing and more difficult to debug than TPU VMs. It is also likely to be unsupported in future - Google’s latest TPU, TPUv4, can only be accessed as a TPU VM, which suggests that TPU Nodes are increasingly going to become a “legacy” access method'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 4262}, page_content='. However, we understand that the only free TPU access is on Colab and Kaggle Kernels, which uses TPU Node - so we’ll try to explain how to handle it if you have to! Check the [TPU example notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/tpu_training-tf.ipynb) for code samples that explain this in more detail.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 4618}, page_content='### What sizes of TPU are available?\\n\\nA single TPU (a v2-8/v3-8/v4-8) runs 8 replicas. TPUs exist in **pods** that can run hundreds or thousands of replicas simultaneously. When you use more than a single TPU but less than a whole pod (for example, a v3-32), your TPU fleet is referred to as a **pod slice.**\\n\\nWhen you access a free TPU via Colab, you generally get a single v2-8 TPU.\\n\\n### I keep hearing about this XLA thing. What’s XLA, and how does it relate to TPUs?'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 5090}, page_content='XLA is an optimizing compiler, used by both TensorFlow and JAX. In JAX it is the only compiler, whereas in TensorFlow it is optional (but mandatory on TPU!). The easiest way to enable it when training a Keras model is to pass the argument `jit_compile=True` to `model.compile()`. If you don’t get any errors and performance is good, that’s a great sign that you’re ready to move to TPU!'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 5478}, page_content='Debugging on TPU is generally a bit harder than on CPU/GPU, so we recommend getting your code running on CPU/GPU with XLA first before trying it on TPU. You don’t have to train for long, of course - just for a few steps to make sure that your model and data pipeline are working like you expect them to.\\n\\n<Tip>'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 5783}, page_content='<Tip>\\n\\nXLA compiled code is usually faster - so even if you’re not planning to run on TPU, adding `jit_compile=True` can improve your performance. Be sure to note the caveats below about XLA compatibility, though!\\n\\n</Tip>\\n\\n<Tip warning={true}>'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 5998}, page_content='</Tip>\\n\\n<Tip warning={true}>\\n\\n**Tip born of painful experience:** Although using `jit_compile=True` is a good way to get a speed boost and test if your CPU/GPU code is XLA-compatible, it can actually cause a lot of problems if you leave it in when actually training on TPU. XLA compilation will happen implicitly on TPU, so remember to remove that line before actually running your code on a TPU!\\n\\n</Tip>\\n\\n### How do I make my model XLA compatible?'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 6404}, page_content='### How do I make my model XLA compatible?\\n\\nIn many cases, your code is probably XLA-compatible already! However, there are a few things that work in normal TensorFlow that don’t work in XLA. We’ve distilled them into three core rules below:\\n\\n<Tip>'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 6647}, page_content='<Tip>\\n\\n**🤗Specific HuggingFace Tip🤗:** We’ve put a lot of effort into rewriting our TensorFlow models and loss functions to be XLA-compatible. Our models and loss functions generally obey rule #1 and #2 by default, so you can skip over them if you’re using `transformers` models. Don’t forget about these rules when writing your own models and loss functions, though!\\n\\n</Tip>\\n\\n#### XLA Rule #1: Your code cannot have “data-dependent conditionals”'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 7095}, page_content='What that means is that any `if` statement cannot depend on values inside a `tf.Tensor`. For example, this code block cannot be compiled with XLA!\\n\\n```python\\nif tf.reduce_sum(tensor) > 10:\\n    tensor = tensor / 2.0\\n```'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 7315}, page_content='This might seem very restrictive at first, but most neural net code doesn’t need to do this. You can often get around this restriction by using `tf.cond` (see the documentation [here](https://www.tensorflow.org/api_docs/python/tf/cond)) or by removing the conditional and finding a clever math trick with indicator variables instead, like so:\\n\\n```python\\nsum_over_10 = tf.cast(tf.reduce_sum(tensor) > 10, tf.float32)\\ntensor = tensor / (1.0 + sum_over_10)\\n```'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 7774}, page_content='This code has exactly the same effect as the code above, but by avoiding a conditional, we ensure it will compile with XLA without problems!\\n\\n#### XLA Rule #2: Your code cannot have “data-dependent shapes”'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 7981}, page_content='What this means is that the shape of all of the `tf.Tensor` objects in your code cannot depend on their values. For example, the function `tf.unique` cannot be compiled with XLA, because it returns a `tensor` containing one instance of each unique value in the input. The shape of this output will obviously be different depending on how repetitive the input `Tensor` was, and so XLA refuses to handle it!'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 8388}, page_content='In general, most neural network code obeys rule #2 by default. However, there are a few common cases where it becomes a problem. One very common one is when you use **label masking**, setting your labels to a negative value to indicate that those positions should be ignored when computing the loss. If you look at NumPy or PyTorch loss functions that support label masking, you will often see code like this that uses [boolean indexing](https://numpy.org/doc/stable/user/basics.indexing'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 8839}, page_content='.org/doc/stable/user/basics.indexing.html#boolean-array-indexing):'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 8907}, page_content='```python\\nlabel_mask = labels >= 0\\nmasked_outputs = outputs[label_mask]\\nmasked_labels = labels[label_mask]\\nloss = compute_loss(masked_outputs, masked_labels)\\nmean_loss = torch.mean(loss)\\n```'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 9099}, page_content='This code is totally fine in NumPy or PyTorch, but it breaks in XLA! Why? Because the shape of `masked_outputs` and `masked_labels` depends on how many positions are masked - that makes it a **data-dependent shape.** However, just like for rule #1, we can often rewrite this code to yield exactly the same output without any data-dependent shapes.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 9448}, page_content='```python\\nlabel_mask = tf.cast(labels >= 0, tf.float32)\\nloss = compute_loss(outputs, labels)\\nloss = loss * label_mask  # Set negative label positions to 0\\nmean_loss = tf.reduce_sum(loss) / tf.reduce_sum(label_mask)\\n```'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 9668}, page_content='Here, we avoid data-dependent shapes by computing the loss for every position, but zeroing out the masked positions in both the numerator and denominator when we calculate the mean, which yields exactly the same result as the first block while maintaining XLA compatibility. Note that we use the same trick as in rule #1 - converting a `tf.bool` to `tf.float32` and using it as an indicator variable. This is a really useful trick, so remember it if you need to convert your own code to XLA!'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 10161}, page_content='#### XLA Rule #3: XLA will need to recompile your model for every different input shape it sees\\n\\nThis is the big one. What this means is that if your input shapes are very variable, XLA will have to recompile your model over and over, which will create huge performance problems. This commonly arises in NLP models, where input texts have variable lengths after tokenization. In other modalities, static shapes are more common and this rule is much less of a problem.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 10630}, page_content='How can you get around rule #3? The key is **padding** - if you pad all your inputs to the same length, and then use an `attention_mask`, you can get the same results as you’d get from variable shapes, but without any XLA issues. However, excessive padding can cause severe slowdown too - if you pad all your samples to the maximum length in the whole dataset, you might end up with batches consisting endless padding tokens, which will waste a lot of compute and memory!'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 11103}, page_content='There isn’t a perfect solution to this problem. However, you can try some tricks. One very useful trick is to **pad batches of samples up to a multiple of a number like 32 or 64 tokens.** This often only increases the number of tokens by a small amount, but it hugely reduces the number of unique input shapes, because every input shape now has to be a multiple of 32 or 64. Fewer unique input shapes means fewer XLA compilations!\\n\\n<Tip>'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 11535}, page_content='<Tip>\\n\\n**🤗Specific HuggingFace Tip🤗:** Our tokenizers and data collators have methods that can help you here. You can use `padding=\"max_length\"` or `padding=\"longest\"` when calling tokenizers to get them to output padded data. Our tokenizers and data collators also have a `pad_to_multiple_of` argument that you can use to reduce the number of unique input shapes you see!\\n\\n</Tip>\\n\\n### How do I actually train my model on TPU?'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 11963}, page_content='Once your training is XLA-compatible and (if you’re using TPU Node / Colab) your dataset has been prepared appropriately, running on TPU is surprisingly easy! All you really need to change in your code is to add a few lines to initialize your TPU, and to ensure that your model and dataset are created inside a `TPUStrategy` scope. Take a look at [our TPU example notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/tpu_training-tf'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 12434}, page_content='.ipynb) to see this in action!'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 12466}, page_content='### Summary\\n\\nThere was a lot in here, so let’s summarize with a quick checklist you can follow when you want to get your model ready for TPU training:'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 12618}, page_content='- Make sure your code follows the three rules of XLA\\n- Compile your model with `jit_compile=True` on CPU/GPU and confirm that you can train it with XLA\\n- Either load your dataset into memory or use a TPU-compatible dataset loading approach (see [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/tpu_training-tf.ipynb))\\n- Migrate your code either to Colab (with accelerator set to “TPU”) or a TPU VM on Google Cloud'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 13076}, page_content='- Add TPU initializer code (see [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/tpu_training-tf.ipynb))\\n- Create your `TPUStrategy` and make sure dataset loading and model creation are inside the `strategy.scope()` (see [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/tpu_training-tf.ipynb))\\n- Don’t forget to take `jit_compile=True` out again when you move to TPU!\\n- 🙏🙏🙏🥺🥺🥺\\n- Call model.fit()\\n- You did it!'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 0}, page_content='!---\\nCopyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\");\\nyou may not use this file except in compliance with the License.\\nYou may obtain a copy of the License at\\n\\n    http://www.apache.org/licenses/LICENSE-2.0'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 238}, page_content='http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \"AS IS\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\nSee the License for the specific language governing permissions and\\nlimitations under the License.\\n-->\\n\\n# Token classification with LayoutLMv3 (PyTorch version)'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 651}, page_content='This directory contains a script, `run_funsd_cord.py`, that can be used to fine-tune (or evaluate) LayoutLMv3 on form understanding datasets, such as [FUNSD](https://guillaumejaume.github.io/FUNSD/) and [CORD](https://github.com/clovaai/cord).\\n\\nThe script `run_funsd_cord.py` leverages the 🤗 Datasets library and the Trainer API. You can easily customize it to your needs.\\n\\n## Fine-tuning on FUNSD'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 1025}, page_content='## Fine-tuning on FUNSD\\n\\nFine-tuning LayoutLMv3 for token classification on [FUNSD](https://guillaumejaume.github.io/FUNSD/) can be done as follows:'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 1175}, page_content='```bash\\npython run_funsd_cord.py \\\\\\n  --model_name_or_path microsoft/layoutlmv3-base \\\\\\n  --dataset_name funsd \\\\\\n  --output_dir layoutlmv3-test \\\\\\n  --do_train \\\\\\n  --do_eval \\\\\\n  --max_steps 1000 \\\\\\n  --evaluation_strategy steps \\\\\\n  --eval_steps 100 \\\\\\n  --learning_rate 1e-5 \\\\\\n  --load_best_model_at_end \\\\\\n  --metric_for_best_model \"eval_f1\" \\\\\\n  --push_to_hub \\\\\\n  --push_to_hub°model_id layoutlmv3-finetuned-funsd\\n```'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 1589}, page_content=\"👀 The resulting model can be found here: https://huggingface.co/nielsr/layoutlmv3-finetuned-funsd. By specifying the `push_to_hub` flag, the model gets uploaded automatically to the hub (regularly), together with a model card, which includes metrics such as precision, recall and F1. Note that you can easily update the model card, as it's just a README file of the respective repo on the hub.\"),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 1984}, page_content='There\\'s also the \"Training metrics\" [tab](https://huggingface.co/nielsr/layoutlmv3-finetuned-funsd/tensorboard), which shows Tensorboard logs over the course of training. Pretty neat, huh?\\n\\n## Fine-tuning on CORD\\n\\nFine-tuning LayoutLMv3 for token classification on [CORD](https://github.com/clovaai/cord) can be done as follows:'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 2314}, page_content='```bash\\npython run_funsd_cord.py \\\\\\n  --model_name_or_path microsoft/layoutlmv3-base \\\\\\n  --dataset_name cord \\\\\\n  --output_dir layoutlmv3-test \\\\\\n  --do_train \\\\\\n  --do_eval \\\\\\n  --max_steps 1000 \\\\\\n  --evaluation_strategy steps \\\\\\n  --eval_steps 100 \\\\\\n  --learning_rate 5e-5 \\\\\\n  --load_best_model_at_end \\\\\\n  --metric_for_best_model \"eval_f1\" \\\\\\n  --push_to_hub \\\\\\n  --push_to_hub°model_id layoutlmv3-finetuned-cord\\n```'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 2726}, page_content='👀 The resulting model can be found here: https://huggingface.co/nielsr/layoutlmv3-finetuned-cord. Note that a model card gets generated automatically in case you specify the `push_to_hub` flag.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 0}, page_content='!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 232}, page_content='http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 583}, page_content='⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer.\\n\\n-->\\n\\n# VisionTextDualEncoder\\n\\n## Overview'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 789}, page_content='The [`VisionTextDualEncoderModel`] can be used to initialize a vision-text dual encoder model with\\nany pretrained vision autoencoding model as the vision encoder (*e.g.* [ViT](vit), [BEiT](beit), [DeiT](deit)) and any pretrained text autoencoding model as the text encoder (*e.g.* [RoBERTa](roberta), [BERT](bert)). Two projection layers are added on top of both the vision and text encoder to project the output embeddings'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 1213}, page_content='to a shared latent space. The projection layers are randomly initialized so the model should be fine-tuned on a\\ndownstream task. This model can be used to align the vision-text embeddings using CLIP like contrastive image-text\\ntraining and then can be used for zero-shot vision tasks such image-classification or retrieval.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 1538}, page_content='In [LiT: Zero-Shot Transfer with Locked-image Text Tuning](https://arxiv.org/abs/2111.07991) it is shown how\\nleveraging pre-trained (locked/frozen) image and text model for contrastive learning yields significant improvement on\\nnew zero-shot vision tasks such as image classification or retrieval.\\n\\n## VisionTextDualEncoderConfig\\n\\n[[autodoc]] VisionTextDualEncoderConfig\\n\\n## VisionTextDualEncoderProcessor\\n\\n[[autodoc]] VisionTextDualEncoderProcessor\\n\\n<frameworkcontent>\\n<pt>'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 1989}, page_content='<frameworkcontent>\\n<pt>\\n\\n## VisionTextDualEncoderModel\\n\\n[[autodoc]] VisionTextDualEncoderModel\\n    - forward\\n\\n</pt>\\n<tf>\\n\\n## FlaxVisionTextDualEncoderModel\\n\\n[[autodoc]] FlaxVisionTextDualEncoderModel\\n    - __call__\\n\\n</tf>\\n<jax>\\n\\n## TFVisionTextDualEncoderModel\\n\\n[[autodoc]] TFVisionTextDualEncoderModel\\n    - call\\n\\n</jax>\\n</frameworkcontent>'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 0}, page_content='!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 232}, page_content='http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 583}, page_content='⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer.\\n\\n-->\\n\\n# Zero-shot object detection\\n\\n[[open-in-colab]]\\n\\nTraditionally, models used for [object detection](object_detection) require labeled image datasets for training,\\nand are limited to detecting the set of classes from the training data.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 986}, page_content='Zero-shot object detection is supported by the [OWL-ViT](../model_doc/owlvit) model which uses a different approach. OWL-ViT\\nis an open-vocabulary object detector. It means that it can detect objects in images based on free-text queries without\\nthe need to fine-tune the model on labeled datasets.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 1285}, page_content='OWL-ViT leverages multi-modal representations to perform open-vocabulary detection. It combines [CLIP](../model_doc/clip) with\\nlightweight object classification and localization heads. Open-vocabulary detection is achieved by embedding free-text queries with the text encoder of CLIP and using them as input to the object classification and localization heads.\\nassociate images and their corresponding textual descriptions, and ViT processes image patches as inputs. The authors'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 1764}, page_content='of OWL-ViT first trained CLIP from scratch and then fine-tuned OWL-ViT end to end on standard object detection datasets using\\na bipartite matching loss.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 1918}, page_content='With this approach, the model can detect objects based on textual descriptions without prior training on labeled datasets.\\n\\nIn this guide, you will learn how to use OWL-ViT:\\n- to detect objects based on text prompts\\n- for batch object detection\\n- for image-guided object detection\\n\\nBefore you begin, make sure you have all the necessary libraries installed:\\n\\n```bash\\npip install -q transformers\\n```\\n\\n## Zero-shot object detection pipeline'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 2318}, page_content='## Zero-shot object detection pipeline\\n\\nThe simplest way to try out inference with OWL-ViT is to use it in a [`pipeline`]. Instantiate a pipeline\\nfor zero-shot object detection from a [checkpoint on the Hugging Face Hub](https://huggingface.co/models?other=owlvit):\\n\\n```python\\n>>> from transformers import pipeline\\n\\n>>> checkpoint = \"google/owlvit-base-patch32\"\\n>>> detector = pipeline(model=checkpoint, task=\"zero-shot-object-detection\")\\n```'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 2762}, page_content='Next, choose an image you\\'d like to detect objects in. Here we\\'ll use the image of astronaut Eileen Collins that is\\na part of the [NASA](https://www.nasa.gov/multimedia/imagegallery/index.html) Great Images dataset.\\n\\n```py\\n>>> import skimage\\n>>> import numpy as np\\n>>> from PIL import Image\\n\\n>>> image = skimage.data.astronaut()\\n>>> image = Image.fromarray(np.uint8(image)).convert(\"RGB\")\\n\\n>>> image\\n```'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 3152}, page_content='>>> image\\n```\\n\\n<div class=\"flex justify-center\">\\n     <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/zero-sh-obj-detection_1.png\" alt=\"Astronaut Eileen Collins\"/>\\n</div>\\n\\nPass the image and the candidate object labels to look for to the pipeline.\\nHere we pass the image directly; other suitable options include a local path to an image or an image url. We also pass text descriptions for all items we want to query the image for.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 3644}, page_content='```py\\n>>> predictions = detector(\\n...     image,\\n...     candidate_labels=[\"human face\", \"rocket\", \"nasa badge\", \"star-spangled banner\"],\\n... )\\n>>> predictions\\n[{\\'score\\': 0.3571370542049408,\\n  \\'label\\': \\'human face\\',\\n  \\'box\\': {\\'xmin\\': 180, \\'ymin\\': 71, \\'xmax\\': 271, \\'ymax\\': 178}},\\n {\\'score\\': 0.28099656105041504,\\n  \\'label\\': \\'nasa badge\\',\\n  \\'box\\': {\\'xmin\\': 129, \\'ymin\\': 348, \\'xmax\\': 206, \\'ymax\\': 427}},\\n {\\'score\\': 0.2110239565372467,\\n  \\'label\\': \\'rocket\\','),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 4077}, page_content=\"'label': 'rocket',\\n  'box': {'xmin': 350, 'ymin': -1, 'xmax': 468, 'ymax': 288}},\\n {'score': 0.13790413737297058,\\n  'label': 'star-spangled banner',\\n  'box': {'xmin': 1, 'ymin': 1, 'xmax': 105, 'ymax': 509}},\\n {'score': 0.11950037628412247,\\n  'label': 'nasa badge',\\n  'box': {'xmin': 277, 'ymin': 338, 'xmax': 327, 'ymax': 380}},\\n {'score': 0.10649408400058746,\\n  'label': 'rocket',\\n  'box': {'xmin': 358, 'ymin': 64, 'xmax': 424, 'ymax': 280}}]\\n```\"),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 4528}, page_content='Let\\'s visualize the predictions:\\n\\n```py\\n>>> from PIL import ImageDraw\\n\\n>>> draw = ImageDraw.Draw(image)\\n\\n>>> for prediction in predictions:\\n...     box = prediction[\"box\"]\\n...     label = prediction[\"label\"]\\n...     score = prediction[\"score\"]\\n\\n...     xmin, ymin, xmax, ymax = box.values()\\n...     draw.rectangle((xmin, ymin, xmax, ymax), outline=\"red\", width=1)\\n...     draw.text((xmin, ymin), f\"{label}: {round(score,2)}\", fill=\"white\")\\n\\n>>> image\\n```'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 4969}, page_content='>>> image\\n```\\n\\n<div class=\"flex justify-center\">\\n     <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/zero-sh-obj-detection_2.png\" alt=\"Visualized predictions on NASA image\"/>\\n</div>\\n\\n## Text-prompted zero-shot object detection by hand\\n\\nNow that you\\'ve seen how to use the zero-shot object detection pipeline, let\\'s replicate the same\\nresult manually.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 5381}, page_content=\"Start by loading the model and associated processor from a [checkpoint on the Hugging Face Hub](https://huggingface.co/models?other=owlvit).\\nHere we'll use the same checkpoint as before:\\n\\n```py\\n>>> from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection\\n\\n>>> model = AutoModelForZeroShotObjectDetection.from_pretrained(checkpoint)\\n>>> processor = AutoProcessor.from_pretrained(checkpoint)\\n```\\n\\nLet's take a different image to switch things up.\\n\\n```py\\n>>> import requests\"),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 5846}, page_content='```py\\n>>> import requests\\n\\n>>> url = \"https://unsplash.com/photos/oj0zeY2Ltk4/download?ixid=MnwxMjA3fDB8MXxzZWFyY2h8MTR8fHBpY25pY3xlbnwwfHx8fDE2Nzc0OTE1NDk&force=true&w=640\"\\n>>> im = Image.open(requests.get(url, stream=True).raw)\\n>>> im\\n```\\n\\n<div class=\"flex justify-center\">\\n     <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/zero-sh-obj-detection_3.png\" alt=\"Beach photo\"/>\\n</div>'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 6291}, page_content='Use the processor to prepare the inputs for the model. The processor combines an image processor that prepares the\\nimage for the model by resizing and normalizing it, and a [`CLIPTokenizer`] that takes care of the text inputs.\\n\\n```py\\n>>> text_queries = [\"hat\", \"book\", \"sunglasses\", \"camera\"]\\n>>> inputs = processor(text=text_queries, images=im, return_tensors=\"pt\")\\n```'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 6663}, page_content='Pass the inputs through the model, post-process, and visualize the results. Since the image processor resized images before\\nfeeding them to the model, you need to use the [`~OwlViTImageProcessor.post_process_object_detection`] method to make sure the predicted bounding\\nboxes have the correct coordinates relative to the original image:\\n\\n```py\\n>>> import torch'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 7001}, page_content='```py\\n>>> import torch\\n\\n>>> with torch.no_grad():\\n...     outputs = model(**inputs)\\n...     target_sizes = torch.tensor([im.size[::-1]])\\n...     results = processor.post_process_object_detection(outputs, threshold=0.1, target_sizes=target_sizes)[0]\\n\\n>>> draw = ImageDraw.Draw(im)\\n\\n>>> scores = results[\"scores\"].tolist()\\n>>> labels = results[\"labels\"].tolist()\\n>>> boxes = results[\"boxes\"].tolist()'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 7401}, page_content='>>> for box, score, label in zip(boxes, scores, labels):\\n...     xmin, ymin, xmax, ymax = box\\n...     draw.rectangle((xmin, ymin, xmax, ymax), outline=\"red\", width=1)\\n...     draw.text((xmin, ymin), f\"{text_queries[label]}: {round(score,2)}\", fill=\"white\")\\n\\n>>> im\\n```\\n\\n<div class=\"flex justify-center\">\\n     <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/zero-sh-obj-detection_4.png\" alt=\"Beach photo with detected objects\"/>\\n</div>'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 7896}, page_content=\"## Batch processing\\n\\nYou can pass multiple sets of images and text queries to search for different (or same) objects in several images.\\nLet's use both an astronaut image and the beach image together.\\nFor batch processing, you should pass text queries as a nested list to the processor and images as lists of PIL images,\\nPyTorch tensors, or NumPy arrays.\"),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 8251}, page_content='```py\\n>>> images = [image, im]\\n>>> text_queries = [\\n...     [\"human face\", \"rocket\", \"nasa badge\", \"star-spangled banner\"],\\n...     [\"hat\", \"book\", \"sunglasses\", \"camera\"],\\n... ]\\n>>> inputs = processor(text=text_queries, images=images, return_tensors=\"pt\")\\n```'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 8513}, page_content=\"Previously for post-processing you passed the single image's size as a tensor, but you can also pass a tuple, or, in case\\nof several images, a list of tuples. Let's create predictions for the two examples, and visualize the second one (`image_idx = 1`).\\n\\n```py\\n>>> with torch.no_grad():\\n...     outputs = model(**inputs)\\n...     target_sizes = [x.size[::-1] for x in images]\\n...     results = processor.post_process_object_detection(outputs, threshold=0.1, target_sizes=target_sizes)\"),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 8998}, page_content='>>> image_idx = 1\\n>>> draw = ImageDraw.Draw(images[image_idx])\\n\\n>>> scores = results[image_idx][\"scores\"].tolist()\\n>>> labels = results[image_idx][\"labels\"].tolist()\\n>>> boxes = results[image_idx][\"boxes\"].tolist()\\n\\n>>> for box, score, label in zip(boxes, scores, labels):\\n...     xmin, ymin, xmax, ymax = box\\n...     draw.rectangle((xmin, ymin, xmax, ymax), outline=\"red\", width=1)\\n...     draw.text((xmin, ymin), f\"{text_queries[image_idx][label]}: {round(score,2)}\", fill=\"white\")'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 9483}, page_content='>>> images[image_idx]\\n```\\n\\n<div class=\"flex justify-center\">\\n     <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/zero-sh-obj-detection_4.png\" alt=\"Beach photo with detected objects\"/>\\n</div>\\n\\n## Image-guided object detection'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 9735}, page_content=\"## Image-guided object detection\\n\\nIn addition to zero-shot object detection with text queries, OWL-ViT offers image-guided object detection. This means\\nyou can use an image query to find similar objects in the target image.\\nUnlike text queries, only a single example image is allowed.\\n\\nLet's take an image with two cats on a couch as a target image, and an image of a single cat\\nas a query:\"),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 10127}, page_content='```py\\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\\n>>> image_target = Image.open(requests.get(url, stream=True).raw)\\n\\n>>> query_url = \"http://images.cocodataset.org/val2017/000000524280.jpg\"\\n>>> query_image = Image.open(requests.get(query_url, stream=True).raw)\\n```\\n\\nLet\\'s take a quick look at the images:\\n\\n```py\\n>>> import matplotlib.pyplot as plt\\n\\n>>> fig, ax = plt.subplots(1, 2)\\n>>> ax[0].imshow(image_target)\\n>>> ax[1].imshow(query_image)\\n```'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 10598}, page_content='<div class=\"flex justify-center\">\\n     <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/zero-sh-obj-detection_5.png\" alt=\"Cats\"/>\\n</div>\\n\\nIn the preprocessing step, instead of text queries, you now need to use `query_images`:\\n\\n```py\\n>>> inputs = processor(images=image_target, query_images=query_image, return_tensors=\"pt\")\\n```'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 10985}, page_content='For predictions, instead of passing the inputs to the model, pass them to [`~OwlViTForObjectDetection.image_guided_detection`]. Draw the predictions\\nas before except now there are no labels.\\n\\n```py\\n>>> with torch.no_grad():\\n...     outputs = model.image_guided_detection(**inputs)\\n...     target_sizes = torch.tensor([image_target.size[::-1]])\\n...     results = processor.post_process_image_guided_detection(outputs=outputs, target_sizes=target_sizes)[0]\\n\\n>>> draw = ImageDraw.Draw(image_target)'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 11441}, page_content='>>> draw = ImageDraw.Draw(image_target)\\n\\n>>> scores = results[\"scores\"].tolist()\\n>>> boxes = results[\"boxes\"].tolist()\\n\\n>>> for box, score, label in zip(boxes, scores, labels):\\n...     xmin, ymin, xmax, ymax = box\\n...     draw.rectangle((xmin, ymin, xmax, ymax), outline=\"white\", width=4)\\n\\n>>> image_target\\n```'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 11731}, page_content='>>> image_target\\n```\\n\\n<div class=\"flex justify-center\">\\n     <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/zero-sh-obj-detection_6.png\" alt=\"Cats with bounding boxes\"/>\\n</div>\\n\\nIf you\\'d like to interactively try out inference with OWL-ViT, check out this demo:\\n\\n<iframe\\n\\tsrc=\"https://adirik-owl-vit.hf.space\"\\n\\tframeborder=\"0\"\\n\\twidth=\"850\"\\n\\theight=\"450\"\\n></iframe>'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 0}, page_content='!---\\nCopyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\");\\nyou may not use this file except in compliance with the License.\\nYou may obtain a copy of the License at\\n\\n    http://www.apache.org/licenses/LICENSE-2.0'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 238}, page_content='http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \"AS IS\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\nSee the License for the specific language governing permissions and\\nlimitations under the License.\\n-->'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 593}, page_content='<p align=\"center\">\\n  <picture>\\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-dark.svg\">\\n    <source media=\"(prefers-color-scheme: light)\" srcset=\"https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-light.svg\">'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 950}, page_content='<img alt=\"Hugging Face Transformers Library\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-light.svg\" width=\"352\" height=\"59\" style=\"max-width: 100%;\">\\n  </picture>\\n  <br/>\\n  <br/>\\n</p>'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 1188}, page_content='<p align=\"center\">\\n    <a href=\"https://circleci.com/gh/huggingface/transformers\">\\n        <img alt=\"Build\" src=\"https://img.shields.io/circleci/build/github/huggingface/transformers/main\">\\n    </a>\\n    <a href=\"https://github.com/huggingface/transformers/blob/main/LICENSE\">\\n        <img alt=\"GitHub\" src=\"https://img.shields.io/github/license/huggingface/transformers.svg?color=blue\">\\n    </a>\\n    <a href=\"https://huggingface.co/docs/transformers/index\">'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 1654}, page_content='<img alt=\"Documentation\" src=\"https://img.shields.io/website/http/huggingface.co/docs/transformers/index.svg?down_color=red&down_message=offline&up_message=online\">\\n    </a>\\n    <a href=\"https://github.com/huggingface/transformers/releases\">\\n        <img alt=\"GitHub release\" src=\"https://img.shields.io/github/release/huggingface/transformers.svg\">\\n    </a>\\n    <a href=\"https://github.com/huggingface/transformers/blob/main/CODE_OF_CONDUCT.md\">'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 2109}, page_content='<img alt=\"Contributor Covenant\" src=\"https://img.shields.io/badge/Contributor%20Covenant-v2.0%20adopted-ff69b4.svg\">\\n    </a>\\n    <a href=\"https://zenodo.org/badge/latestdoi/155220641\"><img src=\"https://zenodo.org/badge/155220641.svg\" alt=\"DOI\"></a>\\n</p>'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 2365}, page_content='<h4 align=\"center\">\\n    <p>\\n        <a href=\"https://github.com/huggingface/transformers/blob/main/README.md\">English</a> |\\n        <a href=\"https://github.com/huggingface/transformers/blob/main/README_zh-hans.md\">简体中文</a> |\\n        <a href=\"https://github.com/huggingface/transformers/blob/main/README_zh-hant.md\">繁體中文</a> |\\n        <a href=\"https://github.com/huggingface/transformers/blob/main/README_ko.md\">한국어</a> |'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 2794}, page_content='<a href=\"https://github.com/huggingface/transformers/blob/main/README_es.md\">Español</a> |\\n        <a href=\"https://github.com/huggingface/transformers/blob/main/README_ja.md\">日本語</a> |\\n        <a href=\"https://github.com/huggingface/transformers/blob/main/README_hd.md\">हिन्दी</a> |\\n        <b>Русский</b>\\n        <a href=\"https://github.com/huggingface/transformers//blob/main/README_te.md\">తెలుగు</a> |\\n    <p>\\n</h4>'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 3215}, page_content='<h3 align=\"center\">\\n    <p>Современное машинное обучение для JAX, PyTorch и TensorFlow</p>\\n</h3>\\n\\n<h3 align=\"center\">\\n    <a href=\"https://hf.co/course\"><img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/course_banner.png\"></a>\\n</h3>\\n\\n🤗 Transformers предоставляет тысячи предварительно обученных моделей для выполнения различных задач, таких как текст, зрение и аудио.\\n\\nЭти модели могут быть применены к:'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 3622}, page_content='Эти модели могут быть применены к:\\n\\n* 📝 Тексту для таких задач, как классификация текстов, извлечение информации, ответы на вопросы, обобщение, перевод, генерация текстов на более чем 100 языках.\\n* 🖼️ Изображениям для задач классификации изображений, обнаружения объектов и сегментации.\\n* 🗣️ Аудио для задач распознавания речи и классификации аудио.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 3973}, page_content='Модели transformers также могут выполнять несколько задач, такие как ответы на табличные вопросы, распознавание оптических символов, извлечение информации из отсканированных документов, классификация видео и ответы на визуальные вопросы.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 4212}, page_content='🤗 Transformers предоставляет API для быстрой загрузки и использования предварительно обученных моделей, их тонкой настройки на собственных датасетах и последующего взаимодействия ими с сообществом на нашем [сайте](https://huggingface.co/models). В то же время каждый python модуль, определяющий архитектуру, полностью автономен и может быть модифицирован для проведения быстрых исследовательских экспериментов.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 4624}, page_content='🤗 Transformers опирается на три самые популярные библиотеки глубокого обучения - [Jax](https://jax.readthedocs.io/en/latest/), [PyTorch](https://pytorch.org/) и [TensorFlow](https://www.tensorflow.org/) - и легко интегрируется между ними. Это позволяет легко обучать модели с помощью одной из них, а затем загружать их для выводов с помощью другой.\\n\\n## Онлайн демонстрация'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 4974}, page_content='## Онлайн демонстрация\\n\\nБольшинство наших моделей можно протестировать непосредственно на их страницах с [сайта](https://huggingface.co/models). Мы также предлагаем [привтаный хостинг моделей, контроль версий и API для выводов](https://huggingface.co/pricing) для публичных и частных моделей.\\n\\nВот несколько примеров:'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 5293}, page_content='В области NLP ( Обработка текстов на естественном языке ):\\n- [Маскированное заполнение слов с помощью BERT](https://huggingface.co/bert-base-uncased?text=Paris+is+the+%5BMASK%5D+of+France)\\n- [Распознавание сущностей с помощью Electra](https://huggingface.co/dbmdz/electra-large-discriminator-finetuned-conll03-english?text=My+name+is+Sarah+and+I+live+in+London+city)\\n- [Генерация текста с помощью GPT-2](https://huggingface.co/gpt2?text=A+long+time+ago%2C+)'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 5751}, page_content='- [Выводы на естественном языке с помощью RoBERTa](https://huggingface.co/roberta-large-mnli?text=The+dog+was+lost.+Nobody+lost+any+animal)'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 5891}, page_content='- [Обобщение с помощью BART](https://huggingface.co/facebook/bart-large-cnn?text=The+tower+is+324+metres+%281%2C063+ft%29+tall%2C+about+the+same+height+as+an+81-storey+building%2C+and+the+tallest+structure+in+Paris.+Its+base+is+square%2C+measuring+125+metres+%28410+ft%29+on+each+side'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 6175}, page_content='.+During+its+construction%2C+the+Eiffel+Tower+surpassed+the+Washington+Monument+to+become+the+tallest+man-made+structure+in+the+world%2C+a+title+it+held+for+41+years+until+the+Chrysler+Building+in+New+York+City+was+finished+in+1930.+It+was+the+first+structure+to+reach+a+height+of+300+metres.+Due+to+the+addition+of+a+broadcasting+aerial+at+the+top+of+the+tower+in+1957%2C+it+is+now+taller+than+the+Chrysler+Building+by+5.2+metres+%2817+ft%29'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 6596}, page_content='.2+metres+%2817+ft%29.+Excluding+transmitters%2C+the+Eiffel+Tower+is+the+second+tallest+free-standing+structure+in+France+after+the+Millau+Viaduct)'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 6744}, page_content='- [Ответы на вопросы с помощью DistilBERT](https://huggingface'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 6806}, page_content='.co/distilbert-base-uncased-distilled-squad?text=Which+name+is+also+used+to+describe+the+Amazon+rainforest+in+English%3F&context=The+Amazon+rainforest+%28Portuguese%3A+Floresta+Amaz%C3%B4nica+or+Amaz%C3%B4nia%3B+Spanish%3A+Selva+Amaz%C3%B3nica%2C+Amazon%C3%ADa+or+usually+Amazonia%3B+French%3A+For%C3%AAt+amazonienne%3B+Dutch%3A+Amazoneregenwoud%29%2C+also+known+in+English+as+Amazonia+or+the+Amazon+Jungle%2C+is+a+moist+broadleaf+forest+that+covers+most+of+the+Amazon+basin+of+South+America'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 7297}, page_content='.+This+basin+encompasses+7%2C000%2C000+square+kilometres+%282%2C700%2C000+sq+mi%29%2C+of+which+5%2C500%2C000+square+kilometres+%282%2C100%2C000+sq+mi%29+are+covered+by+the+rainforest.+This+region+includes+territory+belonging+to+nine+nations.+The+majority+of+the+forest+is+contained+within+Brazil%2C+with+60%25+of+the+rainforest%2C+followed+by+Peru+with+13%25%2C+Colombia+with+10%25%2C+and+with+minor+amounts+in+Venezuela%2C+Ecuador%2C+Bolivia%2C+Guyana%2C+Suriname+and+French+Guiana'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 7779}, page_content='.+States+or+departments+in+four+nations+contain+%22Amazonas%22+in+their+names.+The+Amazon+represents+over+half+of+the+planet%27s+remaining+rainforests%2C+and+comprises+the+largest+and+most+biodiverse+tract+of+tropical+rainforest+in+the+world%2C+with+an+estimated+390+billion+individual+trees+divided+into+16%2C000+species)'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 8102}, page_content='- [Перевод с помощью T5](https://huggingface.co/t5-base?text=My+name+is+Wolfgang+and+I+live+in+Berlin)'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 8206}, page_content='В области компьютерного зрения:\\n- [Классификация изображений с помощью ViT](https://huggingface.co/google/vit-base-patch16-224)\\n- [Обнаружение объектов с помощью DETR](https://huggingface.co/facebook/detr-resnet-50)\\n- [Семантическая сегментация с помощью SegFormer](https://huggingface.co/nvidia/segformer-b0-finetuned-ade-512-512)\\n- [Сегментация паноптикума с помощью MaskFormer](https://huggingface.co/facebook/maskformer-swin-small-coco)'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 8647}, page_content='- [Оценка глубины с помощью DPT](https://huggingface.co/docs/transformers/model_doc/dpt)\\n- [Классификация видео с помощью VideoMAE](https://huggingface.co/docs/transformers/model_doc/videomae)\\n- [Универсальная сегментация с помощью OneFormer](https://huggingface.co/shi-labs/oneformer_ade20k_dinat_large)'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 8953}, page_content='В области звука:\\n- [Автоматическое распознавание речи с помощью Wav2Vec2](https://huggingface.co/facebook/wav2vec2-base-960h)\\n- [Поиск ключевых слов с помощью Wav2Vec2](https://huggingface.co/superb/wav2vec2-base-superb-ks)\\n- [Классификация аудиоданных с помощью траснформера аудиоспектрограмм](https://huggingface.co/MIT/ast-finetuned-audioset-10-10-0.4593)'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 9313}, page_content='В мультимодальных задачах:\\n- [Ответы на вопросы по таблице с помощью TAPAS](https://huggingface.co/google/tapas-base-finetuned-wtq)\\n- [Визуальные ответы на вопросы с помощью ViLT](https://huggingface.co/dandelin/vilt-b32-finetuned-vqa)\\n- [Zero-shot классификация изображений с помощью CLIP](https://huggingface.co/openai/clip-vit-large-patch14)\\n- [Ответы на вопросы по документам с помощью LayoutLM](https://huggingface.co/impira/layoutlm-document-qa)'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 9765}, page_content='- [Zero-shot классификация видео с помощью X-CLIP](https://huggingface.co/docs/transformers/model_doc/xclip)'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 9876}, page_content='## 100 проектов, использующих Transformers\\n\\nTransformers - это не просто набор инструментов для использования предварительно обученных моделей: это сообщество проектов, созданное на его основе, и\\nHugging Face Hub. Мы хотим, чтобы Transformers позволил разработчикам, исследователям, студентам, профессорам, инженерам и всем желающим\\nсоздавать проекты своей мечты.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 10241}, page_content='Чтобы отпраздновать 100 тысяч звезд Transformers, мы решили сделать акцент на сообществе, и создали страницу [awesome-transformers](./awesome-transformers.md), на которой перечислены 100\\nневероятных проектов, созданных с помощью transformers.\\n\\nЕсли вы являетесь владельцем или пользователем проекта, который, по вашему мнению, должен быть включен в этот список, пожалуйста, откройте PR для его добавления!\\n\\n## Если вы хотите получить индивидуальную поддержку от команды Hugging Face'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 10725}, page_content='<a target=\"_blank\" href=\"https://huggingface.co/support\">\\n    <img alt=\"HuggingFace Expert Acceleration Program\" src=\"https://cdn-media.huggingface.co/marketing/transformers/new-support-improved.png\" style=\"max-width: 600px; border: 1px solid #eee; border-radius: 4px; box-shadow: 0 1px 2px 0 rgba(0, 0, 0, 0.05);\">\\n</a><br>\\n\\n## Быстрый гайд'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 11051}, page_content='## Быстрый гайд\\n\\nДля использования модели на заданном входе (текст, изображение, звук, ...) мы предоставляем API `pipeline`. Конвейеры объединяют предварительно обученную модель с препроцессингом, который использовался при ее обучении. Вот как можно быстро использовать конвейер для классификации положительных и отрицательных текстов:\\n\\n```python\\n>>> from transformers import pipeline'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 11388}, page_content='```python\\n>>> from transformers import pipeline\\n\\n# Выделение конвейера для анализа настроений\\n>>> classifier = pipeline(\\'sentiment-analysis\\')\\n>>> classifier(\\'Мы очень рады представить конвейер в transformers.\\')\\n[{\\'label\\': \\'POSITIVE\\', \\'score\\': 0.9996980428695679}]\\n```\\n\\nВторая строка кода загружает и кэширует предварительно обученную модель, используемую конвейером, а третья оценивает ее на заданном тексте. Здесь ответ \"POSITIVE\" с уверенностью 99,97%.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 11844}, page_content='Во многих задачах, как в НЛП, так и в компьютерном зрении и речи, уже есть готовый `pipeline`. Например, мы можем легко извлечь обнаруженные объекты на изображении:\\n\\n``` python\\n>>> import requests\\n>>> from PIL import Image\\n>>> from transformers import pipeline\\n\\n# Скачиваем изображение с милыми котиками\\n>>> url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/coco_sample.png\"\\n>>> image_data = requests.get(url, stream=True).raw\\n>>> image = Image.open(image_data)'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 12342}, page_content=\"# Выделение конвейера для обнаружения объектов\\n>>> object_detector = pipeline('object-detection')\\n>>> object_detector(image)\\n[{'score': 0.9982201457023621,\\n  'label': 'remote',\\n  'box': {'xmin': 40, 'ymin': 70, 'xmax': 175, 'ymax': 117}},\\n {'score': 0.9960021376609802,\\n  'label': 'remote',\\n  'box': {'xmin': 333, 'ymin': 72, 'xmax': 368, 'ymax': 187}},\\n {'score': 0.9954745173454285,\\n  'label': 'couch',\\n  'box': {'xmin': 0, 'ymin': 1, 'xmax': 639, 'ymax': 473}},\\n {'score': 0.9988006353378296,\"),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 12808}, page_content=\"{'score': 0.9988006353378296,\\n  'label': 'cat',\\n  'box': {'xmin': 13, 'ymin': 52, 'xmax': 314, 'ymax': 470}},\\n {'score': 0.9986783862113953,\\n  'label': 'cat',\\n  'box': {'xmin': 345, 'ymin': 23, 'xmax': 640, 'ymax': 368}}]\\n```\"),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 13035}, page_content='Здесь мы получаем список объектов, обнаруженных на изображении, с рамкой вокруг объекта и оценкой достоверности. Слева - исходное изображение, справа прогнозы:\\n\\n<h3 align=\"center\">\\n    <a><img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/coco_sample.png\" width=\"400\"></a>\\n    <a><img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/coco_sample_post_processed.png\" width=\"400\"></a>\\n</h3>'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 13496}, page_content='Подробнее о задачах, поддерживаемых API `pipeline`, можно узнать в [этом учебном пособии](https://huggingface.co/docs/transformers/task_sum)\\n\\nВ дополнение к `pipeline`, для загрузки и использования любой из предварительно обученных моделей в заданной задаче достаточно трех строк кода. Вот версия для PyTorch:\\n```python\\n>>> from transformers import AutoTokenizer, AutoModel'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 13871}, page_content='>>> tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\\n>>> model = AutoModel.from_pretrained(\"bert-base-uncased\")\\n\\n>>> inputs = tokenizer(\"Привет мир!\", return_tensors=\"pt\")\\n>>> outputs = model(**inputs)\\n```\\n\\nА вот эквивалентный код для TensorFlow:\\n```python\\n>>> from transformers import AutoTokenizer, TFAutoModel\\n\\n>>> tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\\n>>> model = TFAutoModel.from_pretrained(\"bert-base-uncased\")'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 14328}, page_content='>>> inputs = tokenizer(\"Привет мир!\", return_tensors=\"tf\")\\n>>> outputs = model(**inputs)\\n```\\n\\nТокенизатор отвечает за всю предварительную обработку, которую ожидает предварительно обученная модель, и может быть вызван непосредственно с помощью одной строки (как в приведенных выше примерах) или на списке. В результате будет получен словарь, который можно использовать в последующем коде или просто напрямую передать в модель с помощью оператора распаковки аргументов **.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 14801}, page_content='Сама модель представляет собой обычный [Pytorch `nn.Module`](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) или [TensorFlow `tf.keras.Model`](https://www.tensorflow.org/api_docs/python/tf/keras/Model) (в зависимости от используемого бэкенда), который можно использовать как обычно. [В этом руководстве](https://huggingface'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 15091}, page_content='. [В этом руководстве](https://huggingface.co/docs/transformers/training) рассказывается, как интегрировать такую модель в классический цикл обучения PyTorch или TensorFlow, или как использовать наш API `Trainer` для быстрой тонкой настройки на новом датасете.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 15353}, page_content='## Почему необходимо использовать transformers?\\n\\n1. Простые в использовании современные модели:\\n    - Высокая производительность в задачах понимания и генерации естественного языка, компьютерного зрения и аудио.\\n    - Низкий входной барьер для преподавателей и практиков.\\n    - Небольшое количество абстракций для пользователя и всего три класса для изучения.\\n    - Единый API для использования всех наших предварительно обученных моделей.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 15794}, page_content='1. Более низкие вычислительные затраты, меньший \"углеродный след\":\\n    - Исследователи могут обмениваться обученными моделями вместо того, чтобы постоянно их переобучать.\\n    - Практики могут сократить время вычислений и производственные затраты.\\n    - Десятки архитектур с более чем 60 000 предварительно обученных моделей для всех модальностей.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 16142}, page_content='1. Выбор подходящего фреймворка для каждого этапа жизни модели:\\n    - Обучение самых современных моделей за 3 строки кода.\\n    - Перемещайте одну модель между фреймворками TF2.0/PyTorch/JAX по своему усмотрению.\\n    - Беспрепятственный выбор подходящего фреймворка для обучения, оценки и производства.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 16445}, page_content='1. Легко настроить модель или пример под свои нужды:\\n    - Мы предоставляем примеры для каждой архитектуры, чтобы воспроизвести результаты, опубликованные их авторами.\\n    - Внутренние компоненты модели раскрываются максимально последовательно.\\n    - Файлы моделей можно использовать независимо от библиотеки для проведения быстрых экспериментов.\\n\\n## Почему я не должен использовать transformers?'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 16843}, page_content='- Данная библиотека не является модульным набором строительных блоков для нейронных сетей. Код в файлах моделей специально не рефакторится дополнительными абстракциями, чтобы исследователи могли быстро итеративно работать с каждой из моделей, не погружаясь в дополнительные абстракции/файлы.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 17135}, page_content='- API обучения не предназначен для работы с любой моделью, а оптимизирован для работы с моделями, предоставляемыми библиотекой. Для работы с общими циклами машинного обучения следует использовать другую библиотеку (возможно, [Accelerate](https://huggingface.co/docs/accelerate)).'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 17415}, page_content='- Несмотря на то, что мы стремимся представить как можно больше примеров использования, скрипты в нашей папке [примеров](https://github.com/huggingface/transformers/tree/main/examples) являются именно примерами. Предполагается, что они не будут работать \"из коробки\" для решения вашей конкретной задачи, и вам придется изменить несколько строк кода, чтобы адаптировать их под свои нужды.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 17804}, page_content='## Установка\\n\\n### С помощью pip\\n\\nДанный репозиторий протестирован на Python 3.8+, Flax 0.4.1+, PyTorch 1.10+ и TensorFlow 2.6+.\\n\\nУстанавливать 🤗 Transformers следует в [виртуальной среде](https://docs.python.org/3/library/venv.html). Если вы не знакомы с виртуальными средами Python, ознакомьтесь с [руководством пользователя](https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/).'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 18217}, page_content='Сначала создайте виртуальную среду с той версией Python, которую вы собираетесь использовать, и активируйте ее.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 18330}, page_content='Затем необходимо установить хотя бы один бекенд из Flax, PyTorch или TensorFlow.\\nПожалуйста, обратитесь к страницам [TensorFlow установочная страница](https://www.tensorflow.org/install/), [PyTorch установочная страница](https://pytorch.org/get-started/locally/#start-locally) и/или [Flax](https://github.com/google/flax#quick-install) и [Jax](https://github.com/google/jax#installation), где описаны команды установки для вашей платформы.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 18771}, page_content='После установки одного из этих бэкендов 🤗 Transformers может быть установлен с помощью pip следующим образом:\\n\\n```bash\\npip install transformers\\n```\\n\\nЕсли вы хотите поиграть с примерами или вам нужен самый современный код и вы не можете ждать нового релиза, вы должны [установить библиотеку из исходного кода](https://huggingface.co/docs/transformers/installation#installing-from-source).\\n\\n### С помощью conda\\n\\nНачиная с версии Transformers v4.0.0, у нас появилсась поддержка conda: `huggingface`.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 19269}, page_content='Установить Transformers с помощью conda можно следующим образом:\\n\\n```bash\\nconda install -c huggingface transformers\\n```\\n\\nО том, как установить Flax, PyTorch или TensorFlow с помощью conda, читайте на страницах, посвященных их установке.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 19507}, page_content='> **_ЗАМЕТКА:_** В операционной системе Windows вам может быть предложено активировать режим разработчика, чтобы воспользоваться преимуществами кэширования. Если для вас это невозможно, сообщите нам об этом [здесь](https://github.com/huggingface/huggingface_hub/issues/1062).\\n\\n## Модельные архитектуры'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 19784}, page_content='## Модельные архитектуры\\n\\n**[Все контрольные точки моделей](https://huggingface.co/models)**, предоставляемые 🤗 Transformers, беспрепятственно интегрируются с huggingface.co [model hub](https://huggingface.co/models), куда они загружаются непосредственно [пользователями](https://huggingface.co/users) и [организациями](https://huggingface.co/organizations).\\n\\nТекущее количество контрольных точек: ![](https://img.shields.io/endpoint?url=https://huggingface.co/api/shields/models&color=brightgreen)'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 20284}, page_content='🤗 В настоящее время Transformers предоставляет следующие архитектуры (подробное описание каждой из них см. [здесь](https://huggingface.co/docs/transformers/model_summary)):'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 20458}, page_content='1. **[ALBERT](https://huggingface.co/docs/transformers/model_doc/albert)** (from Google Research and the Toyota Technological Institute at Chicago) released with the paper [ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://arxiv.org/abs/1909.11942), by Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 20837}, page_content='1. **[ALIGN](https://huggingface.co/docs/transformers/model_doc/align)** (from Google Research) released with the paper [Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision](https://arxiv.org/abs/2102.05918) by Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V. Le, Yunhsuan Sung, Zhen Li, Tom Duerig.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 21205}, page_content='1. **[AltCLIP](https://huggingface.co/docs/transformers/model_doc/altclip)** (from BAAI) released with the paper [AltCLIP: Altering the Language Encoder in CLIP for Extended Language Capabilities](https://arxiv.org/abs/2211.06679) by Chen, Zhongzhi and Liu, Guang and Zhang, Bo-Wen and Ye, Fulong and Yang, Qinghong and Wu, Ledell.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 21537}, page_content='1. **[Audio Spectrogram Transformer](https://huggingface.co/docs/transformers/model_doc/audio-spectrogram-transformer)** (from MIT) released with the paper [AST: Audio Spectrogram Transformer](https://arxiv.org/abs/2104.01778) by Yuan Gong, Yu-An Chung, James Glass.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 21804}, page_content='1. **[Autoformer](https://huggingface.co/docs/transformers/model_doc/autoformer)** (from Tsinghua University) released with the paper [Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting](https://arxiv.org/abs/2106.13008) by Haixu Wu, Jiehui Xu, Jianmin Wang, Mingsheng Long.\\n1. **[Bark](https://huggingface.co/docs/transformers/model_doc/bark)** (from Suno) released in the repository [suno-ai/bark](https://github.com/suno-ai/bark) by Suno AI team.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 22297}, page_content='1. **[BART](https://huggingface.co/docs/transformers/model_doc/bart)** (from Facebook) released with the paper [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/abs/1910.13461) by Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov and Luke Zettlemoyer.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 22687}, page_content='1. **[BARThez](https://huggingface.co/docs/transformers/model_doc/barthez)** (from École polytechnique) released with the paper [BARThez: a Skilled Pretrained French Sequence-to-Sequence Model](https://arxiv.org/abs/2010.12321) by Moussa Kamal Eddine, Antoine J.-P. Tixier, Michalis Vazirgiannis.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 22984}, page_content='1. **[BARTpho](https://huggingface.co/docs/transformers/model_doc/bartpho)** (from VinAI Research) released with the paper [BARTpho: Pre-trained Sequence-to-Sequence Models for Vietnamese](https://arxiv.org/abs/2109.09701) by Nguyen Luong Tran, Duong Minh Le and Dat Quoc Nguyen.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 23264}, page_content='1. **[BEiT](https://huggingface.co/docs/transformers/model_doc/beit)** (from Microsoft) released with the paper [BEiT: BERT Pre-Training of Image Transformers](https://arxiv.org/abs/2106.08254) by Hangbo Bao, Li Dong, Furu Wei.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 23492}, page_content='1. **[BERT](https://huggingface.co/docs/transformers/model_doc/bert)** (from Google) released with the paper [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) by Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 23786}, page_content='1. **[BERT For Sequence Generation](https://huggingface.co/docs/transformers/model_doc/bert-generation)** (from Google) released with the paper [Leveraging Pre-trained Checkpoints for Sequence Generation Tasks](https://arxiv.org/abs/1907.12461) by Sascha Rothe, Shashi Narayan, Aliaksei Severyn.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 24082}, page_content='1. **[BERTweet](https://huggingface.co/docs/transformers/model_doc/bertweet)** (from VinAI Research) released with the paper [BERTweet: A pre-trained language model for English Tweets](https://aclanthology.org/2020.emnlp-demos.2/) by Dat Quoc Nguyen, Thanh Vu and Anh Tuan Nguyen.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 24363}, page_content='1. **[BigBird-Pegasus](https://huggingface.co/docs/transformers/model_doc/bigbird_pegasus)** (from Google Research) released with the paper [Big Bird: Transformers for Longer Sequences](https://arxiv.org/abs/2007.14062) by Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, Amr Ahmed.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 24743}, page_content='1. **[BigBird-RoBERTa](https://huggingface.co/docs/transformers/model_doc/big_bird)** (from Google Research) released with the paper [Big Bird: Transformers for Longer Sequences](https://arxiv.org/abs/2007.14062) by Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, Amr Ahmed.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 25116}, page_content='1. **[BioGpt](https://huggingface.co/docs/transformers/model_doc/biogpt)** (from Microsoft Research AI4Science) released with the paper [BioGPT: generative pre-trained transformer for biomedical text generation and mining](https://academic.oup.com/bib/advance-article/doi/10.1093/bib/bbac409/6713511?guestAccessKey=a66d9b5d-4f83-4017-bb52-405815c907b9) by Renqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang, Hoifung Poon and Tie-Yan Liu.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 25559}, page_content='1. **[BiT](https://huggingface.co/docs/transformers/model_doc/bit)** (from Google AI) released with the paper [Big Transfer (BiT): General Visual Representation Learning](https://arxiv.org/abs/1912.11370) by Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly, Neil Houlsby.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 25876}, page_content='1. **[Blenderbot](https://huggingface.co/docs/transformers/model_doc/blenderbot)** (from Facebook) released with the paper [Recipes for building an open-domain chatbot](https://arxiv.org/abs/2004.13637) by Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu, Jing Xu, Myle Ott, Kurt Shuster, Eric M. Smith, Y-Lan Boureau, Jason Weston.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 26237}, page_content='1. **[BlenderbotSmall](https://huggingface.co/docs/transformers/model_doc/blenderbot-small)** (from Facebook) released with the paper [Recipes for building an open-domain chatbot](https://arxiv.org/abs/2004.13637) by Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu, Jing Xu, Myle Ott, Kurt Shuster, Eric M. Smith, Y-Lan Boureau, Jason Weston.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 26609}, page_content='1. **[BLIP](https://huggingface.co/docs/transformers/model_doc/blip)** (from Salesforce) released with the paper [BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation](https://arxiv.org/abs/2201.12086) by Junnan Li, Dongxu Li, Caiming Xiong, Steven Hoi.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 26915}, page_content='1. **[BLIP-2](https://huggingface.co/docs/transformers/model_doc/blip-2)** (from Salesforce) released with the paper [BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models](https://arxiv.org/abs/2301.12597) by Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi.\\n1. **[BLOOM](https://huggingface.co/docs/transformers/model_doc/bloom)** (from BigScience workshop) released by the [BigScience Workshop](https://bigscience.huggingface.co/).'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 27400}, page_content='1. **[BORT](https://huggingface.co/docs/transformers/model_doc/bort)** (from Alexa) released with the paper [Optimal Subarchitecture Extraction For BERT](https://arxiv.org/abs/2010.10499) by Adrian de Wynter and Daniel J. Perry.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 27629}, page_content='1. **[BridgeTower](https://huggingface.co/docs/transformers/model_doc/bridgetower)** (from Harbin Institute of Technology/Microsoft Research Asia/Intel Labs) released with the paper [BridgeTower: Building Bridges Between Encoders in Vision-Language Representation Learning](https://arxiv.org/abs/2206.08657) by Xiao Xu, Chenfei Wu, Shachar Rosenman, Vasudev Lal, Wanxiang Che, Nan Duan.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 28016}, page_content='1. **[BROS](https://huggingface.co/docs/transformers/model_doc/bros)** (from NAVER CLOVA) released with the paper [BROS: A Pre-trained Language Model Focusing on Text and Layout for Better Key Information Extraction from Documents](https://arxiv.org/abs/2108.04539) by Teakgyu Hong, Donghyun Kim, Mingi Ji, Wonseok Hwang, Daehyun Nam, Sungrae Park.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 28365}, page_content='1. **[ByT5](https://huggingface.co/docs/transformers/model_doc/byt5)** (from Google Research) released with the paper [ByT5: Towards a token-free future with pre-trained byte-to-byte models](https://arxiv.org/abs/2105.13626) by Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, Colin Raffel.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 28704}, page_content='1. **[CamemBERT](https://huggingface.co/docs/transformers/model_doc/camembert)** (from Inria/Facebook/Sorbonne) released with the paper [CamemBERT: a Tasty French Language Model](https://arxiv.org/abs/1911.03894) by Louis Martin*, Benjamin Muller*, Pedro Javier Ortiz Suárez*, Yoann Dupont, Laurent Romary, Éric Villemonte de la Clergerie, Djamé Seddah and Benoît Sagot.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 29075}, page_content='1. **[CANINE](https://huggingface.co/docs/transformers/model_doc/canine)** (from Google Research) released with the paper [CANINE: Pre-training an Efficient Tokenization-Free Encoder for Language Representation](https://arxiv.org/abs/2103.06874) by Jonathan H. Clark, Dan Garrette, Iulia Turc, John Wieting.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 29383}, page_content='1. **[Chinese-CLIP](https://huggingface.co/docs/transformers/model_doc/chinese_clip)** (from OFA-Sys) released with the paper [Chinese CLIP: Contrastive Vision-Language Pretraining in Chinese](https://arxiv.org/abs/2211.01335) by An Yang, Junshu Pan, Junyang Lin, Rui Men, Yichang Zhang, Jingren Zhou, Chang Zhou.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 29697}, page_content='1. **[CLAP](https://huggingface.co/docs/transformers/model_doc/clap)** (from LAION-AI) released with the paper [Large-scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation](https://arxiv.org/abs/2211.06687) by Yusong Wu, Ke Chen, Tianyu Zhang, Yuchen Hui, Taylor Berg-Kirkpatrick, Shlomo Dubnov.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 30040}, page_content='1. **[CLIP](https://huggingface.co/docs/transformers/model_doc/clip)** (from OpenAI) released with the paper [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020) by Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 30440}, page_content='1. **[CLIPSeg](https://huggingface.co/docs/transformers/model_doc/clipseg)** (from University of Göttingen) released with the paper [Image Segmentation Using Text and Image Prompts](https://arxiv.org/abs/2112.10003) by Timo Lüddecke and Alexander Ecker.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 30694}, page_content='1. **[CodeGen](https://huggingface.co/docs/transformers/model_doc/codegen)** (from Salesforce) released with the paper [A Conversational Paradigm for Program Synthesis](https://arxiv.org/abs/2203.13474) by Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, Caiming Xiong.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 31005}, page_content='1. **[CodeLlama](https://huggingface.co/docs/transformers/model_doc/llama_code)** (from MetaAI) released with the paper [Code Llama: Open Foundation Models for Code](https://ai.meta'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 31181}, page_content='.meta.com/research/publications/code-llama-open-foundation-models-for-code/) by Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Défossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, Gabriel Synnaeve.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 31647}, page_content='1. **[Conditional DETR](https://huggingface.co/docs/transformers/model_doc/conditional_detr)** (from Microsoft Research Asia) released with the paper [Conditional DETR for Fast Training Convergence](https://arxiv.org/abs/2108.06152) by Depu Meng, Xiaokang Chen, Zejia Fan, Gang Zeng, Houqiang Li, Yuhui Yuan, Lei Sun, Jingdong Wang.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 31980}, page_content='1. **[ConvBERT](https://huggingface.co/docs/transformers/model_doc/convbert)** (from YituTech) released with the paper [ConvBERT: Improving BERT with Span-based Dynamic Convolution](https://arxiv.org/abs/2008.02496) by Zihang Jiang, Weihao Yu, Daquan Zhou, Yunpeng Chen, Jiashi Feng, Shuicheng Yan.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 32279}, page_content='1. **[ConvNeXT](https://huggingface.co/docs/transformers/model_doc/convnext)** (from Facebook AI) released with the paper [A ConvNet for the 2020s](https://arxiv.org/abs/2201.03545) by Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, Saining Xie.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 32555}, page_content='1. **[ConvNeXTV2](https://huggingface.co/docs/transformers/model_doc/convnextv2)** (from Facebook AI) released with the paper [ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders](https://arxiv.org/abs/2301.00808) by Sanghyun Woo, Shoubhik Debnath, Ronghang Hu, Xinlei Chen, Zhuang Liu, In So Kweon, Saining Xie.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 32888}, page_content='1. **[CPM](https://huggingface.co/docs/transformers/model_doc/cpm)** (from Tsinghua University) released with the paper [CPM: A Large-scale Generative Chinese Pre-trained Language Model](https://arxiv.org/abs/2012'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 33088}, page_content='.org/abs/2012.00413) by Zhengyan Zhang, Xu Han, Hao Zhou, Pei Ke, Yuxian Gu, Deming Ye, Yujia Qin, Yusheng Su, Haozhe Ji, Jian Guan, Fanchao Qi, Xiaozhi Wang, Yanan Zheng, Guoyang Zeng, Huanqi Cao, Shengqi Chen, Daixuan Li, Zhenbo Sun, Zhiyuan Liu, Minlie Huang, Wentao Han, Jie Tang, Juanzi Li, Xiaoyan Zhu, Maosong Sun.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 33410}, page_content='1. **[CPM-Ant](https://huggingface.co/docs/transformers/model_doc/cpmant)** (from OpenBMB) released by the [OpenBMB](https://www.openbmb.org/).\\n1. **[CTRL](https://huggingface.co/docs/transformers/model_doc/ctrl)** (from Salesforce) released with the paper [CTRL: A Conditional Transformer Language Model for Controllable Generation](https://arxiv.org/abs/1909.05858) by Nitish Shirish Keskar*, Bryan McCann*, Lav R. Varshney, Caiming Xiong and Richard Socher.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 33871}, page_content='1. **[CvT](https://huggingface.co/docs/transformers/model_doc/cvt)** (from Microsoft) released with the paper [CvT: Introducing Convolutions to Vision Transformers](https://arxiv.org/abs/2103.15808) by Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, Lei Zhang.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 34155}, page_content='1. **[Data2Vec](https://huggingface.co/docs/transformers/model_doc/data2vec)** (from Facebook) released with the paper [Data2Vec:  A General Framework for Self-supervised Learning in Speech, Vision and Language](https://arxiv.org/abs/2202.03555) by Alexei Baevski, Wei-Ning Hsu, Qiantong Xu, Arun Babu, Jiatao Gu, Michael Auli.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 34483}, page_content='1. **[DeBERTa](https://huggingface.co/docs/transformers/model_doc/deberta)** (from Microsoft) released with the paper [DeBERTa: Decoding-enhanced BERT with Disentangled Attention](https://arxiv.org/abs/2006.03654) by Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 34755}, page_content='1. **[DeBERTa-v2](https://huggingface.co/docs/transformers/model_doc/deberta-v2)** (from Microsoft) released with the paper [DeBERTa: Decoding-enhanced BERT with Disentangled Attention](https://arxiv.org/abs/2006.03654) by Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 35033}, page_content='1. **[Decision Transformer](https://huggingface.co/docs/transformers/model_doc/decision_transformer)** (from Berkeley/Facebook/Google) released with the paper [Decision Transformer: Reinforcement Learning via Sequence Modeling](https://arxiv.org/abs/2106.01345) by Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael Laskin, Pieter Abbeel, Aravind Srinivas, Igor Mordatch.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 35429}, page_content='1. **[Deformable DETR](https://huggingface.co/docs/transformers/model_doc/deformable_detr)** (from SenseTime Research) released with the paper [Deformable DETR: Deformable Transformers for End-to-End Object Detection](https://arxiv.org/abs/2010.04159) by Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, Jifeng Dai.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 35752}, page_content='1. **[DeiT](https://huggingface.co/docs/transformers/model_doc/deit)** (from Facebook) released with the paper [Training data-efficient image transformers & distillation through attention](https://arxiv.org/abs/2012.12877) by Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, Hervé Jégou.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 36077}, page_content='1. **[DePlot](https://huggingface.co/docs/transformers/model_doc/deplot)** (from Google AI) released with the paper [DePlot: One-shot visual language reasoning by plot-to-table translation](https://arxiv.org/abs/2212.10505) by Fangyu Liu, Julian Martin Eisenschlos, Francesco Piccinno, Syrine Krichene, Chenxi Pang, Kenton Lee, Mandar Joshi, Wenhu Chen, Nigel Collier, Yasemin Altun.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 36461}, page_content='1. **[DETA](https://huggingface.co/docs/transformers/model_doc/deta)** (from The University of Texas at Austin) released with the paper [NMS Strikes Back](https://arxiv.org/abs/2212.06137) by Jeffrey Ouyang-Zhang, Jang Hyun Cho, Xingyi Zhou, Philipp Krähenbühl.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 36723}, page_content='1. **[DETR](https://huggingface.co/docs/transformers/model_doc/detr)** (from Facebook) released with the paper [End-to-End Object Detection with Transformers](https://arxiv.org/abs/2005.12872) by Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, Sergey Zagoruyko.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 37025}, page_content='1. **[DialoGPT](https://huggingface.co/docs/transformers/model_doc/dialogpt)** (from Microsoft Research) released with the paper [DialoGPT: Large-Scale Generative Pre-training for Conversational Response Generation](https://arxiv.org/abs/1911.00536) by Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu, Bill Dolan.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 37398}, page_content='1. **[DiNAT](https://huggingface.co/docs/transformers/model_doc/dinat)** (from SHI Labs) released with the paper [Dilated Neighborhood Attention Transformer](https://arxiv.org/abs/2209.15001) by Ali Hassani and Humphrey Shi.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 37623}, page_content='1. **[DINOv2](https://huggingface.co/docs/transformers/model_doc/dinov2)** (from Meta AI) released with the paper [DINOv2: Learning Robust Visual Features without Supervision](https://arxiv.org/abs/2304'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 37812}, page_content='.org/abs/2304.07193) by Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mahmoud Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Hervé Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, Piotr Bojanowski.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 38232}, page_content='1. **[DistilBERT](https://huggingface.co/docs/transformers/model_doc/distilbert)** (from HuggingFace), released together with the paper [DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter](https://arxiv.org/abs/1910.01108) by Victor Sanh, Lysandre Debut and Thomas Wolf. The same method has been applied to compress GPT2 into [DistilGPT2](https://github'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 38612}, page_content='.com/huggingface/transformers/tree/main/examples/research_projects/distillation), RoBERTa into [DistilRoBERTa](https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation), Multilingual BERT into [DistilmBERT](https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation) and a German version of DistilBERT.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 38987}, page_content='1. **[DiT](https://huggingface.co/docs/transformers/model_doc/dit)** (from Microsoft Research) released with the paper [DiT: Self-supervised Pre-training for Document Image Transformer](https://arxiv.org/abs/2203.02378) by Junlong Li, Yiheng Xu, Tengchao Lv, Lei Cui, Cha Zhang, Furu Wei.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 39276}, page_content='1. **[Donut](https://huggingface.co/docs/transformers/model_doc/donut)** (from NAVER), released together with the paper [OCR-free Document Understanding Transformer](https://arxiv.org/abs/2111.15664) by Geewook Kim, Teakgyu Hong, Moonbin Yim, Jeongyeon Nam, Jinyoung Park, Jinyeong Yim, Wonseok Hwang, Sangdoo Yun, Dongyoon Han, Seunghyun Park.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 39621}, page_content='1. **[DPR](https://huggingface.co/docs/transformers/model_doc/dpr)** (from Facebook) released with the paper [Dense Passage Retrieval for Open-Domain Question Answering](https://arxiv.org/abs/2004.04906) by Vladimir Karpukhin, Barlas Oğuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 39942}, page_content='1. **[DPT](https://huggingface.co/docs/transformers/master/model_doc/dpt)** (from Intel Labs) released with the paper [Vision Transformers for Dense Prediction](https://arxiv.org/abs/2103.13413) by René Ranftl, Alexey Bochkovskiy, Vladlen Koltun.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 40189}, page_content='1. **[EfficientFormer](https://huggingface.co/docs/transformers/model_doc/efficientformer)** (from Snap Research) released with the paper [EfficientFormer: Vision Transformers at MobileNetSpeed](https://arxiv.org/abs/2206.01191) by Yanyu Li, Geng Yuan, Yang Wen, Ju Hu, Georgios Evangelidis, Sergey Tulyakov, Yanzhi Wang, Jian Ren.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 40521}, page_content='1. **[EfficientNet](https://huggingface.co/docs/transformers/model_doc/efficientnet)** (from Google Brain) released with the paper [EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks](https://arxiv.org/abs/1905.11946) by Mingxing Tan, Quoc V. Le.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 40790}, page_content='1. **[ELECTRA](https://huggingface.co/docs/transformers/model_doc/electra)** (from Google Research/Stanford University) released with the paper [ELECTRA: Pre-training text encoders as discriminators rather than generators](https://arxiv.org/abs/2003.10555) by Kevin Clark, Minh-Thang Luong, Quoc V. Le, Christopher D. Manning.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 41117}, page_content='1. **[EnCodec](https://huggingface.co/docs/transformers/model_doc/encodec)** (from Meta AI) released with the paper [High Fidelity Neural Audio Compression](https://arxiv.org/abs/2210.13438) by Alexandre Défossez, Jade Copet, Gabriel Synnaeve, Yossi Adi.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 41372}, page_content='1. **[EncoderDecoder](https://huggingface.co/docs/transformers/model_doc/encoder-decoder)** (from Google Research) released with the paper [Leveraging Pre-trained Checkpoints for Sequence Generation Tasks](https://arxiv.org/abs/1907.12461) by Sascha Rothe, Shashi Narayan, Aliaksei Severyn.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 41663}, page_content='1. **[ERNIE](https://huggingface.co/docs/transformers/model_doc/ernie)** (from Baidu) released with the paper [ERNIE: Enhanced Representation through Knowledge Integration](https://arxiv.org/abs/1904.09223) by Yu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Xuyi Chen, Han Zhang, Xin Tian, Danxiang Zhu, Hao Tian, Hua Wu.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 41983}, page_content='1. **[ErnieM](https://huggingface.co/docs/transformers/model_doc/ernie_m)** (from Baidu) released with the paper [ERNIE-M: Enhanced Multilingual Representation by Aligning Cross-lingual Semantics with Monolingual Corpora](https://arxiv.org/abs/2012.15674) by Xuan Ouyang, Shuohuan Wang, Chao Pang, Yu Sun, Hao Tian, Hua Wu, Haifeng Wang.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 42321}, page_content='1. **[ESM](https://huggingface.co/docs/transformers/model_doc/esm)** (from Meta AI) are transformer protein language models.  **ESM-1b** was released with the paper [Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences](https://www.pnas.org/content/118/15/e2016239118) by Alexander Rives, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin, Jason Liu, Demi Guo, Myle Ott, C. Lawrence Zitnick, Jerry Ma, and Rob Fergus'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 42752}, page_content='. Lawrence Zitnick, Jerry Ma, and Rob Fergus. **ESM-1v** was released with the paper [Language models enable zero-shot prediction of the effects of mutations on protein function](https://doi.org/10.1101/2021.07.09.450648) by Joshua Meier, Roshan Rao, Robert Verkuil, Jason Liu, Tom Sercu and Alexander Rives. **ESM-2 and ESMFold** were released with the paper [Language models of protein sequences at the scale of evolution enable accurate structure prediction](https://doi.org/10.1101/2022.07.20'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 43225}, page_content='.org/10.1101/2022.07.20.500902) by Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Allan dos Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Sal Candido, Alexander Rives.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 43420}, page_content='1. **[Falcon](https://huggingface.co/docs/transformers/model_doc/falcon)** (from Technology Innovation Institute) by Almazrouei, Ebtesam and Alobeidli, Hamza and Alshamsi, Abdulaziz and Cappelli, Alessandro and Cojocaru, Ruxandra and Debbah, Merouane and Goffinet, Etienne and Heslow, Daniel and Launay, Julien and Malartic, Quentin and Noune, Badreddine and Pannier, Baptiste and Penedo, Guilherme.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 43820}, page_content='1. **[FLAN-T5](https://huggingface.co/docs/transformers/model_doc/flan-t5)** (from Google AI) released in the repository [google-research/t5x](https://github.com/google-research/t5x/blob/main/docs/models'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 43977}, page_content='.com/google-research/t5x/blob/main/docs/models.md#flan-t5-checkpoints) by Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 44475}, page_content='1. **[FLAN-UL2](https://huggingface.co/docs/transformers/model_doc/flan-ul2)** (from Google AI) released in the repository [google-research/t5x](https://github.com/google-research/t5x/blob/main/docs/models'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 44634}, page_content='.com/google-research/t5x/blob/main/docs/models.md#flan-ul2-checkpoints) by Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 45133}, page_content='1. **[FlauBERT](https://huggingface.co/docs/transformers/model_doc/flaubert)** (from CNRS) released with the paper [FlauBERT: Unsupervised Language Model Pre-training for French](https://arxiv.org/abs/1912.05372) by Hang Le, Loïc Vial, Jibril Frej, Vincent Segonne, Maximin Coavoux, Benjamin Lecouteux, Alexandre Allauzen, Benoît Crabbé, Laurent Besacier, Didier Schwab.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 45504}, page_content='1. **[FLAVA](https://huggingface.co/docs/transformers/model_doc/flava)** (from Facebook AI) released with the paper [FLAVA: A Foundational Language And Vision Alignment Model](https://arxiv.org/abs/2112.04482) by Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, and Douwe Kiela.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 45835}, page_content='1. **[FNet](https://huggingface.co/docs/transformers/model_doc/fnet)** (from Google Research) released with the paper [FNet: Mixing Tokens with Fourier Transforms](https://arxiv.org/abs/2105.03824) by James Lee-Thorp, Joshua Ainslie, Ilya Eckstein, Santiago Ontanon.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 46102}, page_content='1. **[FocalNet](https://huggingface.co/docs/transformers/model_doc/focalnet)** (from Microsoft Research) released with the paper [Focal Modulation Networks](https://arxiv.org/abs/2203.11926) by Jianwei Yang, Chunyuan Li, Xiyang Dai, Lu Yuan, Jianfeng Gao.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 46358}, page_content='1. **[Funnel Transformer](https://huggingface.co/docs/transformers/model_doc/funnel)** (from CMU/Google Brain) released with the paper [Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing](https://arxiv.org/abs/2006.03236) by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 46671}, page_content='1. **[Fuyu](https://huggingface.co/docs/transformers/model_doc/fuyu)** (from ADEPT) Rohan Bavishi, Erich Elsen, Curtis Hawthorne, Maxwell Nye, Augustus Odena, Arushi Somani, Sağnak Taşırlar. Released with the paper [blog post](https://www.adept.ai/blog/fuyu-8b)'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 46933}, page_content='1. **[GIT](https://huggingface.co/docs/transformers/model_doc/git)** (from Microsoft Research) released with the paper [GIT: A Generative Image-to-text Transformer for Vision and Language](https://arxiv.org/abs/2205.14100) by Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, Lijuan Wang.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 47267}, page_content='1. **[GLPN](https://huggingface.co/docs/transformers/model_doc/glpn)** (from KAIST) released with the paper [Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth](https://arxiv.org/abs/2201.07436) by Doyeon Kim, Woonghyun Ga, Pyungwhan Ahn, Donggyu Joo, Sehwan Chun, Junmo Kim.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 47573}, page_content='1. **[GPT](https://huggingface.co/docs/transformers/model_doc/openai-gpt)** (from OpenAI) released with the paper [Improving Language Understanding by Generative Pre-Training](https://blog.openai.com/language-unsupervised/) by Alec Radford, Karthik Narasimhan, Tim Salimans and Ilya Sutskever.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 47867}, page_content='1. **[GPT Neo](https://huggingface.co/docs/transformers/model_doc/gpt_neo)** (from EleutherAI) released in the repository [EleutherAI/gpt-neo](https://github.com/EleutherAI/gpt-neo) by Sid Black, Stella Biderman, Leo Gao, Phil Wang and Connor Leahy.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 48117}, page_content='1. **[GPT NeoX](https://huggingface.co/docs/transformers/model_doc/gpt_neox)** (from EleutherAI) released with the paper [GPT-NeoX-20B: An Open-Source Autoregressive Language Model](https://arxiv.org/abs/2204.06745) by Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, USVSN Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, Samuel Weinbach'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 48588}, page_content='1. **[GPT NeoX Japanese](https://huggingface.co/docs/transformers/model_doc/gpt_neox_japanese)** (from ABEJA) released by Shinya Otani, Takayoshi Makabe, Anuj Arora, and Kyo Hattori.\\n1. **[GPT-2](https://huggingface.co/docs/transformers/model_doc/gpt2)** (from OpenAI) released with the paper [Language Models are Unsupervised Multitask Learners](https://blog.openai.com/better-language-models/) by Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei** and Ilya Sutskever**.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 49077}, page_content='1. **[GPT-J](https://huggingface.co/docs/transformers/model_doc/gptj)** (from EleutherAI) released in the repository [kingoflolz/mesh-transformer-jax](https://github.com/kingoflolz/mesh-transformer-jax/) by Ben Wang and Aran Komatsuzaki.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 49315}, page_content='1. **[GPT-Sw3](https://huggingface.co/docs/transformers/model_doc/gpt-sw3)** (from AI-Sweden) released with the paper [Lessons Learned from GPT-SW3: Building the First Large-Scale Generative Language Model for Swedish](http://www.lrec-conf.org/proceedings/lrec2022/pdf/2022.lrec-1.376.pdf) by Ariel Ekgren, Amaru Cuba Gyllensten, Evangelia Gogoulou, Alice Heiman, Severine Verlinden, Joey Öhman, Fredrik Carlsson, Magnus Sahlgren.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 49746}, page_content=\"1. **[GPTBigCode](https://huggingface.co/docs/transformers/model_doc/gpt_bigcode)** (from BigCode) released with the paper [SantaCoder: don't reach for the stars!](https://arxiv.org/abs/2301\"),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 49936}, page_content='.03988) by Loubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki, Carlos Munoz Ferrandis, Niklas Muennighoff, Mayank Mishra, Alex Gu, Manan Dey, Logesh Kumar Umapathi, Carolyn Jane Anderson, Yangtian Zi, Joel Lamy Poirier, Hailey Schoelkopf, Sergey Troshin, Dmitry Abulkhanov, Manuel Romero, Michael Lappert, Francesco De Toni, Bernardo García del Río, Qian Liu, Shamik Bose, Urvashi Bhattacharyya, Terry Yue Zhuo, Ian Yu, Paulo Villegas, Marco Zocca, Sourab Mangrulkar, David'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 50389}, page_content='Villegas, Marco Zocca, Sourab Mangrulkar, David Lansky, Huu Nguyen, Danish Contractor, Luis Villa, Jia Li, Dzmitry Bahdanau, Yacine Jernite, Sean Hughes, Daniel Fried, Arjun Guha, Harm de Vries, Leandro von Werra'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 50601}, page_content='.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 50603}, page_content='1. **[GPTSAN-japanese](https://huggingface.co/docs/transformers/model_doc/gptsan-japanese)** released in the repository [tanreinama/GPTSAN](https://github.com/tanreinama/GPTSAN/blob/main/report/model.md) by Toshiyuki Sakamoto(tanreinama).'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 50842}, page_content='1. **[Graphormer](https://huggingface.co/docs/transformers/model_doc/graphormer)** (from Microsoft) released with the paper [Do Transformers Really Perform Bad for Graph Representation?](https://arxiv.org/abs/2106.05234) by Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen, Tie-Yan Liu.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 51167}, page_content='1. **[GroupViT](https://huggingface.co/docs/transformers/model_doc/groupvit)** (from UCSD, NVIDIA) released with the paper [GroupViT: Semantic Segmentation Emerges from Text Supervision](https://arxiv.org/abs/2202.11094) by Jiarui Xu, Shalini De Mello, Sifei Liu, Wonmin Byeon, Thomas Breuel, Jan Kautz, Xiaolong Wang.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 51486}, page_content='1. **[HerBERT](https://huggingface.co/docs/transformers/model_doc/herbert)** (from Allegro.pl, AGH University of Science and Technology) released with the paper [KLEJ: Comprehensive Benchmark for Polish Language Understanding](https://www.aclweb.org/anthology/2020.acl-main.111.pdf) by Piotr Rybak, Robert Mroczkowski, Janusz Tracz, Ireneusz Gawlik.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 51836}, page_content='1. **[Hubert](https://huggingface.co/docs/transformers/model_doc/hubert)** (from Facebook) released with the paper [HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units](https://arxiv.org/abs/2106.07447) by Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, Abdelrahman Mohamed.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 52194}, page_content='1. **[I-BERT](https://huggingface.co/docs/transformers/model_doc/ibert)** (from Berkeley) released with the paper [I-BERT: Integer-only BERT Quantization](https://arxiv.org/abs/2101.01321) by Sehoon Kim, Amir Gholami, Zhewei Yao, Michael W. Mahoney, Kurt Keutzer.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 52458}, page_content='1. **[IDEFICS](https://huggingface.co/docs/transformers/model_doc/idefics)** (from HuggingFace) released with the paper [OBELICS: An Open Web-Scale Filtered Dataset of Interleaved Image-Text Documents](https://huggingface.co/papers/2306.16527) by Hugo Laurençon, Lucile Saulnier, Léo Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov, Thomas Wang, Siddharth Karamcheti, Alexander M. Rush, Douwe Kiela, Matthieu Cord, Victor Sanh.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 52892}, page_content='1. **[ImageGPT](https://huggingface.co/docs/transformers/model_doc/imagegpt)** (from OpenAI) released with the paper [Generative Pretraining from Pixels](https://openai.com/blog/image-gpt/) by Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, Ilya Sutskever.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 53175}, page_content='1. **[Informer](https://huggingface.co/docs/transformers/model_doc/informer)** (from Beihang University, UC Berkeley, Rutgers University, SEDD Company) released with the paper [Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting](https://arxiv.org/abs/2012.07436) by Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 53566}, page_content='1. **[InstructBLIP](https://huggingface.co/docs/transformers/model_doc/instructblip)** (from Salesforce) released with the paper [InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning](https://arxiv.org/abs/2305.06500) by Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, Steven Hoi.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 53944}, page_content='1. **[Jukebox](https://huggingface.co/docs/transformers/model_doc/jukebox)** (from OpenAI) released with the paper [Jukebox: A Generative Model for Music](https://arxiv.org/pdf/2005.00341.pdf) by Prafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook Kim, Alec Radford, Ilya Sutskever.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 54233}, page_content='1. **[LayoutLM](https://huggingface.co/docs/transformers/model_doc/layoutlm)** (from Microsoft Research Asia) released with the paper [LayoutLM: Pre-training of Text and Layout for Document Image Understanding](https://arxiv.org/abs/1912.13318) by Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, Ming Zhou.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 54549}, page_content='1. **[LayoutLMv2](https://huggingface.co/docs/transformers/model_doc/layoutlmv2)** (from Microsoft Research Asia) released with the paper [LayoutLMv2: Multi-modal Pre-training for Visually-Rich Document Understanding](https://arxiv.org/abs/2012.14740) by Yang Xu, Yiheng Xu, Tengchao Lv, Lei Cui, Furu Wei, Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha Zhang, Wanxiang Che, Min Zhang, Lidong Zhou.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 54946}, page_content='1. **[LayoutLMv3](https://huggingface.co/docs/transformers/model_doc/layoutlmv3)** (from Microsoft Research Asia) released with the paper [LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking](https://arxiv.org/abs/2204.08387) by Yupan Huang, Tengchao Lv, Lei Cui, Yutong Lu, Furu Wei.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 55256}, page_content='1. **[LayoutXLM](https://huggingface.co/docs/transformers/model_doc/layoutxlm)** (from Microsoft Research Asia) released with the paper [LayoutXLM: Multimodal Pre-training for Multilingual Visually-rich Document Understanding](https://arxiv.org/abs/2104.08836) by Yiheng Xu, Tengchao Lv, Lei Cui, Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha Zhang, Furu Wei.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 55615}, page_content='1. **[LED](https://huggingface.co/docs/transformers/model_doc/led)** (from AllenAI) released with the paper [Longformer: The Long-Document Transformer](https://arxiv.org/abs/2004.05150) by Iz Beltagy, Matthew E. Peters, Arman Cohan.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 55848}, page_content=\"1. **[LeViT](https://huggingface.co/docs/transformers/model_doc/levit)** (from Meta AI) released with the paper [LeViT: A Vision Transformer in ConvNet's Clothing for Faster Inference](https://arxiv.org/abs/2104.01136) by Ben Graham, Alaaeldin El-Nouby, Hugo Touvron, Pierre Stock, Armand Joulin, Hervé Jégou, Matthijs Douze.\"),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 56174}, page_content='1. **[LiLT](https://huggingface.co/docs/transformers/model_doc/lilt)** (from South China University of Technology) released with the paper [LiLT: A Simple yet Effective Language-Independent Layout Transformer for Structured Document Understanding](https://arxiv.org/abs/2202.13669) by Jiapeng Wang, Lianwen Jin, Kai Ding.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 56496}, page_content='1. **[LLaMA](https://huggingface.co/docs/transformers/model_doc/llama)** (from The FAIR team of Meta AI) released with the paper [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971) by Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 56945}, page_content='1. **[Llama2](https://huggingface.co/docs/transformers/model_doc/llama2)** (from The FAIR team of Meta AI) released with the paper [Llama2: Open Foundation and Fine-Tuned Chat Models](https://ai.meta'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 57144}, page_content='.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/XXX) by Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 57593}, page_content='Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushka rMishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing EllenTan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 58044}, page_content='Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, Thomas Scialom'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 58255}, page_content='.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 58257}, page_content='1. **[Longformer](https://huggingface.co/docs/transformers/model_doc/longformer)** (from AllenAI) released with the paper [Longformer: The Long-Document Transformer](https://arxiv.org/abs/2004.05150) by Iz Beltagy, Matthew E. Peters, Arman Cohan.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 58504}, page_content='1. **[LongT5](https://huggingface.co/docs/transformers/model_doc/longt5)** (from Google AI) released with the paper [LongT5: Efficient Text-To-Text Transformer for Long Sequences](https://arxiv.org/abs/2112.07916) by Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon, Jianmo Ni, Yun-Hsuan Sung, Yinfei Yang.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 58819}, page_content='1. **[LUKE](https://huggingface.co/docs/transformers/model_doc/luke)** (from Studio Ousia) released with the paper [LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention](https://arxiv.org/abs/2010.01057) by Ikuya Yamada, Akari Asai, Hiroyuki Shindo, Hideaki Takeda, Yuji Matsumoto.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 59130}, page_content='1. **[LXMERT](https://huggingface.co/docs/transformers/model_doc/lxmert)** (from UNC Chapel Hill) released with the paper [LXMERT: Learning Cross-Modality Encoder Representations from Transformers for Open-Domain Question Answering](https://arxiv.org/abs/1908.07490) by Hao Tan and Mohit Bansal.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 59426}, page_content='1. **[M-CTC-T](https://huggingface.co/docs/transformers/model_doc/mctct)** (from Facebook) released with the paper [Pseudo-Labeling For Massively Multilingual Speech Recognition](https://arxiv.org/abs/2111.00161) by Loren Lugosch, Tatiana Likhomanenko, Gabriel Synnaeve, and Ronan Collobert.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 59718}, page_content='1. **[M2M100](https://huggingface.co/docs/transformers/model_doc/m2m_100)** (from Facebook) released with the paper [Beyond English-Centric Multilingual Machine Translation](https://arxiv.org/abs/2010.11125) by Angela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky, Siddharth Goyal, Mandeep Baines, Onur Celebi, Guillaume Wenzek, Vishrav Chaudhary, Naman Goyal, Tom Birch, Vitaliy Liptchinsky, Sergey Edunov, Edouard Grave, Michael Auli, Armand Joulin.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 60187}, page_content='1. **[MADLAD-400](https://huggingface.co/docs/transformers/model_doc/madlad-400)** (from Google) released with the paper [MADLAD-400: A Multilingual And Document-Level Large Audited Dataset](https://arxiv.org/abs/2309.04662) by Sneha Kudugunta, Isaac Caswell, Biao Zhang, Xavier Garcia, Christopher A. Choquette-Choo, Katherine Lee, Derrick Xin, Aditya Kusupati, Romi Stella, Ankur Bapna, Orhan Firat.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 60589}, page_content='1. **[MarianMT](https://huggingface.co/docs/transformers/model_doc/marian)** Machine translation models trained using [OPUS](http://opus.nlpl.eu/) data by Jörg Tiedemann. The [Marian Framework](https://marian-nmt.github.io/) is being developed by the Microsoft Translator Team.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 60867}, page_content='1. **[MarkupLM](https://huggingface.co/docs/transformers/model_doc/markuplm)** (from Microsoft Research Asia) released with the paper [MarkupLM: Pre-training of Text and Markup Language for Visually-rich Document Understanding](https://arxiv.org/abs/2110.08518) by Junlong Li, Yiheng Xu, Lei Cui, Furu Wei.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 61174}, page_content='1. **[Mask2Former](https://huggingface.co/docs/transformers/model_doc/mask2former)** (from FAIR and UIUC) released with the paper [Masked-attention Mask Transformer for Universal Image Segmentation](https://arxiv.org/abs/2112.01527) by Bowen Cheng, Ishan Misra, Alexander G. Schwing, Alexander Kirillov, Rohit Girdhar.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 61493}, page_content='1. **[MaskFormer](https://huggingface.co/docs/transformers/model_doc/maskformer)** (from Meta and UIUC) released with the paper [Per-Pixel Classification is Not All You Need for Semantic Segmentation](https://arxiv.org/abs/2107.06278) by Bowen Cheng, Alexander G. Schwing, Alexander Kirillov.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 61786}, page_content='1. **[MatCha](https://huggingface.co/docs/transformers/model_doc/matcha)** (from Google AI) released with the paper [MatCha: Enhancing Visual Language Pretraining with Math Reasoning and Chart Derendering](https://arxiv.org/abs/2212.09662) by Fangyu Liu, Francesco Piccinno, Syrine Krichene, Chenxi Pang, Kenton Lee, Mandar Joshi, Yasemin Altun, Nigel Collier, Julian Martin Eisenschlos.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 62174}, page_content='1. **[mBART](https://huggingface.co/docs/transformers/model_doc/mbart)** (from Facebook) released with the paper [Multilingual Denoising Pre-training for Neural Machine Translation](https://arxiv.org/abs/2001.08210) by Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, Luke Zettlemoyer.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 62505}, page_content='1. **[mBART-50](https://huggingface.co/docs/transformers/model_doc/mbart)** (from Facebook) released with the paper [Multilingual Translation with Extensible Multilingual Pretraining and Finetuning](https://arxiv.org/abs/2008.00401) by Yuqing Tang, Chau Tran, Xian Li, Peng-Jen Chen, Naman Goyal, Vishrav Chaudhary, Jiatao Gu, Angela Fan.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 62844}, page_content='1. **[MEGA](https://huggingface.co/docs/transformers/model_doc/mega)** (from Meta/USC/CMU/SJTU) released with the paper [Mega: Moving Average Equipped Gated Attention](https://arxiv.org/abs/2209.10655) by Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig, Jonathan May, and Luke Zettlemoyer.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 63163}, page_content='1. **[Megatron-BERT](https://huggingface.co/docs/transformers/model_doc/megatron-bert)** (from NVIDIA) released with the paper [Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism](https://arxiv.org/abs/1909.08053) by Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper and Bryan Catanzaro.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 63514}, page_content='1. **[Megatron-GPT2](https://huggingface.co/docs/transformers/model_doc/megatron_gpt2)** (from NVIDIA) released with the paper [Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism](https://arxiv.org/abs/1909.08053) by Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper and Bryan Catanzaro.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 63865}, page_content='1. **[MGP-STR](https://huggingface.co/docs/transformers/model_doc/mgp-str)** (from Alibaba Research) released with the paper [Multi-Granularity Prediction for Scene Text Recognition](https://arxiv.org/abs/2209.03592) by Peng Wang, Cheng Da, and Cong Yao.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 64120}, page_content='1. **[mLUKE](https://huggingface.co/docs/transformers/model_doc/mluke)** (from Studio Ousia) released with the paper [mLUKE: The Power of Entity Representations in Multilingual Pretrained Language Models](https://arxiv.org/abs/2110.08151) by Ryokan Ri, Ikuya Yamada, and Yoshimasa Tsuruoka.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 64411}, page_content='1. **[MMS](https://huggingface.co/docs/transformers/model_doc/mms)** (from Facebook) released with the paper [Scaling Speech Technology to 1,000+ Languages](https://arxiv.org/abs/2305.13516) by Vineel Pratap, Andros Tjandra, Bowen Shi, Paden Tomasello, Arun Babu, Sayani Kundu, Ali Elkahky, Zhaoheng Ni, Apoorv Vyas, Maryam Fazel-Zarandi, Alexei Baevski, Yossi Adi, Xiaohui Zhang, Wei-Ning Hsu, Alexis Conneau, Michael Auli.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 64836}, page_content='1. **[MobileBERT](https://huggingface.co/docs/transformers/model_doc/mobilebert)** (from CMU/Google Brain) released with the paper [MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices](https://arxiv.org/abs/2004.02984) by Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, and Denny Zhou.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 65156}, page_content='1. **[MobileNetV1](https://huggingface.co/docs/transformers/model_doc/mobilenet_v1)** (from Google Inc.) released with the paper [MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications](https://arxiv.org/abs/1704.04861) by Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, Hartwig Adam.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 65528}, page_content='1. **[MobileNetV2](https://huggingface.co/docs/transformers/model_doc/mobilenet_v2)** (from Google Inc.) released with the paper [MobileNetV2: Inverted Residuals and Linear Bottlenecks](https://arxiv.org/abs/1801.04381) by Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, Liang-Chieh Chen.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 65830}, page_content='1. **[MobileViT](https://huggingface.co/docs/transformers/model_doc/mobilevit)** (from Apple) released with the paper [MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer](https://arxiv.org/abs/2110.02178) by Sachin Mehta and Mohammad Rastegari.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 66105}, page_content='1. **[MobileViTV2](https://huggingface.co/docs/transformers/model_doc/mobilevitv2)** (from Apple) released with the paper [Separable Self-attention for Mobile Vision Transformers](https://arxiv.org/abs/2206.02680) by Sachin Mehta and Mohammad Rastegari.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 66359}, page_content='1. **[MPNet](https://huggingface.co/docs/transformers/model_doc/mpnet)** (from Microsoft Research) released with the paper [MPNet: Masked and Permuted Pre-training for Language Understanding](https://arxiv.org/abs/2004.09297) by Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, Tie-Yan Liu.\\n1. **[MPT](https://huggingface.co/docs/transformers/model_doc/mpt)** (from MosaiML) released with the repository [llm-foundry](https://github.com/mosaicml/llm-foundry/) by the MosaicML NLP Team.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 66839}, page_content='1. **[MRA](https://huggingface.co/docs/transformers/model_doc/mra)** (from the University of Wisconsin - Madison) released with the paper [Multi Resolution Analysis (MRA) for Approximate Self-Attention](https://arxiv.org/abs/2207.10284) by Zhanpeng Zeng, Sourav Pal, Jeffery Kline, Glenn M Fung, Vikas Singh.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 67148}, page_content='1. **[MT5](https://huggingface.co/docs/transformers/model_doc/mt5)** (from Google AI) released with the paper [mT5: A massively multilingual pre-trained text-to-text transformer](https://arxiv.org/abs/2010.11934) by Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, Colin Raffel.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 67477}, page_content='1. **[MusicGen](https://huggingface.co/docs/transformers/model_doc/musicgen)** (from Meta) released with the paper [Simple and Controllable Music Generation](https://arxiv.org/abs/2306.05284) by Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve, Yossi Adi and Alexandre Défossez.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 67782}, page_content='1. **[MVP](https://huggingface.co/docs/transformers/model_doc/mvp)** (from RUC AI Box) released with the paper [MVP: Multi-task Supervised Pre-training for Natural Language Generation](https://arxiv.org/abs/2206.12131) by Tianyi Tang, Junyi Li, Wayne Xin Zhao and Ji-Rong Wen.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 68059}, page_content='1. **[NAT](https://huggingface.co/docs/transformers/model_doc/nat)** (from SHI Labs) released with the paper [Neighborhood Attention Transformer](https://arxiv.org/abs/2204.07143) by Ali Hassani, Steven Walton, Jiachen Li, Shen Li, and Humphrey Shi.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 68309}, page_content='1. **[Nezha](https://huggingface.co/docs/transformers/model_doc/nezha)** (from Huawei Noah’s Ark Lab) released with the paper [NEZHA: Neural Contextualized Representation for Chinese Language Understanding](https://arxiv.org/abs/1909.00204) by Junqiu Wei, Xiaozhe Ren, Xiaoguang Li, Wenyong Huang, Yi Liao, Yasheng Wang, Jiashu Lin, Xin Jiang, Xiao Chen and Qun Liu.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 68676}, page_content='1. **[NLLB](https://huggingface.co/docs/transformers/model_doc/nllb)** (from Meta) released with the paper [No Language Left Behind: Scaling Human-Centered Machine Translation](https://arxiv.org/abs/2207.04672) by the NLLB team.\\n1. **[NLLB-MOE](https://huggingface.co/docs/transformers/model_doc/nllb-moe)** (from Meta) released with the paper [No Language Left Behind: Scaling Human-Centered Machine Translation](https://arxiv.org/abs/2207.04672) by the NLLB team.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 69142}, page_content='1. **[Nyströmformer](https://huggingface.co/docs/transformers/model_doc/nystromformer)** (from the University of Wisconsin - Madison) released with the paper [Nyströmformer: A Nyström-Based Algorithm for Approximating Self-Attention](https://arxiv.org/abs/2102.03902) by Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li, Vikas Singh.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 69512}, page_content='1. **[OneFormer](https://huggingface.co/docs/transformers/model_doc/oneformer)** (from SHI Labs) released with the paper [OneFormer: One Transformer to Rule Universal Image Segmentation](https://arxiv.org/abs/2211.06220) by Jitesh Jain, Jiachen Li, MangTik Chiu, Ali Hassani, Nikita Orlov, Humphrey Shi.\\n1. **[OpenLlama](https://huggingface.co/docs/transformers/model_doc/open-llama)** (from [s-JoL](https://huggingface.co/s-JoL)) released on GitHub (now removed).'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 69977}, page_content='1. **[OPT](https://huggingface.co/docs/transformers/master/model_doc/opt)** (from Meta AI) released with the paper [OPT: Open Pre-trained Transformer Language Models](https://arxiv.org/abs/2205.01068) by Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen et al.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 70269}, page_content='1. **[OWL-ViT](https://huggingface.co/docs/transformers/model_doc/owlvit)** (from Google AI) released with the paper [Simple Open-Vocabulary Object Detection with Vision Transformers](https://arxiv.org/abs/2205.06230) by Matthias Minderer, Alexey Gritsenko, Austin Stone, Maxim Neumann, Dirk Weissenborn, Alexey Dosovitskiy, Aravindh Mahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran Shen, Xiao Wang, Xiaohua Zhai, Thomas Kipf, and Neil Houlsby.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 70716}, page_content='1. **[Pegasus](https://huggingface.co/docs/transformers/model_doc/pegasus)** (from Google) released with the paper [PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization](https://arxiv.org/abs/1912.08777) by Jingqing Zhang, Yao Zhao, Mohammad Saleh and Peter J. Liu.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 71010}, page_content='1. **[PEGASUS-X](https://huggingface.co/docs/transformers/model_doc/pegasus_x)** (from Google) released with the paper [Investigating Efficiently Extending Transformers for Long Input Summarization](https://arxiv.org/abs/2208.04347) by Jason Phang, Yao Zhao, and Peter J. Liu.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 71287}, page_content='1. **[Perceiver IO](https://huggingface.co/docs/transformers/model_doc/perceiver)** (from Deepmind) released with the paper [Perceiver IO: A General Architecture for Structured Inputs & Outputs](https://arxiv.org/abs/2107.14795) by Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Doersch, Catalin Ionescu, David Ding, Skanda Koppula, Daniel Zoran, Andrew Brock, Evan Shelhamer, Olivier Hénaff, Matthew M. Botvinick, Andrew Zisserman, Oriol Vinyals, João Carreira.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 71766}, page_content='1. **[Persimmon](https://huggingface.co/docs/transformers/main/model_doc/persimmon)** (from ADEPT) released in a [blog post](https://www.adept.ai/blog/persimmon-8b) by Erich Elsen, Augustus Odena, Maxwell Nye, Sağnak Taşırlar, Tri Dao, Curtis Hawthorne, Deepak Moparthi, Arushi Somani.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 72052}, page_content='1. **[Phi](https://huggingface.co/docs/main/transformers/model_doc/phi)** (from Microsoft Research) released with the papers - [Textbooks Are All You Need](https://arxiv.org/abs/2306'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 72221}, page_content='.org/abs/2306.11644) by Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio César Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Harkirat Singh Behl, Xin Wang, Sébastien Bubeck, Ronen Eldan, Adam Tauman Kalai, Yin Tat Lee and Yuanzhi Li, [Textbooks Are All You Need II: phi-1.5 technical report](https://arxiv.org/abs/2309'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 72586}, page_content='.5 technical report](https://arxiv.org/abs/2309.05463) by Yuanzhi Li, Sébastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar and Yin Tat Lee.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 72739}, page_content='1. **[PhoBERT](https://huggingface.co/docs/transformers/model_doc/phobert)** (from VinAI Research) released with the paper [PhoBERT: Pre-trained language models for Vietnamese](https://www.aclweb.org/anthology/2020.findings-emnlp.92/) by Dat Quoc Nguyen and Anh Tuan Nguyen.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 73014}, page_content='1. **[Pix2Struct](https://huggingface.co/docs/transformers/model_doc/pix2struct)** (from Google) released with the paper [Pix2Struct: Screenshot Parsing as Pretraining for Visual Language Understanding](https://arxiv.org/abs/2210.03347) by Kenton Lee, Mandar Joshi, Iulia Turc, Hexiang Hu, Fangyu Liu, Julian Eisenschlos, Urvashi Khandelwal, Peter Shaw, Ming-Wei Chang, Kristina Toutanova.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 73404}, page_content='1. **[PLBart](https://huggingface.co/docs/transformers/model_doc/plbart)** (from UCLA NLP) released with the paper [Unified Pre-training for Program Understanding and Generation](https://arxiv.org/abs/2103.06333) by Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, Kai-Wei Chang.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 73688}, page_content='1. **[PoolFormer](https://huggingface.co/docs/transformers/model_doc/poolformer)** (from Sea AI Labs) released with the paper [MetaFormer is Actually What You Need for Vision](https://arxiv.org/abs/2111.11418) by Yu, Weihao and Luo, Mi and Zhou, Pan and Si, Chenyang and Zhou, Yichen and Wang, Xinchao and Feng, Jiashi and Yan, Shuicheng.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 74027}, page_content='1. **[Pop2Piano](https://huggingface.co/docs/transformers/model_doc/pop2piano)** released with the paper [Pop2Piano : Pop Audio-based Piano Cover Generation](https://arxiv.org/abs/2211.00895) by Jongho Choi and Kyogu Lee.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 74249}, page_content='1. **[ProphetNet](https://huggingface.co/docs/transformers/model_doc/prophetnet)** (from Microsoft Research) released with the paper [ProphetNet: Predicting Future N-gram for Sequence-to-Sequence Pre-training](https://arxiv.org/abs/2001.04063) by Yu Yan, Weizhen Qi, Yeyun Gong, Dayiheng Liu, Nan Duan, Jiusheng Chen, Ruofei Zhang and Ming Zhou.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 74595}, page_content='1. **[PVT](https://huggingface.co/docs/transformers/model_doc/pvt)** (from Nanjing University, The University of Hong Kong etc.) released with the paper [Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions](https://arxiv.org/pdf/2102.12122.pdf) by Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, Ling Shao.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 74985}, page_content='1. **[QDQBert](https://huggingface.co/docs/transformers/model_doc/qdqbert)** (from NVIDIA) released with the paper [Integer Quantization for Deep Learning Inference: Principles and Empirical Evaluation](https://arxiv.org/abs/2004.09602) by Hao Wu, Patrick Judd, Xiaojie Zhang, Mikhail Isaev and Paulius Micikevicius.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 75302}, page_content='1. **[RAG](https://huggingface.co/docs/transformers/model_doc/rag)** (from Facebook) released with the paper [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401) by Patrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, Douwe Kiela.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 75702}, page_content='1. **[REALM](https://huggingface.co/docs/transformers/model_doc/realm.html)** (from Google Research) released with the paper [REALM: Retrieval-Augmented Language Model Pre-Training](https://arxiv.org/abs/2002.08909) by Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat and Ming-Wei Chang.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 75993}, page_content='1. **[Reformer](https://huggingface.co/docs/transformers/model_doc/reformer)** (from Google Research) released with the paper [Reformer: The Efficient Transformer](https://arxiv.org/abs/2001.04451) by Nikita Kitaev, Łukasz Kaiser, Anselm Levskaya.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 76241}, page_content='1. **[RegNet](https://huggingface.co/docs/transformers/model_doc/regnet)** (from META Platforms) released with the paper [Designing Network Design Space](https://arxiv.org/abs/2003.13678) by Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, Piotr Dollár.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 76514}, page_content='1. **[RemBERT](https://huggingface.co/docs/transformers/model_doc/rembert)** (from Google Research) released with the paper [Rethinking embedding coupling in pre-trained language models](https://arxiv.org/abs/2010.12821) by Hyung Won Chung, Thibault Févry, Henry Tsai, M. Johnson, Sebastian Ruder.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 76812}, page_content='1. **[ResNet](https://huggingface.co/docs/transformers/model_doc/resnet)** (from Microsoft Research) released with the paper [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385) by Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 77072}, page_content='1. **[RoBERTa](https://huggingface.co/docs/transformers/model_doc/roberta)** (from Facebook), released together with the paper [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/abs/1907.11692) by Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 77426}, page_content='1. **[RoBERTa-PreLayerNorm](https://huggingface.co/docs/transformers/model_doc/roberta-prelayernorm)** (from Facebook) released with the paper [fairseq: A Fast, Extensible Toolkit for Sequence Modeling](https://arxiv.org/abs/1904.01038) by Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, Michael Auli.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 77771}, page_content='1. **[RoCBert](https://huggingface.co/docs/transformers/model_doc/roc_bert)** (from WeChatAI) released with the paper [RoCBert: Robust Chinese Bert with Multimodal Contrastive Pretraining](https://aclanthology.org/2022.acl-long.65.pdf) by HuiSu, WeiweiShi, XiaoyuShen, XiaoZhou, TuoJi, JiaruiFang, JieZhou.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 78078}, page_content='1. **[RoFormer](https://huggingface.co/docs/transformers/model_doc/roformer)** (from ZhuiyiTechnology), released together with the paper [RoFormer: Enhanced Transformer with Rotary Position Embedding](https://arxiv.org/abs/2104.09864) by Jianlin Su and Yu Lu and Shengfeng Pan and Bo Wen and Yunfeng Liu.\\n1. **[RWKV](https://huggingface.co/docs/transformers/model_doc/rwkv)** (from Bo Peng), released on [this repo](https://github.com/BlinkDL/RWKV-LM) by Bo Peng.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 78542}, page_content='1. **[SegFormer](https://huggingface.co/docs/transformers/model_doc/segformer)** (from NVIDIA) released with the paper [SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers](https://arxiv.org/abs/2105.15203) by Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M. Alvarez, Ping Luo.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 78863}, page_content='1. **[Segment Anything](https://huggingface.co/docs/transformers/model_doc/sam)** (from Meta AI) released with the paper [Segment Anything](https://arxiv.org/pdf/2304.02643v1.pdf) by Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alex Berg, Wan-Yen Lo, Piotr Dollar, Ross Girshick.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 79218}, page_content='1. **[SEW](https://huggingface.co/docs/transformers/model_doc/sew)** (from ASAPP) released with the paper [Performance-Efficiency Trade-offs in Unsupervised Pre-training for Speech Recognition](https://arxiv.org/abs/2109.06870) by Felix Wu, Kwangyoun Kim, Jing Pan, Kyu Han, Kilian Q. Weinberger, Yoav Artzi.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 79527}, page_content='1. **[SEW-D](https://huggingface.co/docs/transformers/model_doc/sew_d)** (from ASAPP) released with the paper [Performance-Efficiency Trade-offs in Unsupervised Pre-training for Speech Recognition](https://arxiv.org/abs/2109.06870) by Felix Wu, Kwangyoun Kim, Jing Pan, Kyu Han, Kilian Q. Weinberger, Yoav Artzi.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 79840}, page_content='1. **[SpeechT5](https://huggingface.co/docs/transformers/model_doc/speecht5)** (from Microsoft Research) released with the paper [SpeechT5: Unified-Modal Encoder-Decoder Pre-Training for Spoken Language Processing](https://arxiv.org/abs/2110.07205) by Junyi Ao, Rui Wang, Long Zhou, Chengyi Wang, Shuo Ren, Yu Wu, Shujie Liu, Tom Ko, Qing Li, Yu Zhang, Zhihua Wei, Yao Qian, Jinyu Li, Furu Wei.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 80235}, page_content='1. **[SpeechToTextTransformer](https://huggingface.co/docs/transformers/model_doc/speech_to_text)** (from Facebook), released together with the paper [fairseq S2T: Fast Speech-to-Text Modeling with fairseq](https://arxiv.org/abs/2010.05171) by Changhan Wang, Yun Tang, Xutai Ma, Anne Wu, Dmytro Okhonko, Juan Pino.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 80550}, page_content='1. **[SpeechToTextTransformer2](https://huggingface.co/docs/transformers/model_doc/speech_to_text_2)** (from Facebook), released together with the paper [Large-Scale Self- and Semi-Supervised Learning for Speech Translation](https://arxiv.org/abs/2104.06678) by Changhan Wang, Anne Wu, Juan Pino, Alexei Baevski, Michael Auli, Alexis Conneau.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 80893}, page_content='1. **[Splinter](https://huggingface.co/docs/transformers/model_doc/splinter)** (from Tel Aviv University), released together with the paper [Few-Shot Question Answering by Pretraining Span Selection](https://arxiv.org/abs/2101.00438) by Ori Ram, Yuval Kirstain, Jonathan Berant, Amir Globerson, Omer Levy.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 81199}, page_content='1. **[SqueezeBERT](https://huggingface.co/docs/transformers/model_doc/squeezebert)** (from Berkeley) released with the paper [SqueezeBERT: What can computer vision teach NLP about efficient neural networks?](https://arxiv.org/abs/2006.11316) by Forrest N. Iandola, Albert E. Shaw, Ravi Krishna, and Kurt W. Keutzer.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 81515}, page_content='1. **[SwiftFormer](https://huggingface.co/docs/transformers/model_doc/swiftformer)** (from MBZUAI) released with the paper [SwiftFormer: Efficient Additive Attention for Transformer-based Real-time Mobile Vision Applications](https://arxiv.org/abs/2303.15446) by Abdelrahman Shaker, Muhammad Maaz, Hanoona Rasheed, Salman Khan, Ming-Hsuan Yang, Fahad Shahbaz Khan.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 81880}, page_content='1. **[Swin Transformer](https://huggingface.co/docs/transformers/model_doc/swin)** (from Microsoft) released with the paper [Swin Transformer: Hierarchical Vision Transformer using Shifted Windows](https://arxiv.org/abs/2103.14030) by Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, Baining Guo.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 82203}, page_content='1. **[Swin Transformer V2](https://huggingface.co/docs/transformers/model_doc/swinv2)** (from Microsoft) released with the paper [Swin Transformer V2: Scaling Up Capacity and Resolution](https://arxiv.org/abs/2111.09883) by Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, Furu Wei, Baining Guo.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 82557}, page_content='1. **[Swin2SR](https://huggingface.co/docs/transformers/model_doc/swin2sr)** (from University of Würzburg) released with the paper [Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration](https://arxiv.org/abs/2209.11345) by Marcos V. Conde, Ui-Jin Choi, Maxime Burchi, Radu Timofte.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 82868}, page_content='1. **[SwitchTransformers](https://huggingface.co/docs/transformers/model_doc/switch_transformers)** (from Google) released with the paper [Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity](https://arxiv.org/abs/2101.03961) by William Fedus, Barret Zoph, Noam Shazeer.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 83180}, page_content='1. **[T5](https://huggingface.co/docs/transformers/model_doc/t5)** (from Google AI) released with the paper [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/abs/1910.10683) by Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 83555}, page_content='1. **[T5v1.1](https://huggingface.co/docs/transformers/model_doc/t5v1.1)** (from Google AI) released in the repository [google-research/text-to-text-transfer-transformer](https://github.com/google-research/text-to-text-transfer-transformer/blob/main/released_checkpoints.md#t511) by Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 83984}, page_content='1. **[Table Transformer](https://huggingface.co/docs/transformers/model_doc/table-transformer)** (from Microsoft Research) released with the paper [PubTables-1M: Towards Comprehensive Table Extraction From Unstructured Documents](https://arxiv.org/abs/2110.00061) by Brandon Smock, Rohith Pesala, Robin Abraham.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 84296}, page_content='1. **[TAPAS](https://huggingface.co/docs/transformers/model_doc/tapas)** (from Google AI) released with the paper [TAPAS: Weakly Supervised Table Parsing via Pre-training](https://arxiv.org/abs/2004.02349) by Jonathan Herzig, Paweł Krzysztof Nowak, Thomas Müller, Francesco Piccinno and Julian Martin Eisenschlos.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 84610}, page_content='1. **[TAPEX](https://huggingface.co/docs/transformers/model_doc/tapex)** (from Microsoft Research) released with the paper [TAPEX: Table Pre-training via Learning a Neural SQL Executor](https://arxiv.org/abs/2107.07653) by Qian Liu, Bei Chen, Jiaqi Guo, Morteza Ziyadi, Zeqi Lin, Weizhu Chen, Jian-Guang Lou.\\n1. **[Time Series Transformer](https://huggingface.co/docs/transformers/model_doc/time_series_transformer)** (from HuggingFace).'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 85048}, page_content='1. **[TimeSformer](https://huggingface.co/docs/transformers/model_doc/timesformer)** (from Facebook) released with the paper [Is Space-Time Attention All You Need for Video Understanding?](https://arxiv.org/abs/2102.05095) by Gedas Bertasius, Heng Wang, Lorenzo Torresani.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 85321}, page_content='1. **[Trajectory Transformer](https://huggingface.co/docs/transformers/model_doc/trajectory_transformers)** (from the University of California at Berkeley) released with the paper [Offline Reinforcement Learning as One Big Sequence Modeling Problem](https://arxiv.org/abs/2106.02039) by Michael Janner, Qiyang Li, Sergey Levine'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 85649}, page_content='1. **[Transformer-XL](https://huggingface.co/docs/transformers/model_doc/transfo-xl)** (from Google/CMU) released with the paper [Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context](https://arxiv.org/abs/1901.02860) by Zihang Dai*, Zhilin Yang*, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan Salakhutdinov.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 85980}, page_content='1. **[TrOCR](https://huggingface.co/docs/transformers/model_doc/trocr)** (from Microsoft), released together with the paper [TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models](https://arxiv.org/abs/2109.10282) by Minghao Li, Tengchao Lv, Lei Cui, Yijuan Lu, Dinei Florencio, Cha Zhang, Zhoujun Li, Furu Wei.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 86317}, page_content='1. **[TVLT](https://huggingface.co/docs/transformers/model_doc/tvlt)** (from UNC Chapel Hill) released with the paper [TVLT: Textless Vision-Language Transformer](https://arxiv.org/abs/2209.14156) by Zineng Tang, Jaemin Cho, Yixin Nie, Mohit Bansal.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 86567}, page_content='1. **[UL2](https://huggingface.co/docs/transformers/model_doc/ul2)** (from Google Research) released with the paper [Unifying Language Learning Paradigms](https://arxiv.org/abs/2205.05131v1) by Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng, Neil Houlsby, Donald Metzler'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 86893}, page_content='1. **[UMT5](https://huggingface.co/docs/transformers/model_doc/umt5)** (from Google Research) released with the paper [UniMax: Fairer and More Effective Language Sampling for Large-Scale Multilingual Pretraining](https://openreview.net/forum?id=kXwdL1cWOAi) by Hyung Won Chung, Xavier Garcia, Adam Roberts, Yi Tay, Orhan Firat, Sharan Narang, Noah Constant.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 87251}, page_content='1. **[UniSpeech](https://huggingface.co/docs/transformers/model_doc/unispeech)** (from Microsoft Research) released with the paper [UniSpeech: Unified Speech Representation Learning with Labeled and Unlabeled Data](https://arxiv.org/abs/2101.07597) by Chengyi Wang, Yu Wu, Yao Qian, Kenichi Kumatani, Shujie Liu, Furu Wei, Michael Zeng, Xuedong Huang.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 87603}, page_content='1. **[UniSpeechSat](https://huggingface.co/docs/transformers/model_doc/unispeech-sat)** (from Microsoft Research) released with the paper [UNISPEECH-SAT: UNIVERSAL SPEECH REPRESENTATION LEARNING WITH SPEAKER AWARE PRE-TRAINING](https://arxiv.org/abs/2110.05752) by Sanyuan Chen, Yu Wu, Chengyi Wang, Zhengyang Chen, Zhuo Chen, Shujie Liu, Jian Wu, Yao Qian, Furu Wei, Jinyu Li, Xiangzhan Yu.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 87995}, page_content='1. **[UPerNet](https://huggingface.co/docs/transformers/model_doc/upernet)** (from Peking University) released with the paper [Unified Perceptual Parsing for Scene Understanding](https://arxiv.org/abs/1807.10221) by Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, Jian Sun.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 88273}, page_content='1. **[VAN](https://huggingface.co/docs/transformers/model_doc/van)** (from Tsinghua University and Nankai University) released with the paper [Visual Attention Network](https://arxiv.org/abs/2202.09741) by Meng-Hao Guo, Cheng-Ze Lu, Zheng-Ning Liu, Ming-Ming Cheng, Shi-Min Hu.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 88551}, page_content='1. **[VideoMAE](https://huggingface.co/docs/transformers/model_doc/videomae)** (from Multimedia Computing Group, Nanjing University) released with the paper [VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training](https://arxiv.org/abs/2203.12602) by Zhan Tong, Yibing Song, Jue Wang, Limin Wang.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 88890}, page_content='1. **[ViLT](https://huggingface.co/docs/transformers/model_doc/vilt)** (from NAVER AI Lab/Kakao Enterprise/Kakao Brain) released with the paper [ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision](https://arxiv.org/abs/2102.03334) by Wonjae Kim, Bokyung Son, Ildoo Kim.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 89189}, page_content='1. **[Vision Transformer (ViT)](https://huggingface.co/docs/transformers/model_doc/vit)** (from Google AI) released with the paper [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929) by Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 89639}, page_content='1. **[VisualBERT](https://huggingface.co/docs/transformers/model_doc/visual_bert)** (from UCLA NLP) released with the paper [VisualBERT: A Simple and Performant Baseline for Vision and Language](https://arxiv.org/pdf/1908.03557) by Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, Kai-Wei Chang.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 89942}, page_content='1. **[ViT Hybrid](https://huggingface.co/docs/transformers/model_doc/vit_hybrid)** (from Google AI) released with the paper [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929) by Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 90385}, page_content='1. **[VitDet](https://huggingface.co/docs/transformers/model_doc/vitdet)** (from Meta AI) released with the paper [Exploring Plain Vision Transformer Backbones for Object Detection](https://arxiv.org/abs/2203.16527) by Yanghao Li, Hanzi Mao, Ross Girshick, Kaiming He.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 90654}, page_content='1. **[ViTMAE](https://huggingface.co/docs/transformers/model_doc/vit_mae)** (from Meta AI) released with the paper [Masked Autoencoders Are Scalable Vision Learners](https://arxiv.org/abs/2111.06377) by Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, Ross Girshick.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 90936}, page_content='1. **[ViTMatte](https://huggingface.co/docs/transformers/main/model_doc/vitmatte)** (from HUST-VL) rreleased with the paper [ViTMatte: Boosting Image Matting with Pretrained Plain Vision Transformers](https://arxiv.org/abs/2305.15272) by Jingfeng Yao, Xinggang Wang, Shusheng Yang, Baoyuan Wang.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 91232}, page_content='1. **[ViTMSN](https://huggingface.co/docs/transformers/model_doc/vit_msn)** (from Meta AI) released with the paper [Masked Siamese Networks for Label-Efficient Learning](https://arxiv.org/abs/2204.07141) by Mahmoud Assran, Mathilde Caron, Ishan Misra, Piotr Bojanowski, Florian Bordes, Pascal Vincent, Armand Joulin, Michael Rabbat, Nicolas Ballas.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 91581}, page_content='1. **[VITS](https://huggingface.co/docs/transformers/model_doc/vits)** (from Kakao Enterprise) released with the paper [Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech](https://arxiv.org/abs/2106.06103) by Jaehyeon Kim, Jungil Kong, Juhee Son.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 91869}, page_content='1. **[ViViT](https://huggingface.co/docs/transformers/model_doc/vivit)** (from Google Research) released with the paper [ViViT: A Video Vision Transformer](https://arxiv.org/abs/2103.15691) by Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lučić, Cordelia Schmid.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 92149}, page_content='1. **[Wav2Vec2](https://huggingface.co/docs/transformers/model_doc/wav2vec2)** (from Facebook AI) released with the paper [wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations](https://arxiv.org/abs/2006.11477) by Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 92453}, page_content='1. **[Wav2Vec2-Conformer](https://huggingface.co/docs/transformers/model_doc/wav2vec2-conformer)** (from Facebook AI) released with the paper [FAIRSEQ S2T: Fast Speech-to-Text Modeling with FAIRSEQ](https://arxiv.org/abs/2010.05171) by Changhan Wang, Yun Tang, Xutai Ma, Anne Wu, Sravya Popuri, Dmytro Okhonko, Juan Pino.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 92775}, page_content='1. **[Wav2Vec2Phoneme](https://huggingface.co/docs/transformers/model_doc/wav2vec2_phoneme)** (from Facebook AI) released with the paper [Simple and Effective Zero-shot Cross-lingual Phoneme Recognition](https://arxiv.org/abs/2109.11680) by Qiantong Xu, Alexei Baevski, Michael Auli.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 93059}, page_content='1. **[WavLM](https://huggingface.co/docs/transformers/model_doc/wavlm)** (from Microsoft Research) released with the paper [WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing](https://arxiv.org/abs/2110.13900) by Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, Jian Wu, Long Zhou, Shuo Ren, Yanmin Qian, Yao Qian, Jian Wu, Michael Zeng, Furu Wei.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 93516}, page_content='1. **[Whisper](https://huggingface.co/docs/transformers/model_doc/whisper)** (from OpenAI) released with the paper [Robust Speech Recognition via Large-Scale Weak Supervision](https://cdn.openai.com/papers/whisper.pdf) by Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, Ilya Sutskever.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 93826}, page_content='1. **[X-CLIP](https://huggingface.co/docs/transformers/model_doc/xclip)** (from Microsoft Research) released with the paper [Expanding Language-Image Pretrained Models for General Video Recognition](https://arxiv.org/abs/2208.02816) by Bolin Ni, Houwen Peng, Minghao Chen, Songyang Zhang, Gaofeng Meng, Jianlong Fu, Shiming Xiang, Haibin Ling.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 94170}, page_content='1. **[X-MOD](https://huggingface.co/docs/transformers/model_doc/xmod)** (from Meta AI) released with the paper [Lifting the Curse of Multilinguality by Pre-training Modular Transformers](http://dx.doi.org/10.18653/v1/2022.naacl-main.255) by Jonas Pfeiffer, Naman Goyal, Xi Lin, Xian Li, James Cross, Sebastian Riedel, Mikel Artetxe.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 94503}, page_content='1. **[XGLM](https://huggingface.co/docs/transformers/model_doc/xglm)** (From Facebook AI) released with the paper [Few-shot Learning with Multilingual Language Models](https://arxiv.org/abs/2112'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 94684}, page_content=\".org/abs/2112.10668) by Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer, Punit Singh Koura, Vishrav Chaudhary, Brian O'Horo, Jeff Wang, Luke Zettlemoyer, Zornitsa Kozareva, Mona Diab, Veselin Stoyanov, Xian Li.\"),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 95020}, page_content='1. **[XLM](https://huggingface.co/docs/transformers/model_doc/xlm)** (from Facebook) released together with the paper [Cross-lingual Language Model Pretraining](https://arxiv.org/abs/1901.07291) by Guillaume Lample and Alexis Conneau.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 95255}, page_content='1. **[XLM-ProphetNet](https://huggingface.co/docs/transformers/model_doc/xlm-prophetnet)** (from Microsoft Research) released with the paper [ProphetNet: Predicting Future N-gram for Sequence-to-Sequence Pre-training](https://arxiv.org/abs/2001.04063) by Yu Yan, Weizhen Qi, Yeyun Gong, Dayiheng Liu, Nan Duan, Jiusheng Chen, Ruofei Zhang and Ming Zhou.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 95609}, page_content='1. **[XLM-RoBERTa](https://huggingface.co/docs/transformers/model_doc/xlm-roberta)** (from Facebook AI), released together with the paper [Unsupervised Cross-lingual Representation Learning at Scale](https://arxiv.org/abs/1911.02116) by Alexis Conneau*, Kartikay Khandelwal*, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer and Veselin Stoyanov.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 96017}, page_content='1. **[XLM-RoBERTa-XL](https://huggingface.co/docs/transformers/model_doc/xlm-roberta-xl)** (from Facebook AI), released together with the paper [Larger-Scale Transformers for Multilingual Masked Language Modeling](https://arxiv.org/abs/2105.00572) by Naman Goyal, Jingfei Du, Myle Ott, Giri Anantharaman, Alexis Conneau.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 96338}, page_content='1. **[XLM-V](https://huggingface.co/docs/transformers/model_doc/xlm-v)** (from Meta AI) released with the paper [XLM-V: Overcoming the Vocabulary Bottleneck in Multilingual Masked Language Models](https://arxiv.org/abs/2301.10472) by Davis Liang, Hila Gonen, Yuning Mao, Rui Hou, Naman Goyal, Marjan Ghazvininejad, Luke Zettlemoyer, Madian Khabsa.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 96686}, page_content='1. **[XLNet](https://huggingface.co/docs/transformers/model_doc/xlnet)** (from Google/CMU) released with the paper [\\u200bXLNet: Generalized Autoregressive Pretraining for Language Understanding](https://arxiv.org/abs/1906.08237) by Zhilin Yang*, Zihang Dai*, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V. Le.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 97005}, page_content='1. **[XLS-R](https://huggingface.co/docs/transformers/model_doc/xls_r)** (from Facebook AI) released with the paper [XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale](https://arxiv.org/abs/2111.09296) by Arun Babu, Changhan Wang, Andros Tjandra, Kushal Lakhotia, Qiantong Xu, Naman Goyal, Kritika Singh, Patrick von Platen, Yatharth Saraf, Juan Pino, Alexei Baevski, Alexis Conneau, Michael Auli.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 97430}, page_content='1. **[XLSR-Wav2Vec2](https://huggingface.co/docs/transformers/model_doc/xlsr_wav2vec2)** (from Facebook AI) released with the paper [Unsupervised Cross-Lingual Representation Learning For Speech Recognition](https://arxiv.org/abs/2006.13979) by Alexis Conneau, Alexei Baevski, Ronan Collobert, Abdelrahman Mohamed, Michael Auli.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 97759}, page_content='1. **[YOLOS](https://huggingface.co/docs/transformers/model_doc/yolos)** (from Huazhong University of Science & Technology) released with the paper [You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection](https://arxiv.org/abs/2106.00666) by Yuxin Fang, Bencheng Liao, Xinggang Wang, Jiemin Fang, Jiyang Qi, Rui Wu, Jianwei Niu, Wenyu Liu.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 98133}, page_content='1. **[YOSO](https://huggingface.co/docs/transformers/model_doc/yoso)** (from the University of Wisconsin - Madison) released with the paper [You Only Sample (Almost) Once: Linear Cost Self-Attention Via Bernoulli Sampling](https://arxiv.org/abs/2111.09714) by Zhanpeng Zeng, Yunyang Xiong, Sathya N. Ravi, Shailesh Acharya, Glenn Fung, Vikas Singh.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 98482}, page_content='1. Want to contribute a new model? We have added a **detailed guide and templates** to guide you in the process of adding a new model. You can find them in the [`templates`](./templates) folder of the repository. Be sure to check the [contributing guidelines](./CONTRIBUTING.md) and contact the maintainers or open an issue to collect feedbacks before starting your PR.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 98853}, page_content='Чтобы проверить, есть ли у каждой модели реализация на Flax, PyTorch или TensorFlow, или связанный с ней токенизатор, поддерживаемый библиотекой 🤗 Tokenizers, обратитесь к [этой таблице](https://huggingface.co/docs/transformers/index#supported-frameworks).'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 99111}, page_content='Эти реализации были протестированы на нескольких наборах данных (см. примеры скриптов) и должны соответствовать производительности оригинальных реализаций. Более подробную информацию о производительности можно найти в разделе \"Примеры\" [документации](https://github.com/huggingface/transformers/tree/main/examples).\\n\\n\\n## Изучи больше'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 99446}, page_content='| Секция | Описание |\\n|-|-|\\n| [Документация](https://huggingface.co/docs/transformers/) | Полная документация по API и гайды |\\n| [Краткие описания задач](https://huggingface.co/docs/transformers/task_summary) | Задачи поддерживаются 🤗 Transformers |\\n| [Пособие по предварительной обработке](https://huggingface.co/docs/transformers/preprocessing) | Использование класса `Tokenizer` для подготовки данных для моделей |'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 99864}, page_content='| [Обучение и доработка](https://huggingface.co/docs/transformers/training) | Использование моделей, предоставляемых 🤗 Transformers, в цикле обучения PyTorch/TensorFlow и API `Trainer`. |\\n| [Быстрый тур: Тонкая настройка/скрипты использования](https://github.com/huggingface/transformers/tree/main/examples) | Примеры скриптов для тонкой настройки моделей на широком спектре задач |'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 100247}, page_content='| [Совместное использование и загрузка моделей](https://huggingface.co/docs/transformers/model_sharing) | Загружайте и делитесь с сообществом своими доработанными моделями |'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 100422}, page_content='## Цитирование'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 100438}, page_content='Теперь у нас есть [статья](https://www.aclweb.org/anthology/2020.emnlp-demos.6/), которую можно цитировать для библиотеки 🤗 Transformers:\\n```bibtex\\n@inproceedings{wolf-etal-2020-transformers,\\n    title = \"Transformers: State-of-the-Art Natural Language Processing\",'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 100708}, page_content='author = \"Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and Rémi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush\",'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 101109}, page_content='booktitle = \"Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations\",\\n    month = oct,\\n    year = \"2020\",\\n    address = \"Online\",\\n    publisher = \"Association for Computational Linguistics\",\\n    url = \"https://www.aclweb.org/anthology/2020.emnlp-demos.6\",\\n    pages = \"38--45\"\\n}\\n```'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 0}, page_content='!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 232}, page_content='http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 583}, page_content='⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer.\\n\\n-->\\n\\n# Share a model'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 746}, page_content='-->\\n\\n# Share a model\\n\\nThe last two tutorials showed how you can fine-tune a model with PyTorch, Keras, and 🤗 Accelerate for distributed setups. The next step is to share your model with the community! At Hugging Face, we believe in openly sharing knowledge and resources to democratize artificial intelligence for everyone. We encourage you to consider sharing your model with the community to help others save time and resources.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 1178}, page_content='In this tutorial, you will learn two methods for sharing a trained or fine-tuned model on the [Model Hub](https://huggingface.co/models):\\n\\n- Programmatically push your files to the Hub.\\n- Drag-and-drop your files to the Hub with the web interface.\\n\\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/XvSGPZFEjDY\" title=\"YouTube video player\"\\nframeborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope;\\npicture-in-picture\" allowfullscreen></iframe>'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 1677}, page_content='<Tip>\\n\\nTo share a model with the community, you need an account on [huggingface.co](https://huggingface.co/join). You can also join an existing organization or create a new one.\\n\\n</Tip>\\n\\n## Repository features\\n\\nEach repository on the Model Hub behaves like a typical GitHub repository. Our repositories offer versioning, commit history, and the ability to visualize differences.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 2057}, page_content=\"The Model Hub's built-in versioning is based on git and [git-lfs](https://git-lfs.github.com/). In other words, you can treat one model as one repository, enabling greater access control and scalability. Version control allows *revisions*, a method for pinning a specific version of a model with a commit hash, tag or branch.\\n\\nAs a result, you can load a specific model version with the `revision` parameter:\"),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 2467}, page_content='```py\\n>>> model = AutoModel.from_pretrained(\\n...     \"julien-c/EsperBERTo-small\", revision=\"v2.0.1\"  # tag name, or branch name, or commit hash\\n... )\\n```\\n\\nFiles are also easily edited in a repository, and you can view the commit history as well as the difference:\\n\\n![vis_diff](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/vis_diff.png)\\n\\n## Setup'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 2837}, page_content='## Setup\\n\\nBefore sharing a model to the Hub, you will need your Hugging Face credentials. If you have access to a terminal, run the following command in the virtual environment where 🤗 Transformers is installed. This will store your access token in your Hugging Face cache folder (`~/.cache/` by default):\\n\\n```bash\\nhuggingface-cli login\\n```'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 3144}, page_content='```bash\\nhuggingface-cli login\\n```\\n\\nIf you are using a notebook like Jupyter or Colaboratory, make sure you have the [`huggingface_hub`](https://huggingface.co/docs/hub/adding-a-library) library installed. This library allows you to programmatically interact with the Hub.\\n\\n```bash\\npip install huggingface_hub\\n```\\n\\nThen use `notebook_login` to sign-in to the Hub, and follow the link [here](https://huggingface.co/settings/token) to generate a token to login with:'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 3609}, page_content='```py\\n>>> from huggingface_hub import notebook_login\\n\\n>>> notebook_login()\\n```\\n\\n## Convert a model for all frameworks\\n\\nTo ensure your model can be used by someone working with a different framework, we recommend you convert and upload your model with both PyTorch and TensorFlow checkpoints. While users are still able to load your model from a different framework if you skip this step, it will be slower because 🤗 Transformers will need to convert the checkpoint on-the-fly.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 4087}, page_content='Converting a checkpoint for another framework is easy. Make sure you have PyTorch and TensorFlow installed (see [here](installation) for installation instructions), and then find the specific model for your task in the other framework. \\n\\n<frameworkcontent>\\n<pt>\\nSpecify `from_tf=True` to convert a checkpoint from TensorFlow to PyTorch:'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 4425}, page_content='```py\\n>>> pt_model = DistilBertForSequenceClassification.from_pretrained(\"path/to/awesome-name-you-picked\", from_tf=True)\\n>>> pt_model.save_pretrained(\"path/to/awesome-name-you-picked\")\\n```\\n</pt>\\n<tf>\\nSpecify `from_pt=True` to convert a checkpoint from PyTorch to TensorFlow:\\n\\n```py\\n>>> tf_model = TFDistilBertForSequenceClassification.from_pretrained(\"path/to/awesome-name-you-picked\", from_pt=True)\\n```\\n\\nThen you can save your new TensorFlow model with its new checkpoint:'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 4901}, page_content='```py\\n>>> tf_model.save_pretrained(\"path/to/awesome-name-you-picked\")\\n```\\n</tf>\\n<jax>\\nIf a model is available in Flax, you can also convert a checkpoint from PyTorch to Flax:\\n\\n```py\\n>>> flax_model = FlaxDistilBertForSequenceClassification.from_pretrained(\\n...     \"path/to/awesome-name-you-picked\", from_pt=True\\n... )\\n```\\n</jax>\\n</frameworkcontent>\\n\\n## Push a model during training\\n\\n<frameworkcontent>\\n<pt>\\n<Youtube id=\"Z1-XMy-GNLQ\"/>'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 5337}, page_content='Sharing a model to the Hub is as simple as adding an extra parameter or callback. Remember from the [fine-tuning tutorial](training), the [`TrainingArguments`] class is where you specify hyperparameters and additional training options. One of these training options includes the ability to push a model directly to the Hub. Set `push_to_hub=True` in your [`TrainingArguments`]:\\n\\n```py\\n>>> training_args = TrainingArguments(output_dir=\"my-awesome-model\", push_to_hub=True)\\n```'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 5814}, page_content='Pass your training arguments as usual to [`Trainer`]:\\n\\n```py\\n>>> trainer = Trainer(\\n...     model=model,\\n...     args=training_args,\\n...     train_dataset=small_train_dataset,\\n...     eval_dataset=small_eval_dataset,\\n...     compute_metrics=compute_metrics,\\n... )\\n```'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 6083}, page_content='After you fine-tune your model, call [`~transformers.Trainer.push_to_hub`] on [`Trainer`] to push the trained model to the Hub. 🤗 Transformers will even automatically add training hyperparameters, training results and framework versions to your model card!\\n\\n```py\\n>>> trainer.push_to_hub()\\n```\\n</pt>\\n<tf>\\nShare a model to the Hub with [`PushToHubCallback`]. In the [`PushToHubCallback`] function, add:'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 6486}, page_content='- An output directory for your model.\\n- A tokenizer.\\n- The `hub_model_id`, which is your Hub username and model name.\\n\\n```py\\n>>> from transformers import PushToHubCallback\\n\\n>>> push_to_hub_callback = PushToHubCallback(\\n...     output_dir=\"./your_model_save_path\", tokenizer=tokenizer, hub_model_id=\"your-username/my-awesome-model\"\\n... )\\n```\\n\\nAdd the callback to [`fit`](https://keras.io/api/models/model_training_apis/), and 🤗 Transformers will push the trained model to the Hub:'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 6967}, page_content='```py\\n>>> model.fit(tf_train_dataset, validation_data=tf_validation_dataset, epochs=3, callbacks=push_to_hub_callback)\\n```\\n</tf>\\n</frameworkcontent>\\n\\n## Use the `push_to_hub` function\\n\\nYou can also call `push_to_hub` directly on your model to upload it to the Hub.\\n\\nSpecify your model name in `push_to_hub`:\\n\\n```py\\n>>> pt_model.push_to_hub(\"my-awesome-model\")\\n```'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 7332}, page_content='This creates a repository under your username with the model name `my-awesome-model`. Users can now load your model with the `from_pretrained` function:\\n\\n```py\\n>>> from transformers import AutoModel\\n\\n>>> model = AutoModel.from_pretrained(\"your_username/my-awesome-model\")\\n```\\n\\nIf you belong to an organization and want to push your model under the organization name instead, just add it to the `repo_id`:\\n\\n```py\\n>>> pt_model.push_to_hub(\"my-awesome-org/my-awesome-model\")\\n```'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 7809}, page_content='The `push_to_hub` function can also be used to add other files to a model repository. For example, add a tokenizer to a model repository:\\n\\n```py\\n>>> tokenizer.push_to_hub(\"my-awesome-model\")\\n```\\n\\nOr perhaps you\\'d like to add the TensorFlow version of your fine-tuned PyTorch model:\\n\\n```py\\n>>> tf_model.push_to_hub(\"my-awesome-model\")\\n```'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 8148}, page_content=\"Now when you navigate to your Hugging Face profile, you should see your newly created model repository. Clicking on the **Files** tab will display all the files you've uploaded to the repository.\\n\\nFor more details on how to create and upload files to a repository, refer to the Hub documentation [here](https://huggingface.co/docs/hub/how-to-upstream).\\n\\n## Upload with the web interface\"),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 8502}, page_content=\"## Upload with the web interface\\n\\nUsers who prefer a no-code approach are able to upload a model through the Hub's web interface. Visit [huggingface.co/new](https://huggingface.co/new) to create a new repository:\\n\\n![new_model_repo](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/new_model_repo.png)\\n\\nFrom here, add some information about your model:\"),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 8884}, page_content='- Select the **owner** of the repository. This can be yourself or any of the organizations you belong to.\\n- Pick a name for your model, which will also be the repository name.\\n- Choose whether your model is public or private.\\n- Specify the license usage for your model.\\n\\nNow click on the **Files** tab and click on the **Add file** button to upload a new file to your repository. Then drag-and-drop a file to upload and add a commit message.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 9327}, page_content=\"![upload_file](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/upload_file.png)\\n\\n## Add a model card\\n\\nTo make sure users understand your model's capabilities, limitations, potential biases and ethical considerations, please add a model card to your repository. The model card is defined in the `README.md` file. You can add a model card by:\\n\\n* Manually creating and uploading a `README.md` file.\\n* Clicking on the **Edit model card** button in your model repository.\"),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 9825}, page_content=\"Take a look at the DistilBert [model card](https://huggingface.co/distilbert-base-uncased) for a good example of the type of information a model card should include. For more details about other options you can control in the `README.md` file such as a model's carbon footprint or widget examples, refer to the documentation [here](https://huggingface.co/docs/hub/models-cards).\"),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 0}, page_content='!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 232}, page_content='http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 583}, page_content='⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer.\\n\\n-->\\n\\n# TimeSformer\\n\\n## Overview'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 746}, page_content='-->\\n\\n# TimeSformer\\n\\n## Overview\\n\\nThe TimeSformer model was proposed in [TimeSformer: Is Space-Time Attention All You Need for Video Understanding?](https://arxiv.org/abs/2102.05095) by Facebook Research.\\nThis work is a milestone in action-recognition field being the first video transformer. It inspired many transformer based video understanding and classification papers.\\n\\nThe abstract from the paper is the following:'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 1168}, page_content='*We present a convolution-free approach to video classification built exclusively on self-attention over space and time. Our method, named \"TimeSformer,\" adapts the standard Transformer architecture to video by enabling spatiotemporal feature learning directly from a sequence of frame-level patches'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 1467}, page_content='. Our experimental study compares different self-attention schemes and suggests that \"divided attention,\" where temporal attention and spatial attention are separately applied within each block, leads to the best video classification accuracy among the design choices considered. Despite the radically new design, TimeSformer achieves state-of-the-art results on several action recognition benchmarks, including the best reported accuracy on Kinetics-400 and Kinetics-600'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 1938}, page_content='. Finally, compared to 3D convolutional networks, our model is faster to train, it can achieve dramatically higher test efficiency (at a small drop in accuracy), and it can also be applied to much longer video clips (over one minute long). Code and models are available at: [this https URL](https://github.com/facebookresearch/TimeSformer).*'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 2281}, page_content='This model was contributed by [fcakyon](https://huggingface.co/fcakyon).\\nThe original code can be found [here](https://github.com/facebookresearch/TimeSformer).\\n\\n## Usage tips\\n\\nThere are many pretrained variants. Select your pretrained model based on the dataset it is trained on. Moreover,\\nthe number of input frames per clip changes based on the model size so you should consider this parameter while selecting your pretrained model.\\n\\n## Resources'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 2718}, page_content='## Resources\\n\\n- [Video classification task guide](../tasks/video_classification)\\n\\n## TimesformerConfig\\n\\n[[autodoc]] TimesformerConfig\\n\\n## TimesformerModel\\n\\n[[autodoc]] TimesformerModel\\n    - forward\\n\\n## TimesformerForVideoClassification\\n\\n[[autodoc]] TimesformerForVideoClassification\\n    - forward'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 0}, page_content='!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 232}, page_content='http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 583}, page_content='⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer.\\n\\n-->\\n\\n# Swin Transformer V2\\n\\n## Overview\\n\\nThe Swin Transformer V2 model was proposed in [Swin Transformer V2: Scaling Up Capacity and Resolution](https://arxiv.org/abs/2111.09883) by Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, Furu Wei, Baining Guo.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 1059}, page_content='The abstract from the paper is the following:'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 1106}, page_content='*Large-scale NLP models have been shown to significantly improve the performance on language tasks with no signs of saturation. They also demonstrate amazing few-shot capabilities like that of human beings. This paper aims to explore large-scale models in computer vision. We tackle three major issues in training and application of large vision models, including training instability, resolution gaps between pre-training and fine-tuning, and hunger on labelled data'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 1573}, page_content='. Three main techniques are proposed: 1) a residual-post-norm method combined with cosine attention to improve training stability; 2) A log-spaced continuous position bias method to effectively transfer models pre-trained using low-resolution images to downstream tasks with high-resolution inputs; 3) A self-supervised pre-training method, SimMIM, to reduce the needs of vast labeled images'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 1964}, page_content='. Through these techniques, this paper successfully trained a 3 billion-parameter Swin Transformer V2 model, which is the largest dense vision model to date, and makes it capable of training with images of up to 1,536×1,536 resolution. It set new performance records on 4 representative vision tasks, including ImageNet-V2 image classification, COCO object detection, ADE20K semantic segmentation, and Kinetics-400 video action classification'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 2406}, page_content=\". Also note our training is much more efficient than that in Google's billion-level visual models, which consumes 40 times less labelled data and 40 times less training time.*\"),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 2583}, page_content='This model was contributed by [nandwalritik](https://huggingface.co/nandwalritik).\\nThe original code can be found [here](https://github.com/microsoft/Swin-Transformer).\\n\\n## Resources\\n\\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with Swin Transformer v2.\\n\\n<PipelineTag pipeline=\"image-classification\"/>'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 2891}, page_content='<PipelineTag pipeline=\"image-classification\"/>\\n\\n- [`Swinv2ForImageClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb).\\n- See also: [Image classification task guide](../tasks/image_classification)\\n\\nBesides that:'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 3309}, page_content=\"Besides that:\\n\\n- [`Swinv2ForMaskedImageModeling`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-pretraining).\\n\\nIf you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\\n\\n## Swinv2Config\\n\\n[[autodoc]] Swinv2Config\\n\\n## Swinv2Model\"),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 3734}, page_content='[[autodoc]] Swinv2Config\\n\\n## Swinv2Model\\n\\n[[autodoc]] Swinv2Model\\n    - forward\\n\\n## Swinv2ForMaskedImageModeling\\n\\n[[autodoc]] Swinv2ForMaskedImageModeling\\n    - forward\\n\\n## Swinv2ForImageClassification\\n\\n[[autodoc]] transformers.Swinv2ForImageClassification\\n    - forward'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 0}, page_content='!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 232}, page_content='http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 583}, page_content='⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer.\\n\\n-->\\n\\n# RemBERT\\n\\n## Overview\\n\\nThe RemBERT model was proposed in [Rethinking Embedding Coupling in Pre-trained Language Models](https://arxiv.org/abs/2010.12821) by Hyung Won Chung, Thibault Févry, Henry Tsai, Melvin Johnson, Sebastian Ruder.\\n\\nThe abstract from the paper is the following:'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 1035}, page_content='*We re-evaluate the standard practice of sharing weights between input and output embeddings in state-of-the-art\\npre-trained language models. We show that decoupled embeddings provide increased modeling flexibility, allowing us to\\nsignificantly improve the efficiency of parameter allocation in the input embedding of multilingual models. By\\nreallocating the input embedding parameters in the Transformer layers, we achieve dramatically better performance on'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 1494}, page_content=\"standard natural language understanding tasks with the same number of parameters during fine-tuning. We also show that\\nallocating additional capacity to the output embedding provides benefits to the model that persist through the\\nfine-tuning stage even though the output embedding is discarded after pre-training. Our analysis shows that larger\\noutput embeddings prevent the model's last layers from overspecializing to the pre-training task and encourage\"),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 1950}, page_content='Transformer representations to be more general and more transferable to other tasks and languages. Harnessing these\\nfindings, we are able to train models that achieve strong performance on the XTREME benchmark without increasing the\\nnumber of parameters at the fine-tuning stage.*'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 2232}, page_content='## Usage tips\\n\\nFor fine-tuning, RemBERT can be thought of as a bigger version of mBERT with an ALBERT-like factorization of the\\nembedding layer. The embeddings are not tied in pre-training, in contrast with BERT, which enables smaller input\\nembeddings (preserved during fine-tuning) and bigger output embeddings (discarded at fine-tuning). The tokenizer is\\nalso similar to the Albert one rather than the BERT one.\\n\\n## Resources'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 2647}, page_content='## Resources\\n\\n- [Text classification task guide](../tasks/sequence_classification)\\n- [Token classification task guide](../tasks/token_classification)\\n- [Question answering task guide](../tasks/question_answering)\\n- [Causal language modeling task guide](../tasks/language_modeling)\\n- [Masked language modeling task guide](../tasks/masked_language_modeling)\\n- [Multiple choice task guide](../tasks/multiple_choice)\\n\\n## RemBertConfig\\n\\n[[autodoc]] RemBertConfig\\n\\n## RemBertTokenizer'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 3079}, page_content='[[autodoc]] RemBertConfig\\n\\n## RemBertTokenizer\\n\\n[[autodoc]] RemBertTokenizer\\n    - build_inputs_with_special_tokens\\n    - get_special_tokens_mask\\n    - create_token_type_ids_from_sequences\\n    - save_vocabulary\\n\\n## RemBertTokenizerFast\\n\\n[[autodoc]] RemBertTokenizerFast\\n    - build_inputs_with_special_tokens\\n    - get_special_tokens_mask\\n    - create_token_type_ids_from_sequences\\n    - save_vocabulary\\n\\n<frameworkcontent>\\n<pt>\\n\\n## RemBertModel\\n\\n[[autodoc]] RemBertModel\\n    - forward'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 3526}, page_content='[[autodoc]] RemBertModel\\n    - forward\\n\\n## RemBertForCausalLM\\n\\n[[autodoc]] RemBertForCausalLM\\n    - forward\\n\\n## RemBertForMaskedLM\\n\\n[[autodoc]] RemBertForMaskedLM\\n    - forward\\n\\n## RemBertForSequenceClassification\\n\\n[[autodoc]] RemBertForSequenceClassification\\n    - forward\\n\\n## RemBertForMultipleChoice\\n\\n[[autodoc]] RemBertForMultipleChoice\\n    - forward\\n\\n## RemBertForTokenClassification\\n\\n[[autodoc]] RemBertForTokenClassification\\n    - forward\\n\\n## RemBertForQuestionAnswering'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 3973}, page_content='## RemBertForQuestionAnswering\\n\\n[[autodoc]] RemBertForQuestionAnswering\\n    - forward\\n\\n</pt>\\n<tf>\\n\\n## TFRemBertModel\\n\\n[[autodoc]] TFRemBertModel\\n    - call\\n\\n## TFRemBertForMaskedLM\\n\\n[[autodoc]] TFRemBertForMaskedLM\\n    - call\\n\\n## TFRemBertForCausalLM\\n\\n[[autodoc]] TFRemBertForCausalLM\\n    - call\\n\\n## TFRemBertForSequenceClassification\\n\\n[[autodoc]] TFRemBertForSequenceClassification\\n    - call\\n\\n## TFRemBertForMultipleChoice\\n\\n[[autodoc]] TFRemBertForMultipleChoice\\n    - call'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 4450}, page_content='## TFRemBertForTokenClassification\\n\\n[[autodoc]] TFRemBertForTokenClassification\\n    - call\\n\\n## TFRemBertForQuestionAnswering\\n\\n[[autodoc]] TFRemBertForQuestionAnswering\\n    - call\\n\\n</tf>\\n</frameworkcontent>'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 0}, page_content='!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 232}, page_content='http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 583}, page_content='⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer.\\n\\n-->\\n\\n# Autoformer\\n\\n## Overview\\n\\nThe Autoformer model was proposed in [Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting](https://arxiv.org/abs/2106.13008) by Haixu Wu, Jiehui Xu, Jianmin Wang, Mingsheng Long.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 1000}, page_content='This model augments the Transformer as a deep decomposition architecture, which can progressively decompose the trend and seasonal components during the forecasting process.\\n\\nThe abstract from the paper is the following:'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 1222}, page_content='*Extending the forecasting time is a critical demand for real applications, such as extreme weather early warning and long-term energy consumption planning. This paper studies the long-term forecasting problem of time series. Prior Transformer-based models adopt various self-attention mechanisms to discover the long-range dependencies. However, intricate temporal patterns of the long-term future prohibit the model from finding reliable dependencies'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 1674}, page_content='. Also, Transformers have to adopt the sparse versions of point-wise self-attentions for long series efficiency, resulting in the information utilization bottleneck. Going beyond Transformers, we design Autoformer as a novel decomposition architecture with an Auto-Correlation mechanism. We break with the pre-processing convention of series decomposition and renovate it as a basic inner block of deep models'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 2083}, page_content='. This design empowers Autoformer with progressive decomposition capacities for complex time series. Further, inspired by the stochastic process theory, we design the Auto-Correlation mechanism based on the series periodicity, which conducts the dependencies discovery and representation aggregation at the sub-series level. Auto-Correlation outperforms self-attention in both efficiency and accuracy'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 2483}, page_content='. In long-term forecasting, Autoformer yields state-of-the-art accuracy, with a 38% relative improvement on six benchmarks, covering five practical applications: energy, traffic, economics, weather and disease.*'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 2696}, page_content='This model was contributed by [elisim](https://huggingface.co/elisim) and [kashif](https://huggingface.co/kashif).\\nThe original code can be found [here](https://github.com/thuml/Autoformer).\\n\\n## Resources'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 2888}, page_content=\"## Resources\\n\\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started. If you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\"),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 3228}, page_content='- Check out the Autoformer blog-post in HuggingFace blog: [Yes, Transformers are Effective for Time Series Forecasting (+ Autoformer)](https://huggingface.co/blog/autoformer)\\n\\n## AutoformerConfig\\n\\n[[autodoc]] AutoformerConfig\\n\\n## AutoformerModel\\n\\n[[autodoc]] AutoformerModel\\n    - forward\\n\\n## AutoformerForPrediction\\n\\n[[autodoc]] AutoformerForPrediction\\n    - forward'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 0}, page_content='!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 232}, page_content='http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 583}, page_content='⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer.\\n\\n-->\\n\\n# LXMERT\\n\\n## Overview'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 774}, page_content='The LXMERT model was proposed in [LXMERT: Learning Cross-Modality Encoder Representations from Transformers](https://arxiv.org/abs/1908.07490) by Hao Tan & Mohit Bansal. It is a series of bidirectional transformer encoders\\n(one for the vision modality, one for the language modality, and then one to fuse both modalities) pretrained using a\\ncombination of masked language modeling, visual-language text alignment, ROI-feature regression, masked'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 1219}, page_content='visual-attribute modeling, masked visual-object modeling, and visual-question answering objectives. The pretraining\\nconsists of multiple multi-modal datasets: MSCOCO, Visual-Genome + Visual-Genome Question Answering, VQA 2.0, and GQA.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 1455}, page_content='The abstract from the paper is the following:'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 1502}, page_content='*Vision-and-language reasoning requires an understanding of visual concepts, language semantics, and, most importantly,\\nthe alignment and relationships between these two modalities. We thus propose the LXMERT (Learning Cross-Modality\\nEncoder Representations from Transformers) framework to learn these vision-and-language connections. In LXMERT, we\\nbuild a large-scale Transformer model that consists of three encoders: an object relationship encoder, a language'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 1965}, page_content='encoder, and a cross-modality encoder. Next, to endow our model with the capability of connecting vision and language\\nsemantics, we pre-train the model with large amounts of image-and-sentence pairs, via five diverse representative\\npretraining tasks: masked language modeling, masked object prediction (feature regression and label classification),\\ncross-modality matching, and image question answering. These tasks help in learning both intra-modality and'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 2422}, page_content='cross-modality relationships. After fine-tuning from our pretrained parameters, our model achieves the state-of-the-art\\nresults on two visual question answering datasets (i.e., VQA and GQA). We also show the generalizability of our\\npretrained cross-modality model by adapting it to a challenging visual-reasoning task, NLVR, and improve the previous\\nbest result by 22% absolute (54% to 76%). Lastly, we demonstrate detailed ablation studies to prove that both our novel'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 2892}, page_content='model components and pretraining strategies significantly contribute to our strong results; and also present several\\nattention visualizations for the different encoders*'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 3063}, page_content='This model was contributed by [eltoto1219](https://huggingface.co/eltoto1219). The original code can be found [here](https://github.com/airsplay/lxmert).\\n\\n## Usage tips'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 3233}, page_content='- Bounding boxes are not necessary to be used in the visual feature embeddings, any kind of visual-spacial features\\n  will work.\\n- Both the language hidden states and the visual hidden states that LXMERT outputs are passed through the\\n  cross-modality layer, so they contain information from both modalities. To access a modality that only attends to\\n  itself, select the vision/language hidden states from the first input in the tuple.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 3670}, page_content='- The bidirectional cross-modality encoder attention only returns attention values when the language modality is used\\n  as the input and the vision modality is used as the context vector. Further, while the cross-modality encoder\\n  contains self-attention for each respective modality and cross-attention, only the cross attention is returned and\\n  both self attention outputs are disregarded.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 4065}, page_content='## Resources\\n\\n- [Question answering task guide](../tasks/question_answering)\\n\\n## LxmertConfig\\n\\n[[autodoc]] LxmertConfig\\n\\n## LxmertTokenizer\\n\\n[[autodoc]] LxmertTokenizer\\n\\n## LxmertTokenizerFast\\n\\n[[autodoc]] LxmertTokenizerFast\\n\\n## Lxmert specific outputs\\n\\n[[autodoc]] models.lxmert.modeling_lxmert.LxmertModelOutput\\n\\n[[autodoc]] models.lxmert.modeling_lxmert.LxmertForPreTrainingOutput\\n\\n[[autodoc]] models.lxmert.modeling_lxmert.LxmertForQuestionAnsweringOutput'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 4527}, page_content='[[autodoc]] models.lxmert.modeling_tf_lxmert.TFLxmertModelOutput\\n\\n[[autodoc]] models.lxmert.modeling_tf_lxmert.TFLxmertForPreTrainingOutput\\n\\n<frameworkcontent>\\n<pt>\\n\\n## LxmertModel\\n\\n[[autodoc]] LxmertModel\\n    - forward\\n\\n## LxmertForPreTraining\\n\\n[[autodoc]] LxmertForPreTraining\\n    - forward\\n\\n## LxmertForQuestionAnswering\\n\\n[[autodoc]] LxmertForQuestionAnswering\\n    - forward\\n\\n</pt>\\n<tf>\\n\\n## TFLxmertModel\\n\\n[[autodoc]] TFLxmertModel\\n    - call\\n\\n## TFLxmertForPreTraining'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 4974}, page_content='## TFLxmertForPreTraining\\n\\n[[autodoc]] TFLxmertForPreTraining\\n    - call\\n\\n</tf>\\n</frameworkcontent>'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 0}, page_content='Author: [@vasudevgupta7](https://github.com/thevasudevgupta/)\\n\\n## Intro\\n\\nIn this project, we fine-tuned [**BigBird**](https://arxiv.org/abs/2007.14062) on [**natural-questions**](https://huggingface.co/datasets/natural_questions) dataset for **question-answering** task on long documents. **BigBird**, is a **sparse-attention based transformer** which extends Transformer based models, such as BERT to much **longer sequences**.\\n\\nRead more about BigBird at https://huggingface.co/blog/big-bird'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 495}, page_content='## Fine-tuning\\n\\n**Setup**\\n\\nYou need to install jax yourself by following the official docs ([refer this](https://github.com/google/jax#installation)). Other requirements for this project can be installed by running following command:\\n\\n```shell\\npip3 install -qr requirements.txt\\n```\\n\\n**Download & prepare dataset**'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 778}, page_content='**Download & prepare dataset**\\n\\nThe Natural Questions corpus contains questions from real users, and it requires QA systems to read and comprehend an entire Wikipedia article that may or may not contain the answer to the question. This corpus takes ~100 GB on disk. We have used HuggingFace datasets to download & process the dataset.\\n\\n```shell\\n# just run following CMD\\npython3 prepare_natural_questions.py'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 1186}, page_content=\"# this will download the whole dataset from HuggingFace Hub & will make it ready for training\\n# this script takes ~3 hours to process the dataset\\n```\\n\\n**Launch Training**\\n\\nWe have trained on Cloud's TPU v3-8. Each epoch took around 4.5 hours and the model got converged in just 2 epochs. You can see complete training args in [this script](bigbird_flax.py).\\n\\n```shell\\n# just run following CMD\\npython3 train.py\"),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 1597}, page_content='# In case, you want to try hparams tuning, you can run wandb sweep\\nwandb sweep --project=bigbird sweep_flax.yaml\\nwandb agent <agent-id-obtained-by-above-CMD>\\n```\\n\\n## Evaluation\\n\\nOur evaluation script is different from the original script and we are evaluating sequences with length up to 4096 for simplicity. We managed to get the **EM score of ~55.2** using our evaluation script.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 1980}, page_content='```shell\\n# download validation-dataset first\\nmkdir natural-questions-validation\\nwget https://huggingface.co/datasets/vasudevgupta/natural-questions-validation/resolve/main/natural_questions-validation.arrow -P natural-questions-validation\\nwget https://huggingface.co/datasets/vasudevgupta/natural-questions-validation/resolve/main/dataset_info.json -P natural-questions-validation'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 2361}, page_content='wget https://huggingface.co/datasets/vasudevgupta/natural-questions-validation/resolve/main/state.json -P natural-questions-validation'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 2497}, page_content='# simply run following command\\npython3 evaluate.py\\n```\\n\\nYou can find our checkpoint on HuggingFace Hub ([see this](https://huggingface.co/vasudevgupta/flax-bigbird-natural-questions)). In case you are interested in PyTorch BigBird fine-tuning, you can refer to [this repositary](https://github.com/thevasudevgupta/bigbird).'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 0}, page_content='!---\\nCopyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\");\\nyou may not use this file except in compliance with the License.\\nYou may obtain a copy of the License at\\n\\n    http://www.apache.org/licenses/LICENSE-2.0'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 238}, page_content='http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \"AS IS\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\nSee the License for the specific language governing permissions and\\nlimitations under the License.\\n-->\\n\\n# 🤗 Transformers Notebooks\\n\\nYou can find here a list of the official notebooks provided by Hugging Face.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 699}, page_content=\"Also, we would like to list here interesting content created by the community.\\nIf you wrote some notebook(s) leveraging 🤗 Transformers and would like to be listed here, please open a\\nPull Request so it can be included under the Community notebooks.\\n\\n\\n## Hugging Face's notebooks 🤗\\n\\n### Documentation notebooks\\n\\nYou can open any page of the documentation as a notebook in Colab (there is a button directly on said pages) but they are also listed here if you need them:\"),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 1168}, page_content='| Notebook     |      Description      |   |   |\\n|:----------|:-------------|:-------------|------:|'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 1269}, page_content='| [Quicktour of the library](https://github.com/huggingface/notebooks/blob/main/transformers_doc/en/quicktour.ipynb)  | A presentation of the various APIs in Transformers |[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/quicktour.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 1679}, page_content='.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/en/transformers_doc/quicktour.ipynb)|'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 1816}, page_content='| [Summary of the tasks](https://github.com/huggingface/notebooks/blob/main/transformers_doc/en/task_summary.ipynb)  | How to run the models of the Transformers library task by task |[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/task_summary.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 2240}, page_content='.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/en/task_summary.ipynb)|'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 2380}, page_content='| [Preprocessing data](https://github.com/huggingface/notebooks/blob/main/transformers_doc/en/preprocessing.ipynb)  | How to use a tokenizer to preprocess your data |[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/preprocessing.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 2788}, page_content='.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/en/preprocessing.ipynb)|'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 2929}, page_content='| [Fine-tuning a pretrained model](https://github.com/huggingface/notebooks/blob/main/transformers_doc/en/training.ipynb)  | How to use the Trainer to fine-tune a pretrained model |[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/training.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 3347}, page_content='.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/en/training.ipynb)|'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 3483}, page_content='| [Summary of the tokenizers](https://github.com/huggingface/notebooks/blob/main/transformers_doc/en/tokenizer_summary.ipynb)  | The differences between the tokenizers algorithm |[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/tokenizer_summary.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 3908}, page_content='.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/en/tokenizer_summary.ipynb)|'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 4053}, page_content='| [Multilingual models](https://github.com/huggingface/notebooks/blob/main/transformers_doc/en/multilingual.ipynb)  | How to use the multilingual models of the library |[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/multilingual.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 4463}, page_content='.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/en/multilingual.ipynb)|'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 4605}, page_content='### PyTorch Examples\\n\\n#### Natural Language Processing[[pytorch-nlp]]'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 4676}, page_content='| Notebook     |      Description      |   |   |\\n|:----------|:-------------|:-------------|------:|'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 4777}, page_content='| [Train your tokenizer](https://github.com/huggingface/notebooks/blob/main/examples/tokenizer_training.ipynb)  | How to train and use your very own tokenizer  |[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/tokenizer_training.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 5174}, page_content='.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/tokenizer_training.ipynb)|'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 5309}, page_content='| [Train your language model](https://github.com/huggingface/notebooks/blob/main/examples/language_modeling_from_scratch.ipynb)   | How to easily start using transformers  |[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling_from_scratch.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 5730}, page_content='.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/language_modeling_from_scratch.ipynb)|'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 5877}, page_content='| [How to fine-tune a model on text classification](https://github.com/huggingface/notebooks/blob/main/examples/text_classification.ipynb)| Show how to preprocess the data and fine-tune a pretrained model on any GLUE task. | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 6329}, page_content='.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/text_classification.ipynb)|'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 6475}, page_content='| [How to fine-tune a model on language modeling](https://github.com/huggingface/notebooks/blob/main/examples/language_modeling.ipynb)| Show how to preprocess the data and fine-tune a pretrained model on a causal or masked LM task. | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 6934}, page_content='.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/language_modeling.ipynb)|'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 7078}, page_content='| [How to fine-tune a model on token classification](https://github.com/huggingface/notebooks/blob/main/examples/token_classification.ipynb)| Show how to preprocess the data and fine-tune a pretrained model on a token classification task (NER, PoS). | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/token_classification.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 7558}, page_content='.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/token_classification.ipynb)|'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 7705}, page_content='| [How to fine-tune a model on question answering](https://github.com/huggingface/notebooks/blob/main/examples/question_answering.ipynb)| Show how to preprocess the data and fine-tune a pretrained model on SQUAD. | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question_answering.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 8156}, page_content='.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/question_answering.ipynb)|'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 8291}, page_content='| [How to fine-tune a model on multiple choice](https://github.com/huggingface/notebooks/blob/main/examples/multiple_choice.ipynb)| Show how to preprocess the data and fine-tune a pretrained model on SWAG. | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multiple_choice.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 8732}, page_content='.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/multiple_choice.ipynb)|'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 8864}, page_content='| [How to fine-tune a model on translation](https://github.com/huggingface/notebooks/blob/main/examples/translation.ipynb)| Show how to preprocess the data and fine-tune a pretrained model on WMT. | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/translation.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 9292}, page_content='.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/translation.ipynb)|'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 9420}, page_content='| [How to fine-tune a model on summarization](https://github.com/huggingface/notebooks/blob/main/examples/summarization.ipynb)| Show how to preprocess the data and fine-tune a pretrained model on XSUM. | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/summarization.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 9855}, page_content='.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/summarization.ipynb)|'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 9985}, page_content='| [How to train a language model from scratch](https://github.com/huggingface/blog/blob/main/notebooks/01_how_to_train.ipynb)| Highlight all the steps to effectively train Transformer model on custom data | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/blog/blob/main/notebooks/01_how_to_train.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 10421}, page_content='.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/blog/blob/main/notebooks/01_how_to_train.ipynb)|'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 10549}, page_content='| [How to generate text](https://github.com/huggingface/blog/blob/main/notebooks/02_how_to_generate.ipynb)| How to use different decoding methods for language generation with transformers | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/blog/blob/main/notebooks/02_how_to_generate.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 10971}, page_content='.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/blog/blob/main/notebooks/02_how_to_generate.ipynb)|'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 11102}, page_content='| [How to generate text (with constraints)](https://github.com/huggingface/blog/blob/main/notebooks/53_constrained_beam_search.ipynb)| How to guide language generation with user-provided constraints | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/blog/blob/main/notebooks/53_constrained_beam_search.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 11543}, page_content='.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/blog/blob/main/notebooks/53_constrained_beam_search.ipynb)|'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 11682}, page_content='| [Reformer](https://github.com/huggingface/blog/blob/main/notebooks/03_reformer.ipynb)| How Reformer pushes the limits of language modeling | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/patrickvonplaten/blog/blob/main/notebooks/03_reformer.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/patrickvonplaten/blog/blob/main/notebooks/03_reformer'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 12175}, page_content='.ipynb)|'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 12185}, page_content='#### Computer Vision[[pytorch-cv]]'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 12221}, page_content='| Notebook                                                                                                                                                                   | Description                                                                                                            |'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 12720}, page_content='|   |'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 12726}, page_content='|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 13175}, page_content='--------------------------------------------------|------:|'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 13235}, page_content='| [How to fine-tune a model on image classification (Torchvision)](https://github.com/huggingface/notebooks/blob/main/examples/image_classification.ipynb)                   | Show how to preprocess the data using Torchvision and fine-tune any pretrained Vision model on Image Classification    | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 13710}, page_content='.ipynb)                 | [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb)|'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 13923}, page_content='| [How to fine-tune a model on image classification (Albumentations)](https://github.com/huggingface/notebooks/blob/main/examples/image_classification_albumentations.ipynb) | Show how to preprocess the data using Albumentations and fine-tune any pretrained Vision model on Image Classification | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification_albumentations'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 14413}, page_content='.ipynb)  | [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/image_classification_albumentations.ipynb)|'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 14626}, page_content='| [How to fine-tune a model on image classification (Kornia)](https://github.com/huggingface/notebooks/blob/main/examples/image_classification_kornia.ipynb)                 | Show how to preprocess the data using Kornia and fine-tune any pretrained Vision model on Image Classification         | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification_kornia'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 15108}, page_content='.ipynb)          | [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/image_classification_kornia.ipynb)|'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 15321}, page_content='| [How to perform zero-shot object detection with OWL-ViT](https://github.com/huggingface/notebooks/blob/main/examples/zeroshot_object_detection_with_owlvit.ipynb)          | Show how to perform zero-shot object detection on images with text queries                                             | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/zeroshot_object_detection_with_owlvit'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 15813}, page_content='.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/zeroshot_object_detection_with_owlvit.ipynb)|'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 16026}, page_content='| [How to fine-tune an image captioning model](https://github.com/huggingface/notebooks/blob/main/examples/image_captioning_blip.ipynb)                                      | Show how to fine-tune BLIP for image captioning on a custom dataset                                                    | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_captioning_blip'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 16502}, page_content='.ipynb)                | [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/image_captioning_blip.ipynb)|'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 16715}, page_content='| [How to build an image similarity system with Transformers](https://github.com/huggingface/notebooks/blob/main/examples/image_similarity.ipynb)                            | Show how to build an image similarity system                                                                           | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_similarity'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 17186}, page_content='.ipynb)                     | [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/image_similarity.ipynb)|'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 17399}, page_content='| [How to fine-tune a SegFormer model on semantic segmentation](https://github.com/huggingface/notebooks/blob/main/examples/semantic_segmentation.ipynb)                     | Show how to preprocess the data and fine-tune a pretrained SegFormer model on Semantic Segmentation                    | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/semantic_segmentation'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 17875}, page_content='.ipynb)                | [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/semantic_segmentation.ipynb)|'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 18088}, page_content='| [How to fine-tune a VideoMAE model on video classification](https://github.com/huggingface/notebooks/blob/main/examples/video_classification.ipynb)          | Show how to preprocess the data and fine-tune a pretrained VideoMAE model on Video Classification                      | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/video_classification'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 18549}, page_content='.ipynb)                | [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/video_classification.ipynb)|'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 18762}, page_content='#### Audio[[pytorch-audio]]'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 18791}, page_content='| Notebook     |      Description      |   |   |\\n|:----------|:-------------|:-------------|------:|'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 18892}, page_content='| [How to fine-tune a speech recognition model in English](https://github.com/huggingface/notebooks/blob/main/examples/speech_recognition.ipynb)| Show how to preprocess the data and fine-tune a pretrained Speech model on TIMIT | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/speech_recognition.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 19347}, page_content='.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/speech_recognition.ipynb)|'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 19492}, page_content='| [How to fine-tune a speech recognition model in any language](https://github.com/huggingface/notebooks/blob/main/examples/multi_lingual_speech_recognition.ipynb)| Show how to preprocess the data and fine-tune a multi-lingually pretrained speech model on Common Voice | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multi_lingual_speech_recognition'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 19954}, page_content='.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/multi_lingual_speech_recognition.ipynb)|'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 20162}, page_content='| [How to fine-tune a model on audio classification](https://github.com/huggingface/notebooks/blob/main/examples/audio_classification.ipynb)| Show how to preprocess the data and fine-tune a pretrained Speech model on Keyword Spotting | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/audio_classification.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 20626}, page_content='.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/audio_classification.ipynb)|'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 20774}, page_content='#### Biological Sequences[[pytorch-bio]]'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 20816}, page_content='| Notebook     | Description                                                                             |   |   |\\n|:----------|:----------------------------------------------------------------------------------------|:-------------|------:|'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 21058}, page_content='| [How to fine-tune a pre-trained protein model](https://github.com/huggingface/notebooks/blob/main/examples/protein_language_modeling.ipynb) | See how to tokenize proteins and fine-tune a large pre-trained protein \"language\" model | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/protein_language_modeling.ipynb) | [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 21526}, page_content='.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/protein_language_modeling.ipynb) |'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 21679}, page_content='| [How to generate protein folds](https://github.com/huggingface/notebooks/blob/main/examples/protein_folding.ipynb) | See how to go from protein sequence to a full protein model and PDB file                | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/protein_folding.ipynb) | [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 22122}, page_content='.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/protein_folding.ipynb) |'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 22255}, page_content='| [How to fine-tune a Nucleotide Transformer model](https://github.com/huggingface/notebooks/blob/main/examples/nucleotide_transformer_dna_sequence_modelling.ipynb) | See how to tokenize DNA and fine-tune a large pre-trained DNA \"language\" model | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/nucleotide_transformer_dna_sequence_modelling'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 22707}, page_content='.ipynb) | [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/nucleotide_transformer_dna_sequence_modelling.ipynb) |'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 22930}, page_content='| [Fine-tune a Nucleotide Transformer model with LoRA](https://github.com/huggingface/notebooks/blob/main/examples/nucleotide_transformer_dna_sequence_modelling_with_peft.ipynb) | Train even larger DNA models in a memory-efficient way | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/nucleotide_transformer_dna_sequence_modelling_with_peft'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 23381}, page_content='.ipynb) | [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/nucleotide_transformer_dna_sequence_modelling_with_peft.ipynb) |'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 23616}, page_content='#### Other modalities[[pytorch-other]]'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 23656}, page_content='| Notebook     | Description                                                                             |   |   |\\n|:----------|:----------------------------------------------------------------------------------------|:-------------|------:|'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 23898}, page_content='| [Probabilistic Time Series Forecasting](https://github.com/huggingface/notebooks/blob/main/examples/time-series-transformers.ipynb) | See how to train Time Series Transformer on a custom dataset                            | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/time-series-transformers.ipynb) | [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 24357}, page_content='.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/time-series-transformers.ipynb) |'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 24510}, page_content='#### Utility notebooks[[pytorch-utility]]'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 24553}, page_content='| Notebook     |      Description      |   |   |\\n|:----------|:-------------|:-------------|------:|\\n| [How to export model to ONNX](https://github.com/huggingface/notebooks/blob/main/examples/onnx-export.ipynb)| Highlight how to export and run inference workloads through ONNX |'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 24833}, page_content='| [How to use Benchmarks](https://github.com/huggingface/notebooks/blob/main/examples/benchmark.ipynb)| How to benchmark models with transformers | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/benchmark.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/benchmark'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 25325}, page_content='.ipynb)|'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 25335}, page_content='### TensorFlow Examples\\n\\n#### Natural Language Processing[[tensorflow-nlp]]'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 25412}, page_content='| Notebook     |      Description      |   |   |\\n|:----------|:-------------|:-------------|------:|'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 25513}, page_content='| [Train your tokenizer](https://github.com/huggingface/notebooks/blob/main/examples/tokenizer_training.ipynb)  | How to train and use your very own tokenizer  |[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/tokenizer_training.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 25910}, page_content='.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/tokenizer_training.ipynb)|'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 26045}, page_content='| [Train your language model](https://github.com/huggingface/notebooks/blob/main/examples/language_modeling_from_scratch-tf.ipynb)   | How to easily start using transformers  |[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling_from_scratch-tf.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 26472}, page_content='.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/language_modeling_from_scratch-tf.ipynb)|'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 26622}, page_content='| [How to fine-tune a model on text classification](https://github.com/huggingface/notebooks/blob/main/examples/text_classification-tf.ipynb)| Show how to preprocess the data and fine-tune a pretrained model on any GLUE task. | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification-tf.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 27080}, page_content='.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/text_classification-tf.ipynb)|'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 27229}, page_content='| [How to fine-tune a model on language modeling](https://github.com/huggingface/notebooks/blob/main/examples/language_modeling-tf.ipynb)| Show how to preprocess the data and fine-tune a pretrained model on a causal or masked LM task. | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling-tf.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 27694}, page_content='.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/language_modeling-tf.ipynb)|'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 27841}, page_content='| [How to fine-tune a model on token classification](https://github.com/huggingface/notebooks/blob/main/examples/token_classification-tf.ipynb)| Show how to preprocess the data and fine-tune a pretrained model on a token classification task (NER, PoS). | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/token_classification-tf.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 28327}, page_content='.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/token_classification-tf.ipynb)|'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 28477}, page_content='| [How to fine-tune a model on question answering](https://github.com/huggingface/notebooks/blob/main/examples/question_answering-tf.ipynb)| Show how to preprocess the data and fine-tune a pretrained model on SQUAD. | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question_answering-tf.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 28924}, page_content='.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/question_answering-tf.ipynb)|'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 29072}, page_content='| [How to fine-tune a model on multiple choice](https://github.com/huggingface/notebooks/blob/main/examples/multiple_choice-tf.ipynb)| Show how to preprocess the data and fine-tune a pretrained model on SWAG. | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multiple_choice-tf.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 29519}, page_content='.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/multiple_choice-tf.ipynb)|'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 29654}, page_content='| [How to fine-tune a model on translation](https://github.com/huggingface/notebooks/blob/main/examples/translation-tf.ipynb)| Show how to preprocess the data and fine-tune a pretrained model on WMT. | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/translation-tf.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 30088}, page_content='.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/translation-tf.ipynb)|'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 30219}, page_content='| [How to fine-tune a model on summarization](https://github.com/huggingface/notebooks/blob/main/examples/summarization-tf.ipynb)| Show how to preprocess the data and fine-tune a pretrained model on XSUM. | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/summarization-tf.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 30660}, page_content='.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/summarization-tf.ipynb)|'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 30794}, page_content='#### Computer Vision[[tensorflow-cv]]'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 30833}, page_content='| Notebook                                                                                                                                                 | Description                                                                                         |   |   |'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 31100}, page_content='|:---------------------------------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------------------|:-------------|------:|'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 31382}, page_content='| [How to fine-tune a model on image classification](https://github.com/huggingface/notebooks/blob/main/examples/image_classification-tf.ipynb)            | Show how to preprocess the data and fine-tune any pretrained Vision model on Image Classification   | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification-tf.ipynb)| [![Open in AWS Studio](https://studiolab'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 31823}, page_content='.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/image_classification-tf.ipynb)|'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 32022}, page_content='| [How to fine-tune a SegFormer model on semantic segmentation](https://github.com/huggingface/notebooks/blob/main/examples/semantic_segmentation-tf.ipynb) | Show how to preprocess the data and fine-tune a pretrained SegFormer model on Semantic Segmentation | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/semantic_segmentation-tf.ipynb)| [![Open in AWS Studio](https://studiolab'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 32465}, page_content='.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/semantic_segmentation-tf.ipynb)|'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 32666}, page_content='#### Biological Sequences[[tensorflow-bio]]'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 32711}, page_content='| Notebook     |      Description      |   |   |\\n|:----------|:-------------|:-------------|------:|'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 32812}, page_content='| [How to fine-tune a pre-trained protein model](https://github.com/huggingface/notebooks/blob/main/examples/protein_language_modeling-tf.ipynb) | See how to tokenize proteins and fine-tune a large pre-trained protein \"language\" model | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/protein_language_modeling-tf.ipynb) | [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 33286}, page_content='.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/protein_language_modeling-tf.ipynb) |'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 33443}, page_content='#### Utility notebooks[[tensorflow-utility]]'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 33489}, page_content='| Notebook     |      Description      |   |                                                                                                                                                                                      |\\n|:----------|:-------------|:-------------|------:|'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 33769}, page_content=\"| [How to train TF/Keras models on TPU](https://github.com/huggingface/notebooks/blob/main/examples/tpu_training-tf.ipynb) | See how to train at high speed on Google's TPU hardware | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/tpu_training-tf.ipynb) | [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker\"),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 34186}, page_content='.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/tpu_training-tf.ipynb) |'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 34320}, page_content='### Optimum notebooks\\n\\n🤗  [Optimum](https://github.com/huggingface/optimum) is an extension of 🤗 Transformers, providing a set of performance optimization tools enabling maximum efficiency to train and run models on targeted hardwares.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 34557}, page_content='| Notebook     |      Description      |   |   |\\n|:----------|:-------------|:-------------|------:|'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 34658}, page_content='| [How to quantize a model with ONNX Runtime for text classification](https://github.com/huggingface/notebooks/blob/main/examples/text_classification_quantization_ort.ipynb)| Show how to apply static and dynamic quantization on a model using [ONNX Runtime](https://github.com/microsoft/onnxruntime) for any GLUE task. | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 35048}, page_content='.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification_quantization_ort.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/text_classification_quantization_ort.ipynb)|'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 35385}, page_content='| [How to quantize a model with Intel Neural Compressor for text classification](https://github.com/huggingface/notebooks/blob/main/examples/text_classification_quantization_inc.ipynb)| Show how to apply static, dynamic and aware training quantization on a model using [Intel Neural Compressor (INC)](https://github.com/intel/neural-compressor) for any GLUE task. | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 35821}, page_content='.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification_quantization_inc.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/text_classification_quantization_inc.ipynb)|'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 36158}, page_content='| [How to fine-tune a model on text classification with ONNX Runtime](https://github.com/huggingface/notebooks/blob/main/examples/text_classification_ort.ipynb)| Show how to preprocess the data and fine-tune a model on any GLUE task using [ONNX Runtime](https://github.com/microsoft/onnxruntime). | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification_ort'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 36639}, page_content='.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/text_classification_ort.ipynb)|'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 36838}, page_content='| [How to fine-tune a model on summarization with ONNX Runtime](https://github.com/huggingface/notebooks/blob/main/examples/summarization_ort.ipynb)| Show how to preprocess the data and fine-tune a model on XSUM using [ONNX Runtime](https://github.com/microsoft/onnxruntime). | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/summarization_ort'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 37292}, page_content='.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/summarization_ort.ipynb)|'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 37486}, page_content='## Community notebooks:\\n\\nMore notebooks developed by the community are available [here](https://hf.co/docs/transformers/community#community-notebooks).'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 0}, page_content='!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 232}, page_content='http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 583}, page_content='⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer.\\n\\n-->\\n\\n# Trainer'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 762}, page_content='The [`Trainer`] class provides an API for feature-complete training in PyTorch, and it supports distributed training on multiple GPUs/TPUs, mixed precision for [NVIDIA GPUs](https://nvidia.github.io/apex/), [AMD GPUs](https://rocm.docs.amd.com/en/latest/rocm.html), and [`torch.amp`](https://pytorch.org/docs/stable/amp.html) for PyTorch. [`Trainer`] goes hand-in-hand with the [`TrainingArguments`] class, which offers a wide range of options to customize how a model is trained'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 1241}, page_content='. Together, these two classes provide a complete training API.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 1305}, page_content=\"[`Seq2SeqTrainer`] and [`Seq2SeqTrainingArguments`] inherit from the [`Trainer`] and [`TrainingArgument`] classes and they're adapted for training models for sequence-to-sequence tasks such as summarization or translation.\\n\\n<Tip warning={true}>\\n\\nThe [`Trainer`] class is optimized for 🤗 Transformers models and can have surprising behaviors\\nwhen used with other models. When using it with your own model, make sure:\"),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 1722}, page_content='- your model always return tuples or subclasses of [`~utils.ModelOutput`]\\n- your model can compute the loss if a `labels` argument is provided and that loss is returned as the first\\n  element of the tuple (if your model returns tuples)\\n- your model can accept multiple label arguments (use `label_names` in [`TrainingArguments`] to indicate their name to the [`Trainer`]) but none of them should be named `\"label\"`\\n\\n</Tip>\\n\\n## Trainer[[api-reference]]\\n\\n[[autodoc]] Trainer\\n    - all'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 2175}, page_content='[[autodoc]] Trainer\\n    - all\\n\\n## Seq2SeqTrainer\\n\\n[[autodoc]] Seq2SeqTrainer\\n    - evaluate\\n    - predict\\n\\n## TrainingArguments\\n\\n[[autodoc]] TrainingArguments\\n    - all\\n\\n## Seq2SeqTrainingArguments\\n\\n[[autodoc]] Seq2SeqTrainingArguments\\n    - all'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 0}, page_content='!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 232}, page_content='http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 583}, page_content='⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer.\\n\\n-->\\n\\n# BigBird\\n\\n## Overview'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 775}, page_content='The BigBird model was proposed in [Big Bird: Transformers for Longer Sequences](https://arxiv.org/abs/2007.14062) by\\nZaheer, Manzil and Guruganesh, Guru and Dubey, Kumar Avinava and Ainslie, Joshua and Alberti, Chris and Ontanon,\\nSantiago and Pham, Philip and Ravula, Anirudh and Wang, Qifan and Yang, Li and others. BigBird, is a sparse-attention\\nbased transformer which extends Transformer based models, such as BERT to much longer sequences. In addition to sparse'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 1242}, page_content='attention, BigBird also applies global attention as well as random attention to the input sequence. Theoretically, it\\nhas been shown that applying sparse, global, and random attention approximates full attention, while being\\ncomputationally much more efficient for longer sequences. As a consequence of the capability to handle longer context,\\nBigBird has shown improved performance on various long document NLP tasks, such as question answering and\\nsummarization, compared to BERT or RoBERTa.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 1737}, page_content='The abstract from the paper is the following:'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 1784}, page_content='*Transformers-based models, such as BERT, have been one of the most successful deep learning models for NLP.\\nUnfortunately, one of their core limitations is the quadratic dependency (mainly in terms of memory) on the sequence\\nlength due to their full attention mechanism. To remedy this, we propose, BigBird, a sparse attention mechanism that\\nreduces this quadratic dependency to linear. We show that BigBird is a universal approximator of sequence functions and'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 2247}, page_content='is Turing complete, thereby preserving these properties of the quadratic, full attention model. Along the way, our\\ntheoretical analysis reveals some of the benefits of having O(1) global tokens (such as CLS), that attend to the entire\\nsequence as part of the sparse attention mechanism. The proposed sparse attention can handle sequences of length up to\\n8x of what was previously possible using similar hardware. As a consequence of the capability to handle longer context,'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 2721}, page_content='BigBird drastically improves performance on various NLP tasks such as question answering and summarization. We also\\npropose novel applications to genomics data.*'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 2884}, page_content='This model was contributed by [vasudevgupta](https://huggingface.co/vasudevgupta). The original code can be found\\n[here](https://github.com/google-research/bigbird).\\n\\n## Usage tips'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 3066}, page_content=\"- For an in-detail explanation on how BigBird's attention works, see [this blog post](https://huggingface.co/blog/big-bird).\\n- BigBird comes with 2 implementations: **original_full** & **block_sparse**. For the sequence length < 1024, using\\n  **original_full** is advised as there is no benefit in using **block_sparse** attention.\\n- The code currently uses window size of 3 blocks and 2 global blocks.\\n- Sequence length must be divisible by block size.\"),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 3520}, page_content=\"- Current implementation supports only **ITC**.\\n- Current implementation doesn't support **num_random_blocks = 0**\\n- BigBird is a model with absolute position embeddings so it's usually advised to pad the inputs on the right rather than\\n  the left.\"),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 3771}, page_content='## Resources\\n\\n- [Text classification task guide](../tasks/sequence_classification)\\n- [Token classification task guide](../tasks/token_classification)\\n- [Question answering task guide](../tasks/question_answering)\\n- [Causal language modeling task guide](../tasks/language_modeling)\\n- [Masked language modeling task guide](../tasks/masked_language_modeling)\\n- [Multiple choice task guide](../tasks/multiple_choice)\\n\\n## BigBirdConfig\\n\\n[[autodoc]] BigBirdConfig\\n\\n## BigBirdTokenizer'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 4203}, page_content='[[autodoc]] BigBirdConfig\\n\\n## BigBirdTokenizer\\n\\n[[autodoc]] BigBirdTokenizer\\n    - build_inputs_with_special_tokens\\n    - get_special_tokens_mask\\n    - create_token_type_ids_from_sequences\\n    - save_vocabulary\\n\\n## BigBirdTokenizerFast\\n\\n[[autodoc]] BigBirdTokenizerFast\\n\\n## BigBird specific outputs\\n\\n[[autodoc]] models.big_bird.modeling_big_bird.BigBirdForPreTrainingOutput\\n\\n<frameworkcontent>\\n<pt>\\n\\n## BigBirdModel\\n\\n[[autodoc]] BigBirdModel\\n    - forward\\n\\n## BigBirdForPreTraining'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 4660}, page_content='## BigBirdForPreTraining\\n\\n[[autodoc]] BigBirdForPreTraining\\n    - forward\\n\\n## BigBirdForCausalLM\\n\\n[[autodoc]] BigBirdForCausalLM\\n    - forward\\n\\n## BigBirdForMaskedLM\\n\\n[[autodoc]] BigBirdForMaskedLM\\n    - forward\\n\\n## BigBirdForSequenceClassification\\n\\n[[autodoc]] BigBirdForSequenceClassification\\n    - forward\\n\\n## BigBirdForMultipleChoice\\n\\n[[autodoc]] BigBirdForMultipleChoice\\n    - forward\\n\\n## BigBirdForTokenClassification\\n\\n[[autodoc]] BigBirdForTokenClassification\\n    - forward'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 5142}, page_content='## BigBirdForQuestionAnswering\\n\\n[[autodoc]] BigBirdForQuestionAnswering\\n    - forward\\n\\n</pt>\\n<jax>\\n\\n## FlaxBigBirdModel\\n\\n[[autodoc]] FlaxBigBirdModel\\n    - __call__\\n\\n## FlaxBigBirdForPreTraining\\n\\n[[autodoc]] FlaxBigBirdForPreTraining\\n    - __call__\\n\\n## FlaxBigBirdForCausalLM\\n\\n[[autodoc]] FlaxBigBirdForCausalLM\\n    - __call__\\n\\n## FlaxBigBirdForMaskedLM\\n\\n[[autodoc]] FlaxBigBirdForMaskedLM\\n    - __call__\\n\\n## FlaxBigBirdForSequenceClassification'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 5548}, page_content='## FlaxBigBirdForSequenceClassification\\n\\n[[autodoc]] FlaxBigBirdForSequenceClassification\\n    - __call__\\n\\n## FlaxBigBirdForMultipleChoice\\n\\n[[autodoc]] FlaxBigBirdForMultipleChoice\\n    - __call__\\n\\n## FlaxBigBirdForTokenClassification\\n\\n[[autodoc]] FlaxBigBirdForTokenClassification\\n    - __call__\\n\\n## FlaxBigBirdForQuestionAnswering\\n\\n[[autodoc]] FlaxBigBirdForQuestionAnswering\\n    - __call__\\n\\n</jax>\\n</frameworkcontent>'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 0}, page_content='!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 232}, page_content='http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 583}, page_content='⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer.\\n\\n-->\\n\\n# Persimmon\\n\\n## Overview\\n\\nThe Persimmon model was created by [ADEPT](https://www.adept.ai/blog/persimmon-8b), and authored by Erich Elsen, Augustus Odena, Maxwell Nye, Sağnak Taşırlar, Tri Dao, Curtis Hawthorne, Deepak Moparthi, Arushi Somani.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 996}, page_content='The authors introduced Persimmon-8B, a decoder model based on the classic transformers architecture, with query and key normalization. Persimmon-8B is a fully permissively-licensed model with approximately 8 billion parameters, released under the Apache license.  Some of the key attributes of Persimmon-8B are long context size (16K), performance, and capabilities for multimodal extensions.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 1390}, page_content=\"The authors showcase their approach to model evaluation, focusing on practical text generation, mirroring how users interact with language models. The work also includes a comparative analysis, pitting Persimmon-8B against other prominent models (MPT 7B Instruct and Llama 2 Base 7B 1-Shot), across various evaluation tasks. The results demonstrate Persimmon-8B's competitive performance, even with limited training data.\"),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 1813}, page_content='In terms of model details, the work outlines the architecture and training methodology of Persimmon-8B, providing insights into its design choices, sequence length, and dataset composition. The authors present a fast inference code that outperforms traditional implementations through operator fusion and CUDA graph utilization while maintaining code coherence'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 2173}, page_content='. They express their anticipation of how the community will leverage this contribution to drive innovation, hinting at further upcoming releases as part of an ongoing series of developments.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 2365}, page_content=\"This model was contributed by [ArthurZ](https://huggingface.co/ArthurZ).\\nThe original code can be found [here](https://github.com/persimmon-ai-labs/adept-inference).\\n\\n## Usage tips\\n\\n<Tip warning={true}>\\n\\nThe `Persimmon` models were trained using `bfloat16`, but the original inference uses `float16` The checkpoints uploaded on the hub use `torch_dtype = 'float16'` which will be\\nused by the `AutoModel` API to cast the checkpoints from `torch.float32` to `torch.float16`.\"),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 2840}, page_content='The `dtype` of the online weights is mostly irrelevant, unless you are using `torch_dtype=\"auto\"` when initializing a model using `model = AutoModelForCausalLM.from_pretrained(\"path\", torch_dtype = \"auto\")`. The reason is that the model will first be downloaded ( using the `dtype` of the checkpoints online) then it will be cast to the default `dtype` of `torch` (becomes `torch.float32`). Users should specify the `torch_dtype` they want, and if they don\\'t it will be `torch.float32`.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 3328}, page_content='Finetuning the model in `float16` is not recommended and known to produce `nan`, as such the model should be fine-tuned in `bfloat16`.\\n\\n</Tip>\\n\\n\\nTips:\\n\\n- To convert the model, you need to clone the original repository using `git clone https://github.com/persimmon-ai-labs/adept-inference`, then get the checkpoints:'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 3645}, page_content='```bash\\ngit clone https://github.com/persimmon-ai-labs/adept-inference\\nwget https://axtkn4xl5cip.objectstorage.us-phoenix-1.oci.customer-oci.com/n/axtkn4xl5cip/b/adept-public-data/o/8b_base_model_release.tar\\ntar -xvf 8b_base_model_release.tar\\npython src/transformers/models/persimmon/convert_persimmon_weights_to_hf.py  --input_dir /path/to/downloaded/persimmon/weights/ --output_dir /output/path \\\\\\n    --pt_model_path /path/to/8b_chat_model_release/iter_0001251/mp_rank_00/model_optim_rng.pt'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 4142}, page_content='--ada_lib_path /path/to/adept-inference\\n```'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 4187}, page_content='For the chat model:\\n```bash\\nwget https://axtkn4xl5cip.objectstorage.us-phoenix-1.oci.customer-oci.com/n/axtkn4xl5cip/b/adept-public-data/o/8b_chat_model_release.tar\\ntar -xvf 8b_base_model_release.tar\\n```\\n\\nThereafter, models can be loaded via:\\n\\n```py\\nfrom transformers import PersimmonForCausalLM, PersimmonTokenizer\\n\\nmodel = PersimmonForCausalLM.from_pretrained(\"/output/path\")\\ntokenizer = PersimmonTokenizer.from_pretrained(\"/output/path\")\\n```'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 4634}, page_content='- Perismmon uses a `sentencepiece` based tokenizer, with a `Unigram` model. It supports bytefallback, which is only available in `tokenizers==0.14.0` for the fast tokenizer.\\nThe `LlamaTokenizer` is used as it is a standard wrapper around sentencepiece. The `chat` template will be updated with the templating functions in a follow up PR!\\n\\n- The authors suggest to use the following prompt format for the chat mode: `f\"human: {prompt}\\\\n\\\\nadept:\"`\\n\\n\\n## PersimmonConfig\\n\\n[[autodoc]] PersimmonConfig'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 5082}, page_content='## PersimmonConfig\\n\\n[[autodoc]] PersimmonConfig\\n\\n## PersimmonModel\\n\\n[[autodoc]] PersimmonModel\\n    - forward\\n\\n## PersimmonForCausalLM\\n\\n[[autodoc]] PersimmonForCausalLM\\n    - forward\\n\\n## PersimmonForSequenceClassification\\n\\n[[autodoc]] PersimmonForSequenceClassification\\n    - forward'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 0}, page_content='*TEMPLATE**\\n=====================================\\n\\n*search & replace the following keywords, e.g.:*\\n`:%s/\\\\[name of model\\\\]/brand_new_bert/g`\\n\\n-[lowercase name of model]  # e.g. brand_new_bert\\n\\n-[camelcase name of model]  # e.g. BrandNewBert\\n\\n-[name of mentor]  # e.g. [Peter](https://github.com/peter)\\n\\n-[link to original repo]\\n\\n-[start date]\\n\\n-[end date]\\n\\n\\n\\nHow to add [camelcase name of model] to 🤗 Transformers?\\n=====================================\\n\\nMentor: [name of mentor]\\n\\nBegin: [start date]'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 454}, page_content=\"Mentor: [name of mentor]\\n\\nBegin: [start date]\\n\\nEstimated End: [end date]\\n\\nAdding a new model is often difficult and requires an in-depth knowledge\\nof the 🤗 Transformers library and ideally also of the model's original\\nrepository. At Hugging Face, we are trying to empower the community more\\nand more to add models independently.\"),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 785}, page_content='The following sections explain in detail how to add [camelcase name of model] \\nto Transformers. You will work closely with [name of mentor] to\\nintegrate [camelcase name of model] into Transformers. By doing so, you will both gain a \\ntheoretical and deep practical understanding of [camelcase name of model]. \\nBut more importantly, you will have made a major\\nopen-source contribution to Transformers. Along the way, you will:'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 1211}, page_content=\"-   get insights into open-source best practices\\n-   understand the design principles of one of the most popular NLP\\n    libraries\\n-   learn how to do efficiently test large NLP models\\n-   learn how to integrate Python utilities like `black`, `ruff`,\\n    `make fix-copies` into a library to always ensure clean and readable\\n    code\\n\\nTo start, let's try to get a general overview of the Transformers\\nlibrary.\\n\\nGeneral overview of 🤗 Transformers\\n----------------------------------\"),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 1692}, page_content=\"First, you should get a general overview of 🤗 Transformers. Transformers \\nis a very opinionated library, so there is a chance that\\nyou don't agree with some of the library's philosophies or design\\nchoices. From our experience, however, we found that the fundamental\\ndesign choices and philosophies of the library are crucial to\\nefficiently scale Transformers while keeping maintenance costs at a\\nreasonable level.\"),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 2107}, page_content='A good first starting point to better understand the library is to read\\nthe [documentation of our philosophy](https://huggingface.co/transformers/philosophy.html).\\nAs a result of our way of working, there are some choices that we try to apply to all models:'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 2366}, page_content='-   Composition is generally favored over abstraction\\n-   Duplicating code is not always bad if it strongly improves the\\n    readability or accessibility of a model\\n-   Model files are as self-contained as possible so that when you read\\n    the code of a specific model, you ideally only have to look into the\\n    respective `modeling_....py` file.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 2716}, page_content=\"In our opinion, the library's code is not just a means to provide a\\nproduct, *e.g.*, the ability to use BERT for inference, but also as the\\nvery product that we want to improve. Hence, when adding a model, the\\nuser is not only the person that will use your model, but also everybody\\nthat will read, try to understand, and possibly tweak your code.\\n\\nWith this in mind, let's go a bit deeper into the general library\\ndesign.\\n\\n### Overview of models\"),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 3140}, page_content=\"### Overview of models\\n\\nTo successfully add a model, it is important to understand the\\ninteraction between your model and its config,\\n`PreTrainedModel`, and `PretrainedConfig`. For\\nexemplary purposes, we will call the PyTorch model to be added to 🤗 Transformers\\n`BrandNewBert`.\\n\\nLet's take a look:\\n\\n![image](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers_overview.png)\"),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 3554}, page_content=\"As you can see, we do make use of inheritance in 🤗 Transformers, but we\\nkeep the level of abstraction to an absolute minimum. There are never\\nmore than two levels of abstraction for any model in the library.\\n`BrandNewBertModel` inherits from\\n`BrandNewBertPreTrainedModel` which in\\nturn inherits from `PreTrainedModel` and that's it. \\nAs a general rule, we want to make sure\\nthat a new model only depends on `PreTrainedModel`. The\"),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 3984}, page_content='important functionalities that are automatically provided to every new\\nmodel are\\n`PreTrainedModel.from_pretrained` and `PreTrainedModel.save_pretrained`, which are \\nused for serialization and deserialization. All\\nof the other important functionalities, such as\\n`BrandNewBertModel.forward` should be\\ncompletely defined in the new `modeling_brand_new_bert.py` module. Next,\\nwe want to make sure that a model with a specific head layer, such as\\n`BrandNewBertForMaskedLM` does not inherit'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 4426}, page_content='`BrandNewBertForMaskedLM` does not inherit\\nfrom `BrandNewBertModel`, but rather uses\\n`BrandNewBertModel` as a component that\\ncan be called in its forward pass to keep the level of abstraction low.\\nEvery new model requires a configuration class, called\\n`BrandNewBertConfig`. This configuration\\nis always stored as an attribute in\\n`PreTrainedModel`, and\\nthus can be accessed via the `config` attribute for all classes\\ninheriting from `BrandNewBertPreTrainedModel`'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 4889}, page_content='```python\\n# assuming that `brand_new_bert` belongs to the organization `brandy`\\nmodel = BrandNewBertModel.from_pretrained(\"brandy/brand_new_bert\")\\nmodel.config  # model has access to its config\\n```'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 5088}, page_content='Similar to the model, the configuration inherits basic serialization and\\ndeserialization functionalities from\\n`PretrainedConfig`. Note\\nthat the configuration and the model are always serialized into two\\ndifferent formats - the model to a `pytorch_model.bin` file\\nand the configuration to a `config.json` file. Calling\\n`PreTrainedModel.save_pretrained` will automatically call\\n`PretrainedConfig.save_pretrained`, so that both model and configuration are saved.\\n\\n### Overview of tokenizers'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 5549}, page_content='### Overview of tokenizers\\n\\nNot quite ready yet :-( This section will be added soon!\\n\\nStep-by-step recipe to add a model to 🤗 Transformers\\n----------------------------------------------------\\n\\nEveryone has different preferences of how to port a model so it can be\\nvery helpful for you to take a look at summaries of how other\\ncontributors ported models to Hugging Face. Here is a list of community\\nblog posts on how to port a model:'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 5983}, page_content='1.  [Porting GPT2\\n    Model](https://medium.com/huggingface/from-tensorflow-to-pytorch-265f40ef2a28)\\n    by [Thomas](https://huggingface.co/thomwolf)\\n2.  [Porting WMT19 MT Model](https://huggingface.co/blog/porting-fsmt)\\n    by [Stas](https://huggingface.co/stas)\\n\\nFrom experience, we can tell you that the most important things to keep\\nin mind when adding a model are:'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 6354}, page_content=\"-   Don't reinvent the wheel! Most parts of the code you will add for\\n    the new 🤗 Transformers model already exist somewhere in 🤗\\n    Transformers. Take some time to find similar, already existing\\n    models and tokenizers you can copy from.\\n    [grep](https://www.gnu.org/software/grep/) and\\n    [rg](https://github.com/BurntSushi/ripgrep) are your friends. Note\\n    that it might very well happen that your model's tokenizer is based\"),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 6796}, page_content=\"on one model implementation, and your model's modeling code on\\n    another one. *E.g.*, FSMT's modeling code is based on BART, while\\n    FSMT's tokenizer code is based on XLM.\\n-   It's more of an engineering challenge than a scientific challenge.\\n    You should spend more time on creating an efficient debugging\\n    environment than trying to understand all theoretical aspects of the\\n    model in the paper.\\n-   Ask for help when you're stuck! Models are the core component of 🤗\"),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 7281}, page_content=\"Transformers so we, at Hugging Face, are more than happy to help\\n    you at every step to add your model. Don't hesitate to ask if you\\n    notice you are not making progress.\"),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 7457}, page_content='In the following, we try to give you a general recipe that we found most\\nuseful when porting a model to 🤗 Transformers.\\n\\nThe following list is a summary of everything that has to be done to add\\na model and can be used by you as a To-Do List:\\n\\n1.  [ ] (Optional) Understood theoretical aspects\\n\\n2.  [ ] Prepared transformers dev environment\\n\\n3.  [ ] Set up debugging environment of the original repository'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 7863}, page_content='4.  [ ] Created script that successfully runs forward pass using\\n    original repository and checkpoint\\n\\n5.  [ ] Successfully opened a PR and added the model skeleton to Transformers\\n\\n6.  [ ] Successfully converted original checkpoint to Transformers\\n    checkpoint\\n\\n7.  [ ] Successfully ran forward pass in Transformers that gives\\n    identical output to original checkpoint\\n\\n8.  [ ] Finished model tests in Transformers\\n\\n9.  [ ] Successfully added Tokenizer in Transformers'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 8340}, page_content='10. [ ] Run end-to-end integration tests\\n\\n11. [ ] Finished docs\\n\\n12. [ ] Uploaded model weights to the hub\\n\\n13. [ ] Submitted the pull request for review\\n\\n14. [ ] (Optional) Added a demo notebook'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 8537}, page_content=\"To begin with, we usually recommend to start by getting a good\\ntheoretical understanding of `[camelcase name of model]`. However, if you prefer to\\nunderstand the theoretical aspects of the model *on-the-job*, then it is\\ntotally fine to directly dive into the `[camelcase name of model]`'s code-base. This\\noption might suit you better, if your engineering skills are better than\\nyour theoretical skill, if you have trouble understanding\"),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 8973}, page_content=\"`[camelcase name of model]`'s paper, or if you just enjoy programming much more than\\nreading scientific papers.\"),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 9086}, page_content='### 1. (Optional) Theoretical aspects of [camelcase name of model]'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 9154}, page_content=\"You should take some time to read *[camelcase name of model]'s* paper, if such\\ndescriptive work exists. There might be large sections of the paper that\\nare difficult to understand. If this is the case, this is fine - don't\\nworry! The goal is not to get a deep theoretical understanding of the\\npaper, but to extract the necessary information required to effectively\\nre-implement the model in 🤗 Transformers. That being said, you don't\"),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 9588}, page_content='have to spend too much time on the theoretical aspects, but rather focus\\non the practical ones, namely:'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 9693}, page_content=\"-   What type of model is *[camelcase name of model]*? BERT-like encoder-only\\n    model? GPT2-like decoder-only model? BART-like encoder-decoder\\n    model? Look at the `model_summary` if\\n    you're not familiar with the differences between those.\\n-   What are the applications of *[camelcase name of model]*? Text\\n    classification? Text generation? Seq2Seq tasks, *e.g.,*\\n    summarization?\\n-   What is the novel feature of the model making it different from\\n    BERT/GPT-2/BART?\"),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 10158}, page_content='BERT/GPT-2/BART?\\n-   Which of the already existing [🤗 Transformers\\n    models](https://huggingface.co/transformers/#contents) is most\\n    similar to *[camelcase name of model]*?\\n-   What type of tokenizer is used? A sentencepiece tokenizer? Word\\n    piece tokenizer? Is it the same tokenizer as used for BERT or BART?'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 10477}, page_content=\"After you feel like you have gotten a good overview of the architecture\\nof the model, you might want to write to [name of mentor] with any\\nquestions you might have. This might include questions regarding the\\nmodel's architecture, its attention layer, etc. We will be more than\\nhappy to help you.\\n\\n\\n#### Additional resources\\n\\n Before diving into the code, here are some additional resources that might be worth taking a look at:\\n \\n - [link 1]\\n - [link 2]\\n - [link 3]\\n - ...\"),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 10951}, page_content=\"#### Make sure you've understood the fundamental aspects of [camelcase name of model]\\n\\nAlright, now you should be ready to take a closer look into the actual code of [camelcase name of model].\\nYou should have understood the following aspects of [camelcase name of model] by now:\\n\\n- [characteristic 1 of [camelcase name of model]]\\n- [characteristic 2 of [camelcase name of model]]\\n- ...\"),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 11338}, page_content=\"If any of the mentioned aspects above are **not** clear to you, now is a great time to talk to [name of mentor].\\n\\n### 2. Next prepare your environment\\n\\n1.  Fork the [repository](https://github.com/huggingface/transformers)\\n    by clicking on the 'Fork' button on the repository's page. This\\n    creates a copy of the code under your GitHub user account.\\n\\n2.  Clone your `transformers` fork to your local disk, and add the base\\n    repository as a remote:\"),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 11798}, page_content='```bash\\n    git clone https://github.com/[your Github handle]/transformers.git\\n    cd transformers\\n    git remote add upstream https://github.com/huggingface/transformers.git\\n    ```\\n\\n3.  Set up a development environment, for instance by running the\\n    following command:\\n\\n    ```bash\\n    python -m venv .env\\n    source .env/bin/activate\\n    pip install -e \".[dev]\"\\n    ```\\n\\nand return to the parent directory\\n\\n```bash\\ncd ..\\n```'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 12210}, page_content=\"```bash\\ncd ..\\n```\\n\\n4.  We recommend adding the PyTorch version of *[camelcase name of model]* to\\n    Transformers. To install PyTorch, please follow the instructions [here](https://pytorch.org/get-started/locally/).\\n\\n**Note:** You don't need to have CUDA installed. Making the new model\\nwork on CPU is sufficient.\\n\\n5.  To port *[camelcase name of model]*, you will also need access to its\\n    original repository:\"),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 12625}, page_content='```bash\\ngit clone [link to original repo].git \\ncd [lowercase name of model]\\npip install -e .\\n```\\n\\nNow you have set up a development environment to port *[camelcase name of model]*\\nto 🤗 Transformers.\\n\\n### Run a pretrained checkpoint using the original repository\\n\\n**3. Set up debugging environment**'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 12925}, page_content='At first, you will work on the original *[camelcase name of model]* repository.\\nOften, the original implementation is very \"researchy\". Meaning that\\ndocumentation might be lacking and the code can be difficult to\\nunderstand. But this should be exactly your motivation to reimplement\\n*[camelcase name of model]*. At Hugging Face, one of our main goals is to *make\\npeople stand on the shoulders of giants* which translates here very well'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 13361}, page_content='into taking a working model and rewriting it to make it as **accessible,\\nuser-friendly, and beautiful** as possible. This is the number-one\\nmotivation to re-implement models into 🤗 Transformers - trying to make\\ncomplex new NLP technology accessible to **everybody**.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 13629}, page_content='You should start thereby by diving into the [original repository]([link to original repo]).\\n\\nSuccessfully running the official pretrained model in the original\\nrepository is often **the most difficult** step. From our experience, it\\nis very important to spend some time getting familiar with the original\\ncode-base. You need to figure out the following:'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 13984}, page_content=\"-   Where to find the pretrained weights?\\n-   How to load the pretrained weights into the corresponding model?\\n-   How to run the tokenizer independently from the model?\\n-   Trace one forward pass so that you know which classes and functions\\n    are required for a simple forward pass. Usually, you only have to\\n    reimplement those functions.\\n-   Be able to locate the important components of the model: Where is\\n    the model's class? Are there model sub-classes, *e.g.*,\"),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 14463}, page_content='EncoderModel, DecoderModel? Where is the self-attention layer? Are\\n    there multiple different attention layers, *e.g.*, *self-attention*,\\n    *cross-attention*...?\\n-   How can you debug the model in the original environment of the repo?\\n    Do you have to add `print` statements, can you work with\\n    an interactive debugger like [ipdb](https://pypi.org/project/ipdb/), or should you use\\n    an efficient IDE to debug the model, like PyCharm?'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 14910}, page_content='It is very important that before you start the porting process, that you\\ncan **efficiently** debug code in the original repository! Also,\\nremember that you are working with an open-source library, so do not\\nhesitate to open an issue, or even a pull request in the original\\nrepository. The maintainers of this repository are most likely very\\nhappy about someone looking into their code!'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 15297}, page_content='At this point, it is really up to you which debugging environment and\\nstrategy you prefer to use to debug the original model. We strongly\\nadvise against setting up a costly GPU environment, but simply work on a\\nCPU both when starting to dive into the original repository and also\\nwhen starting to write the 🤗 Transformers implementation of the model.\\nOnly at the very end, when the model has already been successfully\\nported to 🤗 Transformers, one should verify that the model also works as'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 15788}, page_content='expected on GPU.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 15806}, page_content='In general, there are two possible debugging environments for running\\nthe original model\\n\\n-   [Jupyter notebooks](https://jupyter.org/) / [google colab](https://colab.research.google.com/notebooks/intro.ipynb)\\n-   Local python scripts.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 16043}, page_content='Jupyter notebooks have the advantage that they allow for cell-by-cell\\nexecution which can be helpful to better split logical components from\\none another and to have faster debugging cycles as intermediate results\\ncan be stored. Also, notebooks are often easier to share with other\\ncontributors, which might be very helpful if you want to ask the Hugging\\nFace team for help. If you are familiar with Jupyter notebooks, we\\nstrongly recommend you to work with them.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 16507}, page_content='The obvious disadvantage of Jupyter notebooks is that if you are not\\nused to working with them you will have to spend some time adjusting to\\nthe new programming environment and that you might not be able to use\\nyour known debugging tools anymore, like `ipdb`.\\n\\n**4. Successfully run forward pass**'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 16768}, page_content='**4. Successfully run forward pass**\\n\\nFor each code-base, a good first step is always to load a **small**\\npretrained checkpoint and to be able to reproduce a single forward pass\\nusing a dummy integer vector of input IDs as an input. Such a script\\ncould look like this (in pseudocode):\\n\\n```python\\nmodel = [camelcase name of model]Model.load_pretrained_checkpoint(\"/path/to/checkpoint/\")\\ninput_ids = [0, 4, 5, 2, 3, 7, 9]  # vector of input ids\\noriginal_output = model.predict(input_ids)\\n```'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 17259}, page_content='Next, regarding the debugging strategy, there are generally a few from\\nwhich to choose from:\\n\\n-   Decompose the original model into many small testable components and\\n    run a forward pass on each of those for verification\\n-   Decompose the original model only into the original *tokenizer* and\\n    the original *model*, run a forward pass on those, and use\\n    intermediate print statements or breakpoints for verification'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 17685}, page_content='Again, it is up to you which strategy to choose. Often, one or the other\\nis advantageous depending on the original code base.\\n\\nIf the original code-base allows you to decompose the model into smaller\\nsub-components, *e.g.*, if the original code-base can easily be run in\\neager mode, it is usually worth the effort to do so. There are some\\nimportant advantages to taking the more difficult road in the beginning:'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 18098}, page_content='-   at a later stage when comparing the original model to the Hugging\\n    Face implementation, you can verify automatically for each component\\n    individually that the corresponding component of the 🤗 Transformers\\n    implementation matches instead of relying on visual comparison via\\n    print statements\\n-   it can give you some rope to decompose the big problem of porting a\\n    model into smaller problems of just porting individual components\\n    and thus structure your work better'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 18551}, page_content=\"and thus structure your work better\\n-   separating the model into logical meaningful components will help\\n    you to get a better overview of the model's design and thus to\\n    better understand the model\\n-   at a later stage those component-by-component tests help you to\\n    ensure that no regression occurs as you continue changing your code\"),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 18897}, page_content=\"[Lysandre's](https://gist.github.com/LysandreJik/db4c948f6b4483960de5cbac598ad4ed)\\nintegration checks for ELECTRA gives a nice example of how this can be\\ndone.\"),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 19058}, page_content=\"However, if the original code-base is very complex or only allows\\nintermediate components to be run in a compiled mode, it might be too\\ntime-consuming or even impossible to separate the model into smaller\\ntestable sub-components. A good example is [T5's\\nMeshTensorFlow](https://github.com/tensorflow/mesh/tree/master/mesh_tensorflow)\\nlibrary which is very complex and does not offer a simple way to\\ndecompose the model into its sub-components. For such libraries, one\"),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 19526}, page_content='often relies on verifying print statements.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 19571}, page_content='No matter which strategy you choose, the recommended procedure is often\\nthe same in that you should start to debug the starting layers first and\\nthe ending layers last.\\n\\nIt is recommended that you retrieve the output, either by print\\nstatements or sub-component functions, of the following layers in the\\nfollowing order:'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 19893}, page_content='1.  Retrieve the input IDs passed to the model\\n2.  Retrieve the word embeddings\\n3.  Retrieve the input of the first Transformer layer\\n4.  Retrieve the output of the first Transformer layer\\n5.  Retrieve the output of the following n - 1 Transformer layers\\n6.  Retrieve the output of the whole [camelcase name of model] Model\\n\\nInput IDs should thereby consists of an array of integers, *e.g.*,\\n`input_ids = [0, 4, 4, 3, 2, 4, 1, 7, 19]`'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 20329}, page_content='The outputs of the following layers often consist of multi-dimensional\\nfloat arrays and can look like this:'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 20438}, page_content='```bash\\n[[\\n [-0.1465, -0.6501,  0.1993,  ...,  0.1451,  0.3430,  0.6024],\\n [-0.4417, -0.5920,  0.3450,  ..., -0.3062,  0.6182,  0.7132],\\n [-0.5009, -0.7122,  0.4548,  ..., -0.3662,  0.6091,  0.7648],\\n ...,\\n [-0.5613, -0.6332,  0.4324,  ..., -0.3792,  0.7372,  0.9288],\\n [-0.5416, -0.6345,  0.4180,  ..., -0.3564,  0.6992,  0.9191],\\n [-0.5334, -0.6403,  0.4271,  ..., -0.3339,  0.6533,  0.8694]]],\\n```'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 20840}, page_content='We expect that every model added to 🤗 Transformers passes a couple of\\nintegration tests, meaning that the original model and the reimplemented\\nversion in 🤗 Transformers have to give the exact same output up to a\\nprecision of 0.001! Since it is normal that the exact same model written\\nin different libraries can give a slightly different output depending on\\nthe library framework, we accept an error tolerance of 1e-3 (0.001). It'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 21270}, page_content='is not enough if the model gives nearly the same output, they have to be\\nthe almost identical. Therefore, you will certainly compare the\\nintermediate outputs of the 🤗 Transformers version multiple times\\nagainst the intermediate outputs of the original implementation of\\n*[camelcase name of model]* in which case an **efficient** debugging environment\\nof the original repository is absolutely important. Here is some advice\\nto make your debugging environment as efficient as possible.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 21755}, page_content='-   Find the best way of debugging intermediate results. Is the original\\n    repository written in PyTorch? Then you should probably take the\\n    time to write a longer script that decomposes the original model\\n    into smaller sub-components to retrieve intermediate values. Is the\\n    original repository written in Tensorflow 1? Then you might have to\\n    rely on TensorFlow print operations like\\n    [tf.print](https://www.tensorflow.org/api_docs/python/tf/print) to'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 22230}, page_content='output intermediate values. Is the original repository written in\\n    Jax? Then make sure that the model is **not jitted** when running\\n    the forward pass, *e.g.*, check-out [this\\n    link](https://github.com/google/jax/issues/196).\\n-   Use the smallest pretrained checkpoint you can find. The smaller the\\n    checkpoint, the faster your debug cycle becomes. It is not efficient\\n    if your pretrained model is so big that your forward pass takes more'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 22688}, page_content='than 10 seconds. In case only very large checkpoints are available,\\n    it might make more sense to create a dummy model in the new\\n    environment with randomly initialized weights and save those weights\\n    for comparison with the 🤗 Transformers version of your model\\n-   Make sure you are using the easiest way of calling a forward pass in\\n    the original repository. Ideally, you want to find the function in\\n    the original repository that **only** calls a single forward pass,'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 23177}, page_content=\"*i.e.* that is often called `predict`, `evaluate`, `forward` or\\n    `__call__`. You don't want to debug a function that calls `forward`\\n    multiple times, *e.g.*, to generate text, like\\n    `autoregressive_sample`, `generate`.\\n-   Try to separate the tokenization from the model's\\n    forward pass. If the original repository shows\\n    examples where you have to input a string, then try to find out\\n    where in the forward call the string input is changed to input ids\"),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 23653}, page_content='and start from this point. This might mean that you have to possibly\\n    write a small script yourself or change the original code so that\\n    you can directly input the ids instead of an input string.\\n-   Make sure that the model in your debugging setup is **not** in\\n    training mode, which often causes the model to yield random outputs\\n    due to multiple dropout layers in the model. Make sure that the\\n    forward pass in your debugging environment is **deterministic** so'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 24137}, page_content='that the dropout layers are not used. Or use\\n    `transformers.utils.set_seed` if the old and new\\n    implementations are in the same framework.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 24283}, page_content=\"#### More details on how to create a debugging environment for [camelcase name of model] \\n\\n[TODO FILL: Here the mentor should add very specific information on what the student should do]\\n[to set up an efficient environment for the special requirements of this model]\\n\\n### Port [camelcase name of model] to 🤗 Transformers\\n\\nNext, you can finally start adding new code to 🤗 Transformers. Go into\\nthe clone of your 🤗 Transformers' fork:\\n\\n    cd transformers\"),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 24721}, page_content=\"cd transformers\\n\\nIn the special case that you are adding a model whose architecture\\nexactly matches the model architecture of an existing model you only\\nhave to add a conversion script as described in [this\\nsection](#write-a-conversion-script). In this case, you can just re-use\\nthe whole model architecture of the already existing model.\\n\\nOtherwise, let's start generating a new model with the amazing\\nCookiecutter!\\n\\n**Use the Cookiecutter to automatically generate the model's code**\"),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 25208}, page_content='To begin with head over to the [🤗 Transformers\\ntemplates](https://github.com/huggingface/transformers/tree/main/templates/adding_a_new_model)\\nto make use of our `cookiecutter` implementation to automatically\\ngenerate all the relevant files for your model. Again, we recommend only\\nadding the PyTorch version of the model at first. Make sure you follow\\nthe instructions of the `README.md` on the [🤗 Transformers'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 25619}, page_content='templates](https://github.com/huggingface/transformers/tree/main/templates/adding_a_new_model)\\ncarefully.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 25726}, page_content='**Open a Pull Request on the main huggingface/transformers repo**\\n\\nBefore starting to adapt the automatically generated code, now is the\\ntime to open a \"Work in progress (WIP)\" pull request, *e.g.*, \"\\\\[WIP\\\\]\\nAdd *[camelcase name of model]*\", in 🤗 Transformers so that you and the Hugging\\nFace team can work side-by-side on integrating the model into 🤗\\nTransformers.\\n\\nYou should do the following:\\n\\n1.  Create a branch with a descriptive name from your main branch'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 26190}, page_content='```\\n    git checkout -b add_[lowercase name of model]\\n```\\n\\n2.  Commit the automatically generated code:\\n\\n```\\n    git add .\\n    git commit\\n```\\n\\n3.  Fetch and rebase to current main\\n\\n```\\n    git fetch upstream\\n    git rebase upstream/main\\n```\\n\\n4.  Push the changes to your account using:\\n\\n```\\n    git push -u origin a-descriptive-name-for-my-changes\\n```'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 26543}, page_content='5.  Once you are satisfied, go to the webpage of your fork on GitHub.\\n    Click on \"Pull request\". Make sure to add the GitHub handle of \\n\\t\\t[name of mentor] as a reviewer, so that the Hugging\\n    Face team gets notified for future changes.\\n\\n6.  Change the PR into a draft by clicking on \"Convert to draft\" on the\\n    right of the GitHub pull request web page.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 26904}, page_content=\"In the following, whenever you have done some progress, don't forget to\\ncommit your work and push it to your account so that it shows in the\\npull request. Additionally, you should make sure to update your work\\nwith the current main from time to time by doing:\\n\\n    git fetch upstream\\n    git merge upstream/main\"),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 27217}, page_content='In general, all questions you might have regarding the model or your\\nimplementation should be asked in your PR and discussed/solved in the\\nPR. This way, [name of mentor] will always be notified when you are\\ncommitting new code or if you have a question. It is often very helpful\\nto point [name of mentor] to your added code so that the Hugging\\nFace team can efficiently understand your problem or question.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 27625}, page_content='To do so, you can go to the \"Files changed\" tab where you see all of\\nyour changes, go to a line regarding which you want to ask a question,\\nand click on the \"+\" symbol to add a comment. Whenever a question or\\nproblem has been solved, you can click on the \"Resolve\" button of the\\ncreated comment.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 27922}, page_content='In the same way, [name of mentor] will open comments when reviewing\\nyour code. We recommend asking most questions on GitHub on your PR. For\\nsome very general questions that are not very useful for the public,\\nfeel free to ping [name of mentor] by Slack or email.\\n\\n**5. Adapt the generated models code for [camelcase name of model]**'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 28256}, page_content='At first, we will focus only on the model itself and not care about the\\ntokenizer. All the relevant code should be found in the generated files\\n`src/transformers/models/[lowercase name of model]/modeling_[lowercase name of model].py` and\\n`src/transformers/models/[lowercase name of model]/configuration_[lowercase name of model].py`.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 28591}, page_content='Now you can finally start coding :). The generated code in\\n`src/transformers/models/[lowercase name of model]/modeling_[lowercase name of model].py` will\\neither have the same architecture as BERT if it\\'s an encoder-only model\\nor BART if it\\'s an encoder-decoder model. At this point, you should\\nremind yourself what you\\'ve learned in the beginning about the\\ntheoretical aspects of the model: *How is the model different from BERT\\nor BART?*\\\\\". Implement those changes which often means to change the'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 29089}, page_content='*self-attention* layer, the order of the normalization layer, etc...\\nAgain, it is often useful to look at the similar architecture of already\\nexisting models in Transformers to get a better feeling of how your\\nmodel should be implemented.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 29329}, page_content=\"**Note** that at this point, you don't have to be very sure that your\\ncode is fully correct or clean. Rather, it is advised to add a first\\n*unclean*, copy-pasted version of the original code to\\n`src/transformers/models/[lowercase name of model]/modeling_[lowercase name of model].py`\\nuntil you feel like all the necessary code is added. From our\\nexperience, it is much more efficient to quickly add a first version of\\nthe required code and improve/correct the code iteratively with the\"),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 29815}, page_content='conversion script as described in the next section. The only thing that\\nhas to work at this point is that you can instantiate the 🤗 Transformers\\nimplementation of *[camelcase name of model]*, *i.e.* the following command\\nshould work:'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 30050}, page_content='```python\\nfrom transformers import [camelcase name of model]Model, [camelcase name of model]Config\\nmodel = [camelcase name of model]Model([camelcase name of model]Config())\\n```\\n\\nThe above command will create a model according to the default\\nparameters as defined in `[camelcase name of model]Config()` with random weights,\\nthus making sure that the `init()` methods of all components works.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 30442}, page_content='[TODO FILL: Here the mentor should add very specific information on what exactly has to be changed for this model]\\n[...]\\n[...]\\n\\n**6. Write a conversion script**'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 30604}, page_content='Next, you should write a conversion script that lets you convert the\\ncheckpoint you used to debug *[camelcase name of model]* in the original\\nrepository to a checkpoint compatible with your just created 🤗\\nTransformers implementation of *[camelcase name of model]*. It is not advised to\\nwrite the conversion script from scratch, but rather to look through\\nalready existing conversion scripts in 🤗 Transformers for one that has\\nbeen used to convert a similar model that was written in the same'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 31096}, page_content=\"framework as *[camelcase name of model]*. Usually, it is enough to copy an\\nalready existing conversion script and slightly adapt it for your use\\ncase. Don't hesitate to ask [name of mentor] to point you to a\\nsimilar already existing conversion script for your model.\"),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 31364}, page_content=\"-   If you are porting a model from TensorFlow to PyTorch, a good\\n    starting point might be BERT's conversion script\\n    [here](https://github.com/huggingface/transformers/blob/7acfa95afb8194f8f9c1f4d2c6028224dbed35a2/src/transformers/models/bert/modeling_bert.py#L91)\\n-   If you are porting a model from PyTorch to PyTorch, a good starting\\n    point might be BART's conversion script\"),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 31711}, page_content=\"point might be BART's conversion script\\n    [here](https://github.com/huggingface/transformers/blob/main/src/transformers/models/bart/convert_bart_original_pytorch_checkpoint_to_pytorch.py)\"),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 31902}, page_content=\"In the following, we'll quickly explain how PyTorch models store layer\\nweights and define layer names. In PyTorch, the name of a layer is\\ndefined by the name of the class attribute you give the layer. Let's\\ndefine a dummy model in PyTorch, called `SimpleModel` as follows:\\n\\n```python\\nfrom torch import nn\"),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 32176}, page_content='```python\\nfrom torch import nn\\n\\nclass SimpleModel(nn.Module):\\n    def __init__(self):\\n            super().__init__()\\n            self.dense = nn.Linear(10, 10)\\n            self.intermediate = nn.Linear(10, 10)\\n            self.layer_norm = nn.LayerNorm(10)\\n```\\n\\nNow we can create an instance of this model definition which will fill\\nall weights: `dense`, `intermediate`, `layer_norm` with random weights.\\nWe can print the model to see its architecture\\n\\n```python\\nmodel = SimpleModel()'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 32629}, page_content='```python\\nmodel = SimpleModel()\\n\\nprint(model)\\n```\\n\\nThis will print out the following:\\n\\n```bash\\nSimpleModel(\\n  (dense): Linear(in_features=10, out_features=10, bias=True)\\n  (intermediate): Linear(in_features=10, out_features=10, bias=True)\\n  (layer_norm): LayerNorm((10,), eps=1e-05, elementwise_affine=True)\\n)\\n```\\n\\nWe can see that the layer names are defined by the name of the class\\nattribute in PyTorch. You can print out the weight values of a specific\\nlayer:'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 33093}, page_content='```python\\nprint(model.dense.weight.data)\\n```\\n\\nto see that the weights were randomly initialized'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 33190}, page_content='```bash\\ntensor([[-0.0818,  0.2207, -0.0749, -0.0030,  0.0045, -0.1569, -0.1598,  0.0212,\\n         -0.2077,  0.2157],\\n        [ 0.1044,  0.0201,  0.0990,  0.2482,  0.3116,  0.2509,  0.2866, -0.2190,\\n          0.2166, -0.0212],\\n        [-0.2000,  0.1107, -0.1999, -0.3119,  0.1559,  0.0993,  0.1776, -0.1950,\\n         -0.1023, -0.0447],\\n        [-0.0888, -0.1092,  0.2281,  0.0336,  0.1817, -0.0115,  0.2096,  0.1415,\\n         -0.1876, -0.2467],'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 33615}, page_content='-0.1876, -0.2467],\\n        [ 0.2208, -0.2352, -0.1426, -0.2636, -0.2889, -0.2061, -0.2849, -0.0465,\\n          0.2577,  0.0402],\\n        [ 0.1502,  0.2465,  0.2566,  0.0693,  0.2352, -0.0530,  0.1859, -0.0604,\\n          0.2132,  0.1680],\\n        [ 0.1733, -0.2407, -0.1721,  0.1484,  0.0358, -0.0633, -0.0721, -0.0090,\\n          0.2707, -0.2509],\\n        [-0.1173,  0.1561,  0.2945,  0.0595, -0.1996,  0.2988, -0.0802,  0.0407,\\n          0.1829, -0.1568],'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 34052}, page_content='0.1829, -0.1568],\\n        [-0.1164, -0.2228, -0.0403,  0.0428,  0.1339,  0.0047,  0.1967,  0.2923,\\n          0.0333, -0.0536],\\n        [-0.1492, -0.1616,  0.1057,  0.1950, -0.2807, -0.2710, -0.1586,  0.0739,\\n          0.2220,  0.2358]]).\\n```'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 34295}, page_content='In the conversion script, you should fill those randomly initialized\\nweights with the exact weights of the corresponding layer in the\\ncheckpoint. *E.g.*,\\n\\n```python\\n# retrieve matching layer weights, e.g. by \\n# recursive algorithm\\nlayer_name = \"dense\"\\npretrained_weight = array_of_dense_layer\\n\\nmodel_pointer = getattr(model, \"dense\")\\n\\nmodel_pointer.weight.data = torch.from_numpy(pretrained_weight)\\n```'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 34699}, page_content='While doing so, you must verify that each randomly initialized weight of\\nyour PyTorch model and its corresponding pretrained checkpoint weight\\nexactly match in both **shape and name**. To do so, it is **necessary**\\nto add assert statements for the shape and print out the names of the\\ncheckpoints weights. *E.g.*, you should add statements like:'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 35046}, page_content='```python\\nassert (\\n     model_pointer.weight.shape == pretrained_weight.shape\\n), f\"Pointer shape of random weight {model_pointer.shape} and array shape of checkpoint weight {pretrained_weight.shape} mismatched\"\\n```\\n\\nBesides, you should also print out the names of both weights to make\\nsure they match, *e.g.*,\\n\\n```python\\nlogger.info(f\"Initialize PyTorch weight {layer_name} from {pretrained_weight.name}\")\\n```'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 35457}, page_content=\"If either the shape or the name doesn't match, you probably assigned\\nthe wrong checkpoint weight to a randomly initialized layer of the 🤗\\nTransformers implementation.\\n\\nAn incorrect shape is most likely due to an incorrect setting of the\\nconfig parameters in `[camelcase name of model]Config()` that do not exactly match\\nthose that were used for the checkpoint you want to convert. However, it\\ncould also be that PyTorch's implementation of a layer requires the\\nweight to be transposed beforehand.\"),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 35955}, page_content='Finally, you should also check that **all** required weights are\\ninitialized and print out all checkpoint weights that were not used for\\ninitialization to make sure the model is correctly converted. It is\\ncompletely normal, that the conversion trials fail with either a wrong\\nshape statement or wrong name assignment. This is most likely because\\neither you used incorrect parameters in `[camelcase name of model]Config()`, have a'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 36385}, page_content='wrong architecture in the 🤗 Transformers implementation, you have a bug\\nin the `init()` functions of one of the components of the 🤗 Transformers\\nimplementation or you need to transpose one of the checkpoint weights.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 36602}, page_content='This step should be iterated with the previous step until all weights of\\nthe checkpoint are correctly loaded in the Transformers model. Having\\ncorrectly loaded the checkpoint into the 🤗 Transformers implementation,\\nyou can then save the model under a folder of your choice\\n`/path/to/converted/checkpoint/folder` that should then contain both a\\n`pytorch_model.bin` file and a `config.json` file:\\n\\n```python\\nmodel.save_pretrained(\"/path/to/converted/checkpoint/folder\")\\n```'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 37075}, page_content='[TODO FILL: Here the mentor should add very specific information on what exactly has to be done for the conversion of this model]\\n[...]\\n[...]\\n\\n**7. Implement the forward pass**'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 37253}, page_content='Having managed to correctly load the pretrained weights into the 🤗\\nTransformers implementation, you should now make sure that the forward\\npass is correctly implemented. In [Get familiar with the original\\nrepository](#34-run-a-pretrained-checkpoint-using-the-original-repository),\\nyou have already created a script that runs a forward pass of the model\\nusing the original repository. Now you should write an analogous script\\nusing the 🤗 Transformers implementation instead of the original one. It'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 37749}, page_content='should look as follows:'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 37774}, page_content='[TODO FILL: Here the model name might have to be adapted, *e.g.*, maybe [camelcase name of model]ForConditionalGeneration instead of [camelcase name of model]Model]\\n\\n```python\\nmodel = [camelcase name of model]Model.from_pretrained(\"/path/to/converted/checkpoint/folder\")\\ninput_ids = [0, 4, 4, 3, 2, 4, 1, 7, 19]\\noutput = model(input_ids).last_hidden_states\\n```'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 38136}, page_content='It is very likely that the 🤗 Transformers implementation and the\\noriginal model implementation don\\'t give the exact same output the very\\nfirst time or that the forward pass throws an error. Don\\'t be\\ndisappointed - it\\'s expected! First, you should make sure that the\\nforward pass doesn\\'t throw any errors. It often happens that the wrong\\ndimensions are used leading to a `\"Dimensionality mismatch\"`\\nerror or that the wrong data type object is used, *e.g.*, `torch.long`'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 38605}, page_content=\"instead of `torch.float32`. Don't hesitate to ask [name of mentor]\\nfor help, if you don't manage to solve certain errors.\"),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 38728}, page_content='The final part to make sure the 🤗 Transformers implementation works\\ncorrectly is to ensure that the outputs are equivalent to a precision of\\n`1e-3`. First, you should ensure that the output shapes are identical,\\n*i.e.* `outputs.shape` should yield the same value for the script of the\\n🤗 Transformers implementation and the original implementation. Next, you\\nshould make sure that the output values are identical as well. This one'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 39158}, page_content='of the most difficult parts of adding a new model. Common mistakes why\\nthe outputs are not identical are:'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 39265}, page_content=\"-   Some layers were not added, *i.e.* an activation layer\\n    was not added, or the residual connection was forgotten\\n-   The word embedding matrix was not tied\\n-   The wrong positional embeddings are used because the original\\n    implementation uses on offset\\n-   Dropout is applied during the forward pass. To fix this make sure\\n    `model.training is False` and that no dropout layer is\\n    falsely activated during the forward pass, *i.e.* pass\\n    `self.training` to [PyTorch's functional\"),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 39719}, page_content=\"`self.training` to [PyTorch's functional\\n    dropout](https://pytorch.org/docs/stable/nn.functional.html?highlight=dropout#torch.nn.functional.dropout)\"),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 39872}, page_content='The best way to fix the problem is usually to look at the forward pass\\nof the original implementation and the 🤗 Transformers implementation\\nside-by-side and check if there are any differences. Ideally, you should\\ndebug/print out intermediate outputs of both implementations of the\\nforward pass to find the exact position in the network where the 🤗\\nTransformers implementation shows a different output than the original\\nimplementation. First, make sure that the hard-coded `input_ids` in both'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 40364}, page_content='scripts are identical. Next, verify that the outputs of the first\\ntransformation of the `input_ids` (usually the word embeddings) are\\nidentical. And then work your way up to the very last layer of the\\nnetwork. At some point, you will notice a difference between the two\\nimplementations, which should point you to the bug in the 🤗 Transformers\\nimplementation. From our experience, a simple and efficient way is to\\nadd many print statements in both the original implementation and 🤗'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 40845}, page_content='Transformers implementation, at the same positions in the network\\nrespectively, and to successively remove print statements showing the\\nsame values for intermediate presentions.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 41024}, page_content=\"When you're confident that both implementations yield the same output,\\nverifying the outputs with\\n`torch.allclose(original_output, output, atol=1e-3)`, you're done with\\nthe most difficult part! Congratulations - the work left to be done\\nshould be a cakewalk 😊.\\n\\n**8. Adding all necessary model tests**\"),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 41327}, page_content='At this point, you have successfully added a new model. However, it is\\nvery much possible that the model does not yet fully comply with the\\nrequired design. To make sure, the implementation is fully compatible\\nwith 🤗 Transformers, all common tests should pass. The Cookiecutter\\nshould have automatically added a test file for your model, probably\\nunder the same `tests/test_modeling_[lowercase name of model].py`. Run this test\\nfile to verify that all common tests pass:'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 41799}, page_content='```python\\npytest tests/test_modeling_[lowercase name of model].py\\n```\\n\\n[TODO FILL: Here the mentor should add very specific information on what tests are likely to fail after having implemented the model\\n, e.g. given the model, it might be very likely that `test_attention_output` fails]\\n[...]\\n[...]\\n\\nHaving fixed all common tests, it is now crucial to ensure that all the\\nnice work you have done is well tested, so that'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 42221}, page_content='-   a)  The community can easily understand your work by looking at\\n        specific tests of *[camelcase name of model]*\\n\\n-   b)  Future changes to your model will not break any important\\n        feature of the model.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 42441}, page_content='At first, integration tests should be added. Those integration tests\\nessentially do the same as the debugging scripts you used earlier to\\nimplement the model to 🤗 Transformers. A template of those model tests\\nis already added by the Cookiecutter, called\\n`[camelcase name of model]ModelIntegrationTests` and only has to be filled out by\\nyou. To ensure that those tests are passing, run'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 42827}, page_content='```python\\nRUN_SLOW=1 pytest -sv tests/test_modeling_[lowercase name of model].py::[camelcase name of model]ModelIntegrationTests\\n```\\n\\n**Note:** In case you are using Windows, you should replace `RUN_SLOW=1` with `SET RUN_SLOW=1`\\n\\nSecond, all features that are special to *[camelcase name of model]* should be\\ntested additionally in a separate test under\\n`[camelcase name of model]ModelTester`/`[camelcase name of model]ModelTest`. This part is often\\nforgotten but is extremely useful in two ways:'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 43325}, page_content='-   It helps to transfer the knowledge you have acquired during the\\n    model addition to the community by showing how the special features\\n    of *[camelcase name of model]* should work.\\n-   Future contributors can quickly test changes to the model by running\\n    those special tests.\\n\\n[TODO FILL: Here the mentor should add very specific information on what special features of the model should be tested additionally]\\n[...]\\n[...]\\n\\n**9. Implement the tokenizer**'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 43759}, page_content=\"**9. Implement the tokenizer**\\n\\nNext, we should add the tokenizer of *[camelcase name of model]*. Usually, the\\ntokenizer is equivalent or very similar to an already existing tokenizer\\nof 🤗 Transformers.\\n\\n[TODO FILL: Here the mentor should add a comment whether a new tokenizer is required or if this is not the case which existing tokenizer closest resembles \\n [camelcase name of model]'s tokenizer and how the tokenizer should be implemented]\\n [...]\\n [...]\"),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 44218}, page_content=\"It is very important to find/extract the original tokenizer file and to\\nmanage to load this file into the 🤗 Transformers' implementation of the\\ntokenizer.\\n\\nFor [camelcase name of model], the tokenizer files can be found here:\\n- [To be filled out by mentor]\\n\\nand having implemented the 🤗 Transformers' version of the tokenizer can be loaded as follows:\\n\\n[To be filled out by mentor]\"),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 44571}, page_content='[To be filled out by mentor]\\n\\nTo ensure that the tokenizer works correctly, it is recommended to first\\ncreate a script in the original repository that inputs a string and\\nreturns the `input_ids`. It could look similar to this (in pseudo-code):'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 44816}, page_content='```bash\\ninput_str = \"This is a long example input string containing special characters .$?-, numbers 2872 234 12 and words.\"\\nmodel = [camelcase name of model]Model.load_pretrained_checkpoint(\"/path/to/checkpoint/\")\\ninput_ids = model.tokenize(input_str)\\n```'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 45074}, page_content='You might have to take a deeper look again into the original repository\\nto find the correct tokenizer function or you might even have to do\\nchanges to your clone of the original repository to only output the\\n`input_ids`. Having written a functional tokenization script that uses\\nthe original repository, an analogous script for 🤗 Transformers should\\nbe created. It should look similar to this:'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 45469}, page_content='```python\\nfrom transformers import [camelcase name of model]Tokenizer\\ninput_str = \"This is a long example input string containing special characters .$?-, numbers 2872 234 12 and words.\"\\n\\ntokenizer = [camelcase name of model]Tokenizer.from_pretrained(\"/path/to/tokenizer/folder/\")\\n\\ninput_ids = tokenizer(input_str).input_ids\\n```\\n\\nWhen both `input_ids` yield the same values, as a final step a tokenizer\\ntest file should also be added.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 45905}, page_content='[TODO FILL: Here mentor should point the student to test files of similar tokenizers]\\n\\nAnalogous to the modeling test files of *[camelcase name of model]*, the\\ntokenization test files of *[camelcase name of model]* should contain a couple of\\nhard-coded integration tests.\\n\\n[TODO FILL: Here mentor should again point to an existing similar test of another model that the student can copy & adapt]\\n\\n**10. Run End-to-end integration tests**'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 46344}, page_content='Having added the tokenizer, you should also add a couple of end-to-end\\nintegration tests using both the model and the tokenizer to\\n`tests/test_modeling_[lowercase name of model].py` in 🤗 Transformers. Such a test\\nshould show on a meaningful text-to-text sample that the 🤗 Transformers\\nimplementation works as expected. A meaningful text-to-text sample can\\ninclude *e.g.* a source-to-target-translation pair, an\\narticle-to-summary pair, a question-to-answer pair, etc... If none of'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 46825}, page_content='the ported checkpoints has been fine-tuned on a downstream task it is\\nenough to simply rely on the model tests. In a final step to ensure that\\nthe model is fully functional, it is advised that you also run all tests\\non GPU. It can happen that you forgot to add some `.to(self.device)`\\nstatements to internal tensors of the model, which in such a test would\\nshow in an error. In case you have no access to a GPU, the Hugging Face\\nteam can take care of running those tests for you.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 47306}, page_content='**11. Add Docstring**'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 47329}, page_content=\"Now, all the necessary functionality for *[camelcase name of model]* is added -\\nyou're almost done! The only thing left to add is a nice docstring and\\na doc page. The Cookiecutter should have added a template file called\\n`docs/source/model_doc/[lowercase name of model].rst` that you should fill out.\\nUsers of your model will usually first look at this page before using\\nyour model. Hence, the documentation must be understandable and concise.\"),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 47773}, page_content=\"It is very useful for the community to add some *Tips* to show how the\\nmodel should be used. Don't hesitate to ping [name of mentor]\\nregarding the docstrings.\"),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 47933}, page_content='Next, make sure that the docstring added to\\n`src/transformers/models/[lowercase name of model]/modeling_[lowercase name of model].py` is\\ncorrect and included all necessary inputs and outputs. It is always to\\ngood to remind oneself that documentation should be treated at least as\\ncarefully as the code in 🤗 Transformers since the documentation is\\nusually the first contact point of the community with the model.\\n\\n**Code refactor**'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 48346}, page_content='**Code refactor**\\n\\nGreat, now you have added all the necessary code for *[camelcase name of model]*.\\nAt this point, you should correct some potential incorrect code style by\\nrunning:\\n\\n```bash\\nmake style\\n```\\n\\nand verify that your coding style passes the quality check:\\n\\n```bash\\nmake quality\\n```'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 48615}, page_content=\"```bash\\nmake quality\\n```\\n\\nThere are a couple of other very strict design tests in 🤗 Transformers\\nthat might still be failing, which shows up in the tests of your pull\\nrequest. This is often because of some missing information in the\\ndocstring or some incorrect naming. [name of mentor] will surely\\nhelp you if you're stuck here.\"),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 48945}, page_content=\"Lastly, it is always a good idea to refactor one's code after having\\nensured that the code works correctly. With all tests passing, now it's\\na good time to go over the added code again and do some refactoring.\\n\\nYou have now finished the coding part, congratulation! 🎉 You are\\nAwesome! 😎\\n\\n**12. Upload the models to the model hub**\"),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 49233}, page_content=\"**12. Upload the models to the model hub**\\n\\nIn this final part, you should convert and upload all checkpoints to the\\nmodel hub and add a model card for each uploaded model checkpoint. You\\nshould work alongside [name of mentor] here to decide on a fitting\\nname for each checkpoint and to get the required access rights to be\\nable to upload the model under the author's organization of\\n*[camelcase name of model]*.\"),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 49647}, page_content='It is worth spending some time to create fitting model cards for each\\ncheckpoint. The model cards should highlight the specific\\ncharacteristics of this particular checkpoint, *e.g.*, On which dataset\\nwas the checkpoint pretrained/fine-tuned on? On what down-stream task\\nshould the model be used? And also include some code on how to correctly\\nuse the model.\\n\\n**13. (Optional) Add notebook**'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 50006}, page_content='**13. (Optional) Add notebook**\\n\\nIt is very helpful to add a notebook that showcases in-detail how\\n*[camelcase name of model]* can be used for inference and/or fine-tuned on a\\ndownstream task. This is not mandatory to merge your PR, but very useful\\nfor the community.\\n\\n**14. Submit your finished PR**'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 50275}, page_content=\"**14. Submit your finished PR**\\n\\nYou're done programming now and can move to the last step, which is\\ngetting your PR merged into main. Usually, [name of mentor]\\nshould have helped you already at this point, but it is worth taking\\nsome time to give your finished PR a nice description and eventually add\\ncomments to your code, if you want to point out certain design choices\\nto your reviewer.\\n\\n### Share your work!!\"),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 50668}, page_content=\"### Share your work!!\\n\\nNow, it's time to get some credit from the community for your work!\\nHaving completed a model addition is a major contribution to\\nTransformers and the whole NLP community. Your code and the ported\\npre-trained models will certainly be used by hundreds and possibly even\\nthousands of developers and researchers. You should be proud of your\\nwork and share your achievement with the community.\"),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 51081}, page_content='**You have made another model that is super easy to access for everyone\\nin the community! 🤯**'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 0}, page_content='!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\");\\nyou may not use this file except in compliance with the License.\\nYou may obtain a copy of the License at\\n\\n    http://www.apache.org/licenses/LICENSE-2.0'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 238}, page_content='http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \"AS IS\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\nSee the License for the specific language governing permissions and\\nlimitations under the License.\\n-->\\n\\n## Whole Word Mask Language Model'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 593}, page_content='## Whole Word Mask Language Model\\n\\n\\nThese scripts leverage the 🤗 Datasets library and the Trainer API. You can easily customize them to your needs if you\\nneed extra processing on your datasets.\\n\\nThe following examples, will run on a datasets hosted on our [hub](https://huggingface.co/datasets) or with your own\\ntext files for training and validation. We give examples of both below.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 980}, page_content='The BERT authors released a new version of BERT using Whole Word Masking in May 2019. Instead of masking randomly\\nselected tokens (which may be part of words), they mask randomly selected words (masking all the tokens corresponding\\nto that word). This technique has been refined for Chinese in [this paper](https://arxiv.org/abs/1906.08101).'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 1323}, page_content=\"To fine-tune a model using whole word masking, use the following script:\\n```bash\\npython run_mlm_wwm.py \\\\\\n    --model_name_or_path roberta-base \\\\\\n    --dataset_name wikitext \\\\\\n    --dataset_config_name wikitext-2-raw-v1 \\\\\\n    --do_train \\\\\\n    --do_eval \\\\\\n    --output_dir /tmp/test-mlm-wwm\\n```\\n\\nFor Chinese models, we need to generate a reference files (which requires the ltp library), because it's tokenized at\\nthe character level.\\n\\n**Q :** Why a reference file?\"),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 1757}, page_content=\"**Q :** Why a reference file?\\n\\n**A :** Suppose we have a Chinese sentence like: `我喜欢你` The original Chinese-BERT will tokenize it as\\n`['我','喜','欢','你']` (character level). But `喜欢` is a whole word. For whole word masking proxy, we need a result\\nlike `['我','喜','##欢','你']`, so we need a reference file to tell the model which position of the BERT original token\\nshould be added `##`.\\n\\n**Q :** Why LTP ?\"),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 2141}, page_content='**Q :** Why LTP ?\\n\\n**A :** Cause the best known Chinese WWM BERT is [Chinese-BERT-wwm](https://github.com/ymcui/Chinese-BERT-wwm) by HIT.\\nIt works well on so many Chines Task like CLUE (Chinese GLUE). They use LTP, so if we want to fine-tune their model,\\nwe need LTP.\\n\\nYou could run the following:\\n\\n\\n```bash\\nexport TRAIN_FILE=/path/to/train/file\\nexport LTP_RESOURCE=/path/to/ltp/tokenizer\\nexport BERT_RESOURCE=/path/to/bert/tokenizer\\nexport SAVE_PATH=/path/to/data/ref.txt'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 2615}, page_content='python run_chinese_ref.py \\\\\\n    --file_name=$TRAIN_FILE \\\\\\n    --ltp=$LTP_RESOURCE \\\\\\n    --bert=$BERT_RESOURCE \\\\\\n    --save_path=$SAVE_PATH\\n```\\n\\nThen you can run the script like this: \\n\\n\\n```bash\\nexport TRAIN_FILE=/path/to/train/file\\nexport VALIDATION_FILE=/path/to/validation/file\\nexport TRAIN_REF_FILE=/path/to/train/chinese_ref/file\\nexport VALIDATION_REF_FILE=/path/to/validation/chinese_ref/file\\nexport OUTPUT_DIR=/tmp/test-mlm-wwm'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 3050}, page_content='python run_mlm_wwm.py \\\\\\n    --model_name_or_path roberta-base \\\\\\n    --train_file $TRAIN_FILE \\\\\\n    --validation_file $VALIDATION_FILE \\\\\\n    --train_ref_file $TRAIN_REF_FILE \\\\\\n    --validation_ref_file $VALIDATION_REF_FILE \\\\\\n    --do_train \\\\\\n    --do_eval \\\\\\n    --output_dir $OUTPUT_DIR\\n```\\n\\n**Note1:** On TPU, you should the flag `--pad_to_max_length` to make sure all your batches have the same length.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 3455}, page_content=\"**Note2:** And if you have any questions or something goes wrong when runing this code, don't hesitate to pin @wlhgtc.\"),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 0}, page_content='!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 232}, page_content='http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 583}, page_content='⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer.\\n\\n-->\\n\\n# MarianMT'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 746}, page_content='-->\\n\\n# MarianMT\\n\\n<div class=\"flex flex-wrap space-x-1\">\\n<a href=\"https://huggingface.co/models?filter=marian\">\\n<img alt=\"Models\" src=\"https://img.shields.io/badge/All_model_pages-marian-blueviolet\">\\n</a>\\n<a href=\"https://huggingface.co/spaces/docs-demos/opus-mt-zh-en\">\\n<img alt=\"Spaces\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue\">\\n</a>\\n</div>\\n\\n## Overview'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 1125}, page_content='## Overview\\n\\nA framework for translation models, using the same models as BART. Translations should be similar, but not identical to output in the test set linked to in each model card.\\nThis model was contributed by [sshleifer](https://huggingface.co/sshleifer).\\n\\n\\n## Implementation Notes'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 1415}, page_content='- Each model is about 298 MB on disk, there are more than 1,000 models.\\n- The list of supported language pairs can be found [here](https://huggingface.co/Helsinki-NLP).\\n- Models were originally trained by [Jörg Tiedemann](https://researchportal.helsinki.fi/en/persons/j%C3%B6rg-tiedemann) using the [Marian](https://marian-nmt.github.io/) C++ library, which supports fast training and translation.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 1813}, page_content=\"- All models are transformer encoder-decoders with 6 layers in each component. Each model's performance is documented\\n  in a model card.\\n- The 80 opus models that require BPE preprocessing are not supported.\\n- The modeling code is the same as [`BartForConditionalGeneration`] with a few minor modifications:\"),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 2124}, page_content='- static (sinusoid) positional embeddings (`MarianConfig.static_position_embeddings=True`)\\n  - no layernorm_embedding (`MarianConfig.normalize_embedding=False`)\\n  - the model starts generating with `pad_token_id` (which has 0 as a token_embedding) as the prefix (Bart uses\\n    `<s/>`),\\n- Code to bulk convert models can be found in `convert_marian_to_pytorch.py`.\\n\\n\\n## Naming'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 2501}, page_content='- All model names use the following format: `Helsinki-NLP/opus-mt-{src}-{tgt}`\\n- The language codes used to name models are inconsistent. Two digit codes can usually be found [here](https://developers.google.com/admin-sdk/directory/v1/languages), three digit codes require googling \"language\\n  code {code}\".\\n- Codes formatted like `es_AR` are usually `code_{region}`. That one is Spanish from Argentina.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 2905}, page_content='- The models were converted in two stages. The first 1000 models use ISO-639-2 codes to identify languages, the second\\n  group use a combination of ISO-639-5 codes and ISO-639-2 codes.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 3092}, page_content='## Examples\\n\\n- Since Marian models are smaller than many other translation models available in the library, they can be useful for\\n  fine-tuning experiments and integration tests.\\n- [Fine-tune on GPU](https://github.com/huggingface/transformers/blob/master/examples/legacy/seq2seq/train_distil_marian_enro.sh)\\n\\n## Multilingual Models'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 3427}, page_content=\"- All model names use the following format: `Helsinki-NLP/opus-mt-{src}-{tgt}`:\\n- If a model can output multiple languages, and you should specify a language code by prepending the desired output\\n  language to the `src_text`.\\n- You can see a models's supported language codes in its model card, under target constituents, like in [opus-mt-en-roa](https://huggingface.co/Helsinki-NLP/opus-mt-en-roa).\"),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 3827}, page_content='- Note that if a model is only multilingual on the source side, like `Helsinki-NLP/opus-mt-roa-en`, no language\\n  codes are required.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 3962}, page_content='New multi-lingual models from the [Tatoeba-Challenge repo](https://github.com/Helsinki-NLP/Tatoeba-Challenge)\\nrequire 3 character language codes:\\n\\n```python\\n>>> from transformers import MarianMTModel, MarianTokenizer\\n\\n>>> src_text = [\\n...     \">>fra<< this is a sentence in english that we want to translate to french\",\\n...     \">>por<< This should go to portuguese\",\\n...     \">>esp<< And this to Spanish\",\\n... ]'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 4376}, page_content='>>> model_name = \"Helsinki-NLP/opus-mt-en-roa\"\\n>>> tokenizer = MarianTokenizer.from_pretrained(model_name)\\n>>> print(tokenizer.supported_language_codes)\\n[\\'>>zlm_Latn<<\\', \\'>>mfe<<\\', \\'>>hat<<\\', \\'>>pap<<\\', \\'>>ast<<\\', \\'>>cat<<\\', \\'>>ind<<\\', \\'>>glg<<\\', \\'>>wln<<\\', \\'>>spa<<\\', \\'>>fra<<\\', \\'>>ron<<\\', \\'>>por<<\\', \\'>>ita<<\\', \\'>>oci<<\\', \\'>>arg<<\\', \\'>>min<<\\']'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 4723}, page_content='>>> model = MarianMTModel.from_pretrained(model_name)\\n>>> translated = model.generate(**tokenizer(src_text, return_tensors=\"pt\", padding=True))\\n>>> [tokenizer.decode(t, skip_special_tokens=True) for t in translated]\\n[\"c\\'est une phrase en anglais que nous voulons traduire en français\",\\n \\'Isto deve ir para o português.\\',\\n \\'Y esto al español\\']\\n```\\n\\nHere is the code to see all available pretrained models on the hub:\\n\\n```python\\nfrom huggingface_hub import list_models'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 5191}, page_content='model_list = list_models()\\norg = \"Helsinki-NLP\"\\nmodel_ids = [x.modelId for x in model_list if x.modelId.startswith(org)]\\nsuffix = [x.split(\"/\")[1] for x in model_ids]\\nold_style_multi_models = [f\"{org}/{s}\" for s in suffix if s != s.lower()]\\n```\\n\\n## Old Style Multi-Lingual Models\\n\\nThese are the old style multi-lingual models ported from the OPUS-MT-Train repo: and the members of each language\\ngroup:'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 5594}, page_content=\"```python no-style\\n['Helsinki-NLP/opus-mt-NORTH_EU-NORTH_EU',\\n 'Helsinki-NLP/opus-mt-ROMANCE-en',\\n 'Helsinki-NLP/opus-mt-SCANDINAVIA-SCANDINAVIA',\\n 'Helsinki-NLP/opus-mt-de-ZH',\\n 'Helsinki-NLP/opus-mt-en-CELTIC',\\n 'Helsinki-NLP/opus-mt-en-ROMANCE',\\n 'Helsinki-NLP/opus-mt-es-NORWAY',\\n 'Helsinki-NLP/opus-mt-fi-NORWAY',\\n 'Helsinki-NLP/opus-mt-fi-ZH',\\n 'Helsinki-NLP/opus-mt-fi_nb_no_nn_ru_sv_en-SAMI',\\n 'Helsinki-NLP/opus-mt-sv-NORWAY',\\n 'Helsinki-NLP/opus-mt-sv-ZH']\\nGROUP_MEMBERS = {\"),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 6031}, page_content=\"'Helsinki-NLP/opus-mt-sv-ZH']\\nGROUP_MEMBERS = {\\n 'ZH': ['cmn', 'cn', 'yue', 'ze_zh', 'zh_cn', 'zh_CN', 'zh_HK', 'zh_tw', 'zh_TW', 'zh_yue', 'zhs', 'zht', 'zh'],\"),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 6193}, page_content=\"'ROMANCE': ['fr', 'fr_BE', 'fr_CA', 'fr_FR', 'wa', 'frp', 'oc', 'ca', 'rm', 'lld', 'fur', 'lij', 'lmo', 'es', 'es_AR', 'es_CL', 'es_CO', 'es_CR', 'es_DO', 'es_EC', 'es_ES', 'es_GT', 'es_HN', 'es_MX', 'es_NI', 'es_PA', 'es_PE', 'es_PR', 'es_SV', 'es_UY', 'es_VE', 'pt', 'pt_br', 'pt_BR', 'pt_PT', 'gl', 'lad', 'an', 'mwl', 'it', 'it_IT', 'co', 'nap', 'scn', 'vec', 'sc', 'ro', 'la'],\\n 'NORTH_EU': ['de', 'nl', 'fy', 'af', 'da', 'fo', 'is', 'no', 'nb', 'nn', 'sv'],\"),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 6658}, page_content=\"'SCANDINAVIA': ['da', 'fo', 'is', 'no', 'nb', 'nn', 'sv'],\\n 'SAMI': ['se', 'sma', 'smj', 'smn', 'sms'],\\n 'NORWAY': ['nb_NO', 'nb', 'nn_NO', 'nn', 'nog', 'no_nb', 'no'],\\n 'CELTIC': ['ga', 'cy', 'br', 'gd', 'kw', 'gv']\\n}\\n```\"),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 6882}, page_content='Example of translating english to many romance languages, using old-style 2 character language codes\\n\\n\\n```python\\n>>> from transformers import MarianMTModel, MarianTokenizer\\n\\n>>> src_text = [\\n...     \">>fr<< this is a sentence in english that we want to translate to french\",\\n...     \">>pt<< This should go to portuguese\",\\n...     \">>es<< And this to Spanish\",\\n... ]\\n\\n>>> model_name = \"Helsinki-NLP/opus-mt-en-ROMANCE\"\\n>>> tokenizer = MarianTokenizer.from_pretrained(model_name)'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 7361}, page_content='>>> model = MarianMTModel.from_pretrained(model_name)\\n>>> translated = model.generate(**tokenizer(src_text, return_tensors=\"pt\", padding=True))\\n>>> tgt_text = [tokenizer.decode(t, skip_special_tokens=True) for t in translated]\\n[\"c\\'est une phrase en anglais que nous voulons traduire en français\", \\n \\'Isto deve ir para o português.\\',\\n \\'Y esto al español\\']\\n```\\n\\n## Resources'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 7721}, page_content='## Resources\\n\\n- [Translation task guide](../tasks/translation)\\n- [Summarization task guide](../tasks/summarization)\\n- [Causal language modeling task guide](../tasks/language_modeling)\\n\\n## MarianConfig\\n\\n[[autodoc]] MarianConfig\\n\\n## MarianTokenizer\\n\\n[[autodoc]] MarianTokenizer\\n    - build_inputs_with_special_tokens\\n\\n<frameworkcontent>\\n<pt>\\n\\n## MarianModel\\n\\n[[autodoc]] MarianModel\\n    - forward\\n\\n## MarianMTModel\\n\\n[[autodoc]] MarianMTModel\\n    - forward\\n\\n## MarianForCausalLM'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 8176}, page_content='## MarianForCausalLM\\n\\n[[autodoc]] MarianForCausalLM\\n    - forward\\n\\n</pt>\\n<tf>\\n\\n## TFMarianModel\\n\\n[[autodoc]] TFMarianModel\\n    - call\\n\\n## TFMarianMTModel\\n\\n[[autodoc]] TFMarianMTModel\\n    - call\\n\\n</tf>\\n<jax>\\n\\n## FlaxMarianModel\\n\\n[[autodoc]] FlaxMarianModel\\n    - __call__\\n\\n## FlaxMarianMTModel\\n\\n[[autodoc]] FlaxMarianMTModel\\n    - __call__\\n\\n</jax>\\n</frameworkcontent>'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 0}, page_content='!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 232}, page_content='http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 583}, page_content='⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer.\\n\\n-->\\n\\n# Wav2Vec2-Conformer\\n\\n## Overview\\n\\nThe Wav2Vec2-Conformer was added to an updated version of [fairseq S2T: Fast Speech-to-Text Modeling with fairseq](https://arxiv.org/abs/2010.05171) by Changhan Wang, Yun Tang, Xutai Ma, Anne Wu, Sravya Popuri, Dmytro Okhonko, Juan Pino.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 1025}, page_content='The official results of the model can be found in Table 3 and Table 4 of the paper.\\n\\nThe Wav2Vec2-Conformer weights were released by the Meta AI team within the [Fairseq library](https://github.com/pytorch/fairseq/blob/main/examples/wav2vec/README.md#pre-trained-models).\\n\\nThis model was contributed by [patrickvonplaten](https://huggingface.co/patrickvonplaten).\\nThe original code can be found [here](https://github.com/pytorch/fairseq/tree/main/examples/wav2vec).\\n\\n## Usage tips'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 1507}, page_content='- Wav2Vec2-Conformer follows the same architecture as Wav2Vec2, but replaces the *Attention*-block with a *Conformer*-block\\n  as introduced in [Conformer: Convolution-augmented Transformer for Speech Recognition](https://arxiv.org/abs/2005.08100).\\n- For the same number of layers, Wav2Vec2-Conformer requires more parameters than Wav2Vec2, but also yields \\nan improved word error rate.\\n- Wav2Vec2-Conformer uses the same tokenizer and feature extractor as Wav2Vec2.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 1973}, page_content='- Wav2Vec2-Conformer can use either no relative position embeddings, Transformer-XL-like position embeddings, or\\n  rotary position embeddings by setting the correct `config.position_embeddings_type`.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 2174}, page_content='## Resources\\n\\n- [Audio classification task guide](../tasks/audio_classification)\\n- [Automatic speech recognition task guide](../tasks/asr)\\n\\n## Wav2Vec2ConformerConfig\\n\\n[[autodoc]] Wav2Vec2ConformerConfig\\n\\n## Wav2Vec2Conformer specific outputs\\n\\n[[autodoc]] models.wav2vec2_conformer.modeling_wav2vec2_conformer.Wav2Vec2ConformerForPreTrainingOutput\\n\\n## Wav2Vec2ConformerModel\\n\\n[[autodoc]] Wav2Vec2ConformerModel\\n    - forward\\n\\n## Wav2Vec2ConformerForCTC'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 2600}, page_content='## Wav2Vec2ConformerForCTC\\n\\n[[autodoc]] Wav2Vec2ConformerForCTC\\n    - forward\\n\\n## Wav2Vec2ConformerForSequenceClassification\\n\\n[[autodoc]] Wav2Vec2ConformerForSequenceClassification\\n    - forward\\n\\n## Wav2Vec2ConformerForAudioFrameClassification\\n\\n[[autodoc]] Wav2Vec2ConformerForAudioFrameClassification\\n    - forward\\n\\n## Wav2Vec2ConformerForXVector\\n\\n[[autodoc]] Wav2Vec2ConformerForXVector\\n    - forward\\n\\n## Wav2Vec2ConformerForPreTraining\\n\\n[[autodoc]] Wav2Vec2ConformerForPreTraining\\n    - forward'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 0}, page_content='!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 232}, page_content='http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 583}, page_content=\"⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer.\\n\\n-->\\n\\n# TAPEX\\n\\n<Tip warning={true}>\\n\\nThis model is in maintenance mode only, we don't accept any new PRs changing its code.\\n\\nIf you run into any issues running this model, please reinstall the last version that supported this model: v4.30.0.\\nYou can do so by running the following command: `pip install -U transformers==4.30.0`.\\n\\n</Tip>\"),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 1075}, page_content='</Tip>\\n\\n## Overview\\n\\nThe TAPEX model was proposed in [TAPEX: Table Pre-training via Learning a Neural SQL Executor](https://arxiv.org/abs/2107.07653) by Qian Liu,\\nBei Chen, Jiaqi Guo, Morteza Ziyadi, Zeqi Lin, Weizhu Chen, Jian-Guang Lou. TAPEX pre-trains a BART model to solve synthetic SQL queries, after\\nwhich it can be fine-tuned to answer natural language questions related to tabular data, as well as performing table fact checking.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 1516}, page_content='TAPEX has been fine-tuned on several datasets: \\n- [SQA](https://www.microsoft.com/en-us/download/details.aspx?id=54253) (Sequential Question Answering by Microsoft)\\n- [WTQ](https://github.com/ppasupat/WikiTableQuestions) (Wiki Table Questions by Stanford University)\\n- [WikiSQL](https://github.com/salesforce/WikiSQL) (by Salesforce)\\n- [TabFact](https://tabfact.github.io/) (by USCB NLP Lab).\\n\\nThe abstract from the paper is the following:'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 1957}, page_content='*Recent progress in language model pre-training has achieved a great success via leveraging large-scale unstructured textual data. However, it is\\nstill a challenge to apply pre-training on structured tabular data due to the absence of large-scale high-quality tabular data. In this paper, we\\npropose TAPEX to show that table pre-training can be achieved by learning a neural SQL executor over a synthetic corpus, which is obtained by automatically'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 2405}, page_content='synthesizing executable SQL queries and their execution outputs. TAPEX addresses the data scarcity challenge via guiding the language model to mimic a SQL\\nexecutor on the diverse, large-scale and high-quality synthetic corpus. We evaluate TAPEX on four benchmark datasets. Experimental results demonstrate that\\nTAPEX outperforms previous table pre-training approaches by a large margin and achieves new state-of-the-art results on all of them. This includes improvements'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 2876}, page_content='on the weakly-supervised WikiSQL denotation accuracy to 89.5% (+2.3%), the WikiTableQuestions denotation accuracy to 57.5% (+4.8%), the SQA denotation accuracy\\nto 74.5% (+3.5%), and the TabFact accuracy to 84.2% (+3.2%). To our knowledge, this is the first work to exploit table pre-training via synthetic executable programs\\nand to achieve new state-of-the-art results on various downstream tasks.*'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 3277}, page_content='## Usage tips'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 3292}, page_content='- TAPEX is a generative (seq2seq) model. One can directly plug in the weights of TAPEX into a BART model. \\n- TAPEX has checkpoints on the hub that are either pre-trained only, or fine-tuned on WTQ, SQA, WikiSQL and TabFact.\\n- Sentences + tables are presented to the model as `sentence + \" \" + linearized table`. The linearized table has the following format: \\n  `col: col1 | col2 | col 3 row 1 : val1 | val2 | val3 row 2 : ...`.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 3721}, page_content='- TAPEX has its own tokenizer, that allows to prepare all data for the model easily. One can pass Pandas DataFrames and strings to the tokenizer,\\n  and it will automatically create the `input_ids` and `attention_mask` (as shown in the usage examples below).'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 3981}, page_content='### Usage: inference\\n\\nBelow, we illustrate how to use TAPEX for table question answering. As one can see, one can directly plug in the weights of TAPEX into a BART model.\\nWe use the [Auto API](auto), which will automatically instantiate the appropriate tokenizer ([`TapexTokenizer`]) and model ([`BartForConditionalGeneration`]) for us,\\nbased on the configuration file of the checkpoint on the hub.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 4381}, page_content='```python\\n>>> from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\n>>> import pandas as pd\\n\\n>>> tokenizer = AutoTokenizer.from_pretrained(\"microsoft/tapex-large-finetuned-wtq\")\\n>>> model = AutoModelForSeq2SeqLM.from_pretrained(\"microsoft/tapex-large-finetuned-wtq\")'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 4657}, page_content='>>> # prepare table + question\\n>>> data = {\"Actors\": [\"Brad Pitt\", \"Leonardo Di Caprio\", \"George Clooney\"], \"Number of movies\": [\"87\", \"53\", \"69\"]}\\n>>> table = pd.DataFrame.from_dict(data)\\n>>> question = \"how many movies does Leonardo Di Caprio have?\"\\n\\n>>> encoding = tokenizer(table, question, return_tensors=\"pt\")\\n\\n>>> # let the model generate an answer autoregressively\\n>>> outputs = model.generate(**encoding)'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 5072}, page_content=\">>> # decode back to text\\n>>> predicted_answer = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\\n>>> print(predicted_answer)\\n53\\n```\\n\\nNote that [`TapexTokenizer`] also supports batched inference. Hence, one can provide a batch of different tables/questions, or a batch of a single table\\nand multiple questions, or a batch of a single query and multiple tables. Let's illustrate this:\"),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 5469}, page_content='```python\\n>>> # prepare table + question\\n>>> data = {\"Actors\": [\"Brad Pitt\", \"Leonardo Di Caprio\", \"George Clooney\"], \"Number of movies\": [\"87\", \"53\", \"69\"]}\\n>>> table = pd.DataFrame.from_dict(data)\\n>>> questions = [\\n...     \"how many movies does Leonardo Di Caprio have?\",\\n...     \"which actor has 69 movies?\",\\n...     \"what\\'s the first name of the actor who has 87 movies?\",\\n... ]\\n>>> encoding = tokenizer(table, questions, padding=True, return_tensors=\"pt\")'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 5931}, page_content=\">>> # let the model generate an answer autoregressively\\n>>> outputs = model.generate(**encoding)\\n\\n>>> # decode back to text\\n>>> tokenizer.batch_decode(outputs, skip_special_tokens=True)\\n[' 53', ' george clooney', ' brad pitt']\\n```\"),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 6163}, page_content='In case one wants to do table verification (i.e. the task of determining whether a given sentence is supported or refuted by the contents\\nof a table), one can instantiate a [`BartForSequenceClassification`] model. TAPEX has checkpoints on the hub fine-tuned on TabFact, an important\\nbenchmark for table fact checking (it achieves 84% accuracy). The code example below again leverages the [Auto API](auto).\\n\\n```python\\n>>> from transformers import AutoTokenizer, AutoModelForSequenceClassification'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 6660}, page_content='>>> tokenizer = AutoTokenizer.from_pretrained(\"microsoft/tapex-large-finetuned-tabfact\")\\n>>> model = AutoModelForSequenceClassification.from_pretrained(\"microsoft/tapex-large-finetuned-tabfact\")\\n\\n>>> # prepare table + sentence\\n>>> data = {\"Actors\": [\"Brad Pitt\", \"Leonardo Di Caprio\", \"George Clooney\"], \"Number of movies\": [\"87\", \"53\", \"69\"]}\\n>>> table = pd.DataFrame.from_dict(data)\\n>>> sentence = \"George Clooney has 30 movies\"\\n\\n>>> encoding = tokenizer(table, sentence, return_tensors=\"pt\")'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 7156}, page_content='>>> # forward pass\\n>>> outputs = model(**encoding)\\n\\n>>> # print prediction\\n>>> predicted_class_idx = outputs.logits[0].argmax(dim=0).item()\\n>>> print(model.config.id2label[predicted_class_idx])\\nRefused\\n```\\n\\n<Tip> \\n\\nTAPEX architecture is the same as BART, except for tokenization. Refer to [BART documentation](bart) for information on \\nconfiguration classes and their parameters. TAPEX-specific tokenizer is documented below.  \\n\\n</Tip>\\n\\n## TapexTokenizer'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 7585}, page_content='</Tip>\\n\\n## TapexTokenizer\\n\\n[[autodoc]] TapexTokenizer\\n    - __call__\\n    - save_vocabulary'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 0}, page_content='!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 232}, page_content='http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 583}, page_content='⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer.\\n\\n-->\\n\\n# Video classification\\n\\n[[open-in-colab]]'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 794}, page_content='Video classification is the task of assigning a label or class to an entire video. Videos are expected to have only one class for each video. Video classification models take a video as input and return a prediction about which class the video belongs to. These models can be used to categorize what a video is all about. A real-world application of video classification is action / activity recognition, which is useful for fitness applications'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 1239}, page_content='. It is also helpful for vision-impaired individuals, especially when they are commuting.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 1330}, page_content='This guide will show you how to:\\n\\n1. Fine-tune [VideoMAE](https://huggingface.co/docs/transformers/main/en/model_doc/videomae) on a subset of the [UCF101](https://www.crcv.ucf.edu/data/UCF101.php) dataset.\\n2. Use your fine-tuned model for inference.\\n\\n<Tip>\\nThe task illustrated in this tutorial is supported by the following model architectures:\\n\\n<!--This tip is automatically generated by `make fix-copies`, do not fill manually!-->'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 1765}, page_content='[TimeSformer](../model_doc/timesformer), [VideoMAE](../model_doc/videomae), [ViViT](../model_doc/vivit)\\n\\n<!--End of the generated tip-->\\n\\n</Tip>\\n\\nBefore you begin, make sure you have all the necessary libraries installed:\\n\\n```bash\\npip install -q pytorchvideo transformers evaluate\\n```\\n\\nYou will use [PyTorchVideo](https://pytorchvideo.org/) (dubbed `pytorchvideo`) to process and prepare the videos.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 2166}, page_content='We encourage you to log in to your Hugging Face account so you can upload and share your model with the community. When prompted, enter your token to log in:\\n\\n```py\\n>>> from huggingface_hub import notebook_login\\n\\n>>> notebook_login()\\n```\\n\\n## Load UCF101 dataset\\n\\nStart by loading a subset of the [UCF-101 dataset](https://www.crcv.ucf.edu/data/UCF101.php). This will give you a chance to experiment and make sure everything works before spending more time training on the full dataset.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 2653}, page_content='```py\\n>>> from huggingface_hub import hf_hub_download\\n\\n>>> hf_dataset_identifier = \"sayakpaul/ucf101-subset\"\\n>>> filename = \"UCF101_subset.tar.gz\"\\n>>> file_path = hf_hub_download(repo_id=hf_dataset_identifier, filename=filename, repo_type=\"dataset\")\\n```\\n\\nAfter the subset has been downloaded, you need to extract the compressed archive:\\n\\n```py \\n>>> import tarfile\\n\\n>>> with tarfile.open(file_path) as t:\\n...      t.extractall(\".\")\\n```\\n\\nAt a high level, the dataset is organized like so:'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 3141}, page_content='```bash\\nUCF101_subset/\\n    train/\\n        BandMarching/\\n            video_1.mp4\\n            video_2.mp4\\n            ...\\n        Archery\\n            video_1.mp4\\n            video_2.mp4\\n            ...\\n        ...\\n    val/\\n        BandMarching/\\n            video_1.mp4\\n            video_2.mp4\\n            ...\\n        Archery\\n            video_1.mp4\\n            video_2.mp4\\n            ...\\n        ...\\n    test/\\n        BandMarching/\\n            video_1.mp4\\n            video_2.mp4\\n            ...'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 3608}, page_content='video_2.mp4\\n            ...\\n        Archery\\n            video_1.mp4\\n            video_2.mp4\\n            ...\\n        ...\\n```'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 3733}, page_content=\"The (`sorted`) video paths appear like so:\\n\\n```bash\\n...\\n'UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g07_c04.avi',\\n'UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g07_c06.avi',\\n'UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c01.avi',\\n'UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g09_c02.avi',\\n'UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g09_c06.avi'\\n...\\n```\"),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 4132}, page_content=\"You will notice that there are video clips belonging to the same group / scene where group is denoted by `g` in the video file paths. `v_ApplyEyeMakeup_g07_c04.avi` and `v_ApplyEyeMakeup_g07_c06.avi`, for example.\\n\\nFor the validation and evaluation splits, you wouldn't want to have video clips from the same group / scene to prevent [data leakage](https://www.kaggle.com/code/alexisbcook/data-leakage). The subset that you are using in this tutorial takes this information into account.\"),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 4621}, page_content='Next up, you will derive the set of labels present in the dataset. Also, create two dictionaries that\\'ll be helpful when initializing the model:\\n\\n* `label2id`: maps the class names to integers.\\n* `id2label`: maps the integers to class names. \\n\\n```py \\n>>> class_labels = sorted({str(path).split(\"/\")[2] for path in all_video_file_paths})\\n>>> label2id = {label: i for i, label in enumerate(class_labels)}\\n>>> id2label = {i: label for label, i in label2id.items()}'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 5084}, page_content='>>> print(f\"Unique classes: {list(label2id.keys())}.\")\\n\\n# Unique classes: [\\'ApplyEyeMakeup\\', \\'ApplyLipstick\\', \\'Archery\\', \\'BabyCrawling\\', \\'BalanceBeam\\', \\'BandMarching\\', \\'BaseballPitch\\', \\'Basketball\\', \\'BasketballDunk\\', \\'BenchPress\\'].\\n```\\n\\nThere are 10 unique classes. For each class, there are 30 videos in the training set.\\n\\n## Load a model to fine-tune'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 5408}, page_content=\"## Load a model to fine-tune\\n\\nInstantiate a video classification model from a pretrained checkpoint and its associated image processor. The model's encoder comes with pre-trained parameters, and the classification head is randomly initialized. The image processor will come in handy when writing the preprocessing pipeline for our dataset.\\n\\n```py \\n>>> from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification\"),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 5841}, page_content='>>> model_ckpt = \"MCG-NJU/videomae-base\"\\n>>> image_processor = VideoMAEImageProcessor.from_pretrained(model_ckpt)\\n>>> model = VideoMAEForVideoClassification.from_pretrained(\\n...     model_ckpt,\\n...     label2id=label2id,\\n...     id2label=id2label,\\n...     ignore_mismatched_sizes=True,  # provide this in case you\\'re planning to fine-tune an already fine-tuned checkpoint\\n... )\\n```\\n\\nWhile the model is loading, you might notice the following warning:'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 6293}, page_content=\"```bash\\nSome weights of the model checkpoint at MCG-NJU/videomae-base were not used when initializing VideoMAEForVideoClassification: [..., 'decoder.decoder_layers.1.attention.output.dense.bias', 'decoder.decoder_layers.2.attention.attention.key.weight']\"),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 6548}, page_content='- This IS expected if you are initializing VideoMAEForVideoClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\\n- This IS NOT expected if you are initializing VideoMAEForVideoClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 7043}, page_content=\"Some weights of VideoMAEForVideoClassification were not initialized from the model checkpoint at MCG-NJU/videomae-base and are newly initialized: ['classifier.bias', 'classifier.weight']\\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\\n```\"),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 7346}, page_content=\"The warning is telling us we are throwing away some weights (e.g. the weights and bias of the `classifier` layer) and randomly initializing some others (the weights and bias of a new `classifier` layer). This is expected in this case, because we are adding a new head for which we don't have pretrained weights, so the library warns us we should fine-tune this model before using it for inference, which is exactly what we are going to do.\"),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 7787}, page_content='**Note** that [this checkpoint](https://huggingface.co/MCG-NJU/videomae-base-finetuned-kinetics) leads to better performance on this task as the checkpoint was obtained fine-tuning on a similar downstream task having considerable domain overlap. You can check out [this checkpoint](https://huggingface.co/sayakpaul/videomae-base-finetuned-kinetics-finetuned-ucf101-subset) which was obtained by fine-tuning `MCG-NJU/videomae-base-finetuned-kinetics`.  \\n\\n## Prepare the datasets for training'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 8241}, page_content='## Prepare the datasets for training\\n\\nFor preprocessing the videos, you will leverage the [PyTorchVideo library](https://pytorchvideo.org/). Start by importing the dependencies we need. \\n\\n```py \\n>>> import pytorchvideo.data\\n\\n>>> from pytorchvideo.transforms import (\\n...     ApplyTransformToKey,\\n...     Normalize,\\n...     RandomShortSideScale,\\n...     RemoveKey,\\n...     ShortSideScale,\\n...     UniformTemporalSubsample,\\n... )'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 8670}, page_content='>>> from torchvision.transforms import (\\n...     Compose,\\n...     Lambda,\\n...     RandomCrop,\\n...     RandomHorizontalFlip,\\n...     Resize,\\n... )\\n```'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 8821}, page_content='For the training dataset transformations, use a combination of uniform temporal subsampling, pixel normalization, random cropping, and random horizontal flipping. For the validation and evaluation dataset transformations, keep the same transformation chain except for random cropping and horizontal flipping. To learn more about the details of these transformations check out the [official documentation of PyTorchVideo](https://pytorchvideo.org).'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 9272}, page_content='Use the `image_processor` associated with the pre-trained model to obtain the following information:\\n\\n* Image mean and standard deviation with which the video frame pixels will be normalized.\\n* Spatial resolution to which the video frames will be resized.\\n\\nStart by defining some constants.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 9529}, page_content='Start by defining some constants.\\n\\n```py\\n>>> mean = image_processor.image_mean\\n>>> std = image_processor.image_std\\n>>> if \"shortest_edge\" in image_processor.size:\\n...     height = width = image_processor.size[\"shortest_edge\"]\\n>>> else:\\n...     height = image_processor.size[\"height\"]\\n...     width = image_processor.size[\"width\"]\\n>>> resize_to = (height, width)'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 9892}, page_content='>>> num_frames_to_sample = model.config.num_frames\\n>>> sample_rate = 4\\n>>> fps = 30\\n>>> clip_duration = num_frames_to_sample * sample_rate / fps\\n```\\n\\nNow, define the dataset-specific transformations and the datasets respectively. Starting with the training set:'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 10156}, page_content='```py \\n>>> train_transform = Compose(\\n...     [\\n...         ApplyTransformToKey(\\n...             key=\"video\",\\n...             transform=Compose(\\n...                 [\\n...                     UniformTemporalSubsample(num_frames_to_sample),\\n...                     Lambda(lambda x: x / 255.0),\\n...                     Normalize(mean, std),\\n...                     RandomShortSideScale(min_size=256, max_size=320),\\n...                     RandomCrop(resize_to),'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 10568}, page_content='...                     RandomCrop(resize_to),\\n...                     RandomHorizontalFlip(p=0.5),\\n...                 ]\\n...             ),\\n...         ),\\n...     ]\\n... )'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 10741}, page_content='>>> train_dataset = pytorchvideo.data.Ucf101(\\n...     data_path=os.path.join(dataset_root_path, \"train\"),\\n...     clip_sampler=pytorchvideo.data.make_clip_sampler(\"random\", clip_duration),\\n...     decode_audio=False,\\n...     transform=train_transform,\\n... )\\n```\\n\\nThe same sequence of workflow can be applied to the validation and evaluation sets:'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 11090}, page_content='```py \\n>>> val_transform = Compose(\\n...     [\\n...         ApplyTransformToKey(\\n...             key=\"video\",\\n...             transform=Compose(\\n...                 [\\n...                     UniformTemporalSubsample(num_frames_to_sample),\\n...                     Lambda(lambda x: x / 255.0),\\n...                     Normalize(mean, std),\\n...                     Resize(resize_to),\\n...                 ]\\n...             ),\\n...         ),\\n...     ]\\n... )'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 11542}, page_content='>>> val_dataset = pytorchvideo.data.Ucf101(\\n...     data_path=os.path.join(dataset_root_path, \"val\"),\\n...     clip_sampler=pytorchvideo.data.make_clip_sampler(\"uniform\", clip_duration),\\n...     decode_audio=False,\\n...     transform=val_transform,\\n... )'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 11796}, page_content='>>> test_dataset = pytorchvideo.data.Ucf101(\\n...     data_path=os.path.join(dataset_root_path, \"test\"),\\n...     clip_sampler=pytorchvideo.data.make_clip_sampler(\"uniform\", clip_duration),\\n...     decode_audio=False,\\n...     transform=val_transform,\\n... )\\n```'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 12056}, page_content=\"**Note**: The above dataset pipelines are taken from the [official PyTorchVideo example](https://pytorchvideo.org/docs/tutorial_classification#dataset). We're using the [`pytorchvideo.data.Ucf101()`](https://pytorchvideo.readthedocs.io/en/latest/api/data/data.html#pytorchvideo.data.Ucf101) function because it's tailored for the UCF-101 dataset. Under the hood, it returns a [`pytorchvideo.data.labeled_video_dataset.LabeledVideoDataset`](https://pytorchvideo.readthedocs.io/en/latest/api/data/data\"),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 12516}, page_content='.readthedocs.io/en/latest/api/data/data.html#pytorchvideo.data.LabeledVideoDataset) object. `LabeledVideoDataset` class is the base class for all things video in the PyTorchVideo dataset. So, if you want to use a custom dataset not supported off-the-shelf by PyTorchVideo, you can extend the `LabeledVideoDataset` class accordingly. Refer to the `data` API [documentation to](https://pytorchvideo.readthedocs.io/en/latest/api/data/data.html) learn more'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 12924}, page_content='.io/en/latest/api/data/data.html) learn more. Also, if your dataset follows a similar structure (as shown above), then using the `pytorchvideo.data.Ucf101()` should work just fine.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 13107}, page_content='You can access the `num_videos` argument to know the number of videos in the dataset.\\n\\n```py\\n>>> print(train_dataset.num_videos, val_dataset.num_videos, test_dataset.num_videos)\\n# (300, 30, 75)\\n```\\n\\n## Visualize the preprocessed video for better debugging \\n\\n```py \\n>>> import imageio\\n>>> import numpy as np\\n>>> from IPython.display import Image'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 13453}, page_content='>>> def unnormalize_img(img):\\n...     \"\"\"Un-normalizes the image pixels.\"\"\"\\n...     img = (img * std) + mean\\n...     img = (img * 255).astype(\"uint8\")\\n...     return img.clip(0, 255)'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 13637}, page_content='>>> def create_gif(video_tensor, filename=\"sample.gif\"):\\n...     \"\"\"Prepares a GIF from a video tensor.\\n...     \\n...     The video tensor is expected to have the following shape:\\n...     (num_frames, num_channels, height, width).\\n...     \"\"\"\\n...     frames = []\\n...     for video_frame in video_tensor:\\n...         frame_unnormalized = unnormalize_img(video_frame.permute(1, 2, 0).numpy())\\n...         frames.append(frame_unnormalized)\\n...     kargs = {\"duration\": 0.25}'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 14073}, page_content='...     kargs = {\"duration\": 0.25}\\n...     imageio.mimsave(filename, frames, \"GIF\", **kargs)\\n...     return filename'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 14191}, page_content='>>> def display_gif(video_tensor, gif_name=\"sample.gif\"):\\n...     \"\"\"Prepares and displays a GIF from a video tensor.\"\"\"\\n...     video_tensor = video_tensor.permute(1, 0, 2, 3)\\n...     gif_filename = create_gif(video_tensor, gif_name)\\n...     return Image(filename=gif_filename)\\n\\n>>> sample_video = next(iter(train_dataset))\\n>>> video_tensor = sample_video[\"video\"]\\n>>> display_gif(video_tensor)\\n```'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 14592}, page_content='<div class=\"flex justify-center\">\\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/sample_gif.gif\" alt=\"Person playing basketball\"/>\\n</div>\\n\\n## Train the model'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 14816}, page_content='Leverage [`Trainer`](https://huggingface.co/docs/transformers/main_classes/trainer) from  🤗 Transformers for training the model. To instantiate a `Trainer`, you need to define the training configuration and an evaluation metric. The most important is the [`TrainingArguments`](https://huggingface.co/transformers/main_classes/trainer.html#transformers.TrainingArguments), which is a class that contains all the attributes to configure the training'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 15263}, page_content='. It requires an output folder name, which will be used to save the checkpoints of the model. It also helps sync all the information in the model repository on 🤗 Hub.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 15431}, page_content=\"Most of the training arguments are self-explanatory, but one that is quite important here is `remove_unused_columns=False`. This one will drop any features not used by the model's call function. By default it's `True` because usually it's ideal to drop unused feature columns, making it easier to unpack inputs into the model's call function\"),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 15772}, page_content=\". But, in this case, you need the unused features ('video' in particular) in order to create `pixel_values` (which is a mandatory key our model expects in its inputs).\"),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 15942}, page_content='```py \\n>>> from transformers import TrainingArguments, Trainer\\n\\n>>> model_name = model_ckpt.split(\"/\")[-1]\\n>>> new_model_name = f\"{model_name}-finetuned-ucf101-subset\"\\n>>> num_epochs = 4'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 16130}, page_content='>>> args = TrainingArguments(\\n...     new_model_name,\\n...     remove_unused_columns=False,\\n...     evaluation_strategy=\"epoch\",\\n...     save_strategy=\"epoch\",\\n...     learning_rate=5e-5,\\n...     per_device_train_batch_size=batch_size,\\n...     per_device_eval_batch_size=batch_size,\\n...     warmup_ratio=0.1,\\n...     logging_steps=10,\\n...     load_best_model_at_end=True,\\n...     metric_for_best_model=\"accuracy\",\\n...     push_to_hub=True,'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 16543}, page_content='...     push_to_hub=True,\\n...     max_steps=(train_dataset.num_videos // batch_size) * num_epochs,\\n... )\\n```'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 16653}, page_content='The dataset returned by `pytorchvideo.data.Ucf101()` doesn\\'t implement the `__len__` method. As such, we must define `max_steps` when instantiating `TrainingArguments`. \\n\\nNext, you need to define a function to compute the metrics from the predictions, which will use the `metric` you\\'ll load now. The only preprocessing you have to do is to take the argmax of our predicted logits:\\n\\n```py\\nimport evaluate\\n\\nmetric = evaluate.load(\"accuracy\")'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 17059}, page_content='metric = evaluate.load(\"accuracy\")\\n\\n\\ndef compute_metrics(eval_pred):\\n    predictions = np.argmax(eval_pred.predictions, axis=1)\\n    return metric.compute(predictions=predictions, references=eval_pred.label_ids)\\n```\\n\\n**A note on evaluation**:'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 17275}, page_content=\"**A note on evaluation**:\\n\\nIn the [VideoMAE paper](https://arxiv.org/abs/2203.12602), the authors use the following evaluation strategy. They evaluate the model on several clips from test videos and apply different crops to those clips and report the aggregate score. However, in the interest of simplicity and brevity, we don't consider that in this tutorial.\"),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 17637}, page_content='Also, define a `collate_fn`, which will be used to batch examples together. Each batch consists of 2 keys, namely `pixel_values` and `labels`.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 17781}, page_content='```py \\n>>> def collate_fn(examples):\\n...     # permute to (num_frames, num_channels, height, width)\\n...     pixel_values = torch.stack(\\n...         [example[\"video\"].permute(1, 0, 2, 3) for example in examples]\\n...     )\\n...     labels = torch.tensor([example[\"label\"] for example in examples])\\n...     return {\"pixel_values\": pixel_values, \"labels\": labels}\\n```\\n\\nThen you just pass all of this along with the datasets to `Trainer`:'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 18215}, page_content='```py \\n>>> trainer = Trainer(\\n...     model,\\n...     args,\\n...     train_dataset=train_dataset,\\n...     eval_dataset=val_dataset,\\n...     tokenizer=image_processor,\\n...     compute_metrics=compute_metrics,\\n...     data_collator=collate_fn,\\n... )\\n```\\n\\nYou might wonder why you passed along the `image_processor` as a tokenizer when you preprocessed the data already. This is only to make sure the image processor configuration file (stored as JSON) will also be uploaded to the repo on the Hub.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 18710}, page_content='Now fine-tune our model by calling the `train` method:\\n\\n```py \\n>>> train_results = trainer.train()\\n```\\n\\nOnce training is completed, share your model to the Hub with the [`~transformers.Trainer.push_to_hub`] method so everyone can use your model:\\n\\n```py\\n>>> trainer.push_to_hub()\\n```\\n\\n## Inference\\n\\nGreat, now that you have fine-tuned a model, you can use it for inference!\\n\\nLoad a video for inference:\\n\\n```py \\n>>> sample_test_video = next(iter(test_dataset))\\n```'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 19174}, page_content='<div class=\"flex justify-center\">\\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/sample_gif_two.gif\" alt=\"Teams playing basketball\"/>\\n</div>'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 19380}, page_content='The simplest way to try out your fine-tuned model for inference is to use it in a [`pipeline`](https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#transformers.VideoClassificationPipeline). Instantiate a `pipeline` for video classification with your model, and pass your video to it:\\n\\n```py\\n>>> from transformers import pipeline'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 19730}, page_content='>>> video_cls = pipeline(model=\"my_awesome_video_cls_model\")\\n>>> video_cls(\"https://huggingface.co/datasets/sayakpaul/ucf101-subset/resolve/main/v_BasketballDunk_g14_c06.avi\")\\n[{\\'score\\': 0.9272987842559814, \\'label\\': \\'BasketballDunk\\'},\\n {\\'score\\': 0.017777055501937866, \\'label\\': \\'BabyCrawling\\'},\\n {\\'score\\': 0.01663011871278286, \\'label\\': \\'BalanceBeam\\'},\\n {\\'score\\': 0.009560945443809032, \\'label\\': \\'BandMarching\\'},\\n {\\'score\\': 0.0068979403004050255, \\'label\\': \\'BaseballPitch\\'}]\\n```'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 20206}, page_content='You can also manually replicate the results of the `pipeline` if you\\'d like.\\n\\n\\n```py\\n>>> def run_inference(model, video):\\n...     # (num_frames, num_channels, height, width)\\n...     perumuted_sample_test_video = video.permute(1, 0, 2, 3)\\n...     inputs = {\\n...         \"pixel_values\": perumuted_sample_test_video.unsqueeze(0),\\n...         \"labels\": torch.tensor(\\n...             [sample_test_video[\"label\"]]\\n...         ),  # this can be skipped if you don\\'t have labels available.\\n...     }'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 20699}, page_content='...     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\\n...     inputs = {k: v.to(device) for k, v in inputs.items()}\\n...     model = model.to(device)\\n\\n...     # forward pass\\n...     with torch.no_grad():\\n...         outputs = model(**inputs)\\n...         logits = outputs.logits\\n\\n...     return logits\\n```\\n\\nNow, pass your input to the model and return the `logits`:\\n\\n```\\n>>> logits = run_inference(trained_model, sample_test_video[\"video\"])\\n```'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 21167}, page_content='Decoding the `logits`, we get: \\n\\n```py \\n>>> predicted_class_idx = logits.argmax(-1).item()\\n>>> print(\"Predicted class:\", model.config.id2label[predicted_class_idx])\\n# Predicted class: BasketballDunk\\n```'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 0}, page_content='!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 232}, page_content='http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 583}, page_content='⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer.\\n\\n-->\\n\\n# OpenAI GPT'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 746}, page_content='-->\\n\\n# OpenAI GPT\\n\\n<div class=\"flex flex-wrap space-x-1\">\\n<a href=\"https://huggingface.co/models?filter=openai-gpt\">\\n<img alt=\"Models\" src=\"https://img.shields.io/badge/All_model_pages-openai--gpt-blueviolet\">\\n</a>\\n<a href=\"https://huggingface.co/spaces/docs-demos/openai-gpt\">\\n<img alt=\"Spaces\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue\">\\n</a>\\n</div>\\n\\n## Overview'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 1133}, page_content=\"## Overview\\n\\nOpenAI GPT model was proposed in [Improving Language Understanding by Generative Pre-Training](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)\\nby Alec Radford, Karthik Narasimhan, Tim Salimans and Ilya Sutskever. It's a causal (unidirectional) transformer\\npre-trained using language modeling on a large corpus will long range dependencies, the Toronto Book Corpus.\\n\\nThe abstract from the paper is the following:\"),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 1632}, page_content='*Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering,\\nsemantic similarity assessment, and document classification. Although large unlabeled text corpora are abundant,\\nlabeled data for learning these specific tasks is scarce, making it challenging for discriminatively trained models to\\nperform adequately. We demonstrate that large gains on these tasks can be realized by generative pretraining of a'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 2098}, page_content='language model on a diverse corpus of unlabeled text, followed by discriminative fine-tuning on each specific task. In\\ncontrast to previous approaches, we make use of task-aware input transformations during fine-tuning to achieve\\neffective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our\\napproach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 2565}, page_content='discriminatively trained models that use architectures specifically crafted for each task, significantly improving upon\\nthe state of the art in 9 out of the 12 tasks studied.*'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 2742}, page_content='[Write With Transformer](https://transformer.huggingface.co/doc/gpt) is a webapp created and hosted by Hugging Face\\nshowcasing the generative capabilities of several models. GPT is one of them.\\n\\nThis model was contributed by [thomwolf](https://huggingface.co/thomwolf). The original code can be found [here](https://github.com/openai/finetune-transformer-lm).\\n\\n## Usage tips'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 3103}, page_content=\"## Usage tips\\n\\n- GPT is a model with absolute position embeddings so it's usually advised to pad the inputs on the right rather than\\n  the left.\\n- GPT was trained with a causal language modeling (CLM) objective and is therefore powerful at predicting the next\\n  token in a sequence. Leveraging this feature allows GPT-2 to generate syntactically coherent text as it can be\\n  observed in the *run_generation.py* example script.\\n\\n\\nNote:\"),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 3532}, page_content=\"Note:\\n\\nIf you want to reproduce the original tokenization process of the *OpenAI GPT* paper, you will need to install `ftfy`\\nand `SpaCy`:\\n\\n```bash\\npip install spacy ftfy==4.4.3\\npython -m spacy download en\\n```\\n\\nIf you don't install `ftfy` and `SpaCy`, the [`OpenAIGPTTokenizer`] will default to tokenize\\nusing BERT's `BasicTokenizer` followed by Byte-Pair Encoding (which should be fine for most usage, don't worry).\\n\\n## Resources\"),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 3949}, page_content='## Resources\\n\\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with OpenAI GPT. If you\\'re interested in submitting a resource to be included here, please feel free to open a Pull Request and we\\'ll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\\n\\n<PipelineTag pipeline=\"text-classification\"/>'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 4305}, page_content='<PipelineTag pipeline=\"text-classification\"/>\\n\\n- A blog post on [outperforming OpenAI GPT-3 with SetFit for text-classification](https://www.philschmid.de/getting-started-setfit).\\n- See also: [Text classification task guide](../tasks/sequence_classification)\\n\\n<PipelineTag pipeline=\"text-generation\"/>'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 4608}, page_content='- A blog on how to [Finetune a non-English GPT-2 Model with Hugging Face](https://www.philschmid.de/fine-tune-a-non-english-gpt-2-model-with-huggingface).\\n- A blog on [How to generate text: using different decoding methods for language generation with Transformers](https://huggingface.co/blog/how-to-generate) with GPT-2.\\n- A blog on [Training CodeParrot 🦜 from Scratch](https://huggingface.co/blog/codeparrot), a large GPT-2 model.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 5042}, page_content='- A blog on [Faster Text Generation with TensorFlow and XLA](https://huggingface.co/blog/tf-xla-generate) with GPT-2.\\n- A blog on [How to train a Language Model with Megatron-LM](https://huggingface.co/blog/megatron-training) with a GPT-2 model.\\n- A notebook on how to [finetune GPT2 to generate lyrics in the style of your favorite artist](https://colab.research.google.com/github/AlekseyKorshuk/huggingartists/blob/master/huggingartists-demo.ipynb). 🌎'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 5496}, page_content='- A notebook on how to [finetune GPT2 to generate tweets in the style of your favorite Twitter user](https://colab.research.google.com/github/borisdayma/huggingtweets/blob/master/huggingtweets-demo.ipynb). 🌎\\n- [Causal language modeling](https://huggingface.co/course/en/chapter7/6?fw=pt#training-a-causal-language-model-from-scratch) chapter of the 🤗 Hugging Face Course.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 5868}, page_content='- [`OpenAIGPTLMHeadModel`] is supported by this [causal language modeling example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/language-modeling#gpt-2gpt-and-causal-language-modeling), [text generation example script](https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-generation/run_generation.py) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling.ipynb).'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 6349}, page_content='- [`TFOpenAIGPTLMHeadModel`] is supported by this [causal language modeling example script](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/language-modeling#run_clmpy) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling-tf.ipynb).\\n- See also: [Causal language modeling task guide](../tasks/language_modeling)'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 6749}, page_content='<PipelineTag pipeline=\"token-classification\"/>\\n\\n- A course material on [Byte-Pair Encoding tokenization](https://huggingface.co/course/en/chapter6/5).\\n\\n## OpenAIGPTConfig\\n\\n[[autodoc]] OpenAIGPTConfig\\n\\n## OpenAIGPTTokenizer\\n\\n[[autodoc]] OpenAIGPTTokenizer\\n    - save_vocabulary\\n\\n## OpenAIGPTTokenizerFast\\n\\n[[autodoc]] OpenAIGPTTokenizerFast\\n\\n## OpenAI specific outputs\\n\\n[[autodoc]] models.openai.modeling_openai.OpenAIGPTDoubleHeadsModelOutput'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 7193}, page_content='[[autodoc]] models.openai.modeling_tf_openai.TFOpenAIGPTDoubleHeadsModelOutput\\n\\n<frameworkcontent>\\n<pt>\\n\\n## OpenAIGPTModel\\n\\n[[autodoc]] OpenAIGPTModel\\n    - forward\\n\\n## OpenAIGPTLMHeadModel\\n\\n[[autodoc]] OpenAIGPTLMHeadModel\\n    - forward\\n\\n## OpenAIGPTDoubleHeadsModel\\n\\n[[autodoc]] OpenAIGPTDoubleHeadsModel\\n    - forward\\n\\n## OpenAIGPTForSequenceClassification\\n\\n[[autodoc]] OpenAIGPTForSequenceClassification\\n    - forward\\n\\n</pt>\\n<tf>\\n\\n## TFOpenAIGPTModel\\n\\n[[autodoc]] TFOpenAIGPTModel\\n    - call'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 7649}, page_content='[[autodoc]] TFOpenAIGPTModel\\n    - call\\n\\n## TFOpenAIGPTLMHeadModel\\n\\n[[autodoc]] TFOpenAIGPTLMHeadModel\\n    - call\\n\\n## TFOpenAIGPTDoubleHeadsModel\\n\\n[[autodoc]] TFOpenAIGPTDoubleHeadsModel\\n    - call\\n\\n## TFOpenAIGPTForSequenceClassification\\n\\n[[autodoc]] TFOpenAIGPTForSequenceClassification\\n    - call\\n\\n</tf>\\n</frameworkcontent>'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 0}, page_content='!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\");\\nyou may not use this file except in compliance with the License.\\nYou may obtain a copy of the License at\\n\\n    http://www.apache.org/licenses/LICENSE-2.0'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 238}, page_content='http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \"AS IS\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\nSee the License for the specific language governing permissions and\\nlimitations under the License.\\n-->'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 593}, page_content='<!---\\nA useful guide for English-Chinese translation of Hugging Face documentation\\n- Add space around English words and numbers when they appear between Chinese characters. E.g., 共 100 多种语言; 使用 transformers 库。\\n- Use square quotes, e.g.,「引用」\\n\\nDictionary'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 835}, page_content='Dictionary\\n\\nHugging Face: 抱抱脸\\ntoken: 词符（并用括号标注原英文）\\ntokenize: 词符化（并用括号标注原英文）\\ntokenizer: 词符化器（并用括号标注原英文）\\ntransformer: transformer（不翻译）\\npipeline: 流水线\\nAPI: API (不翻译）\\ninference: 推理\\nTrainer: 训练器。当作为类名出现时不翻译。\\npretrained/pretrain: 预训练\\nfinetune: 微调\\ncommunity: 社区\\nexample: 当特指仓库中 example 目录时翻译为「用例」\\nPython data structures (e.g., list, set, dict): 翻译为列表，集合，词典，并用括号标注原英文\\nNLP/Natural Language Processing: 以 NLP 出现时不翻译，以 Natural Language Processing 出现时翻译为自然语言处理\\ncheckpoint: 检查点\\n-->'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 1304}, page_content='<p align=\"center\">\\n    <br>\\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers_logo_name.png\" width=\"400\"/>\\n    <br>\\n</p>\\n<p align=\"center\">\\n    <a href=\"https://circleci.com/gh/huggingface/transformers\">\\n        <img alt=\"Build\" src=\"https://img.shields.io/circleci/build/github/huggingface/transformers/main\">\\n    </a>\\n    <a href=\"https://github.com/huggingface/transformers/blob/main/LICENSE\">'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 1764}, page_content='<img alt=\"GitHub\" src=\"https://img.shields.io/github/license/huggingface/transformers.svg?color=blue\">\\n    </a>\\n    <a href=\"https://huggingface.co/docs/transformers/index\">\\n        <img alt=\"Documentation\" src=\"https://img.shields.io/website/http/huggingface.co/docs/transformers/index.svg?down_color=red&down_message=offline&up_message=online\">\\n    </a>\\n    <a href=\"https://github.com/huggingface/transformers/releases\">'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 2196}, page_content='<img alt=\"GitHub release\" src=\"https://img.shields.io/github/release/huggingface/transformers.svg\">\\n    </a>\\n    <a href=\"https://github.com/huggingface/transformers/blob/main/CODE_OF_CONDUCT.md\">\\n        <img alt=\"Contributor Covenant\" src=\"https://img.shields.io/badge/Contributor%20Covenant-v2.0%20adopted-ff69b4.svg\">\\n    </a>\\n    <a href=\"https://zenodo.org/badge/latestdoi/155220641\"><img src=\"https://zenodo.org/badge/155220641.svg\" alt=\"DOI\"></a>\\n</p>'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 2657}, page_content='<h4 align=\"center\">\\n    <p>\\n        <a href=\"https://github.com/huggingface/transformers/\">English</a> |\\n        <b>简体中文</b> |\\n        <a href=\"https://github.com/huggingface/transformers/blob/main/README_zh-hant.md\">繁體中文</a> |\\n        <a href=\"https://github.com/huggingface/transformers/blob/main/README_ko.md\">한국어</a> |\\n        <a href=\"https://github.com/huggingface/transformers/blob/main/README_es.md\">Español</a> |'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 3087}, page_content='<a href=\"https://github.com/huggingface/transformers/blob/main/README_ja.md\">日本語</a> |\\n        <a href=\"https://github.com/huggingface/transformers/blob/main/README_hd.md\">हिन्दी</a>\\n        <a href=\"https://github.com/huggingface/transformers//blob/main/README_te.md\">తెలుగు</a> |\\n    </p>\\n</h4>'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 3385}, page_content='<h3 align=\"center\">\\n    <p>为 Jax、PyTorch 和 TensorFlow 打造的先进的自然语言处理</p>\\n</h3>\\n\\n<h3 align=\"center\">\\n    <a href=\"https://hf.co/course\"><img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/course_banner.png\"></a>\\n</h3>\\n\\n🤗 Transformers 提供了数以千计的预训练模型，支持 100 多种语言的文本分类、信息抽取、问答、摘要、翻译、文本生成。它的宗旨是让最先进的 NLP 技术人人易用。\\n\\n🤗 Transformers 提供了便于快速下载和使用的API，让你可以把预训练模型用在给定文本、在你的数据集上微调然后通过 [model hub](https://huggingface.co/models) 与社区共享。同时，每个定义的 Python 模块均完全独立，方便修改和快速研究实验。'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 3876}, page_content='🤗 Transformers 支持三个最热门的深度学习库： [Jax](https://jax.readthedocs.io/en/latest/), [PyTorch](https://pytorch.org/) 以及 [TensorFlow](https://www.tensorflow.org/) — 并与之无缝整合。你可以直接使用一个框架训练你的模型然后用另一个加载和推理。\\n\\n## 在线演示\\n\\n你可以直接在模型页面上测试大多数 [model hub](https://huggingface.co/models) 上的模型。 我们也提供了 [私有模型托管、模型版本管理以及推理API](https://huggingface.co/pricing)。'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 4209}, page_content='这里是一些例子：\\n- [用 BERT 做掩码填词](https://huggingface.co/bert-base-uncased?text=Paris+is+the+%5BMASK%5D+of+France)\\n- [用 Electra 做命名实体识别](https://huggingface.co/dbmdz/electra-large-discriminator-finetuned-conll03-english?text=My+name+is+Sarah+and+I+live+in+London+city)\\n- [用 GPT-2 做文本生成](https://huggingface.co/gpt2?text=A+long+time+ago%2C+)\\n- [用 RoBERTa 做自然语言推理](https://huggingface.co/roberta-large-mnli?text=The+dog+was+lost.+Nobody+lost+any+animal)'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 4653}, page_content='- [用 BART 做文本摘要](https://huggingface.co/facebook/bart-large-cnn?text=The+tower+is+324+metres+%281%2C063+ft%29+tall%2C+about+the+same+height+as+an+81-storey+building%2C+and+the+tallest+structure+in+Paris.+Its+base+is+square%2C+measuring+125+metres+%28410+ft%29+on+each+side'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 4925}, page_content='.+During+its+construction%2C+the+Eiffel+Tower+surpassed+the+Washington+Monument+to+become+the+tallest+man-made+structure+in+the+world%2C+a+title+it+held+for+41+years+until+the+Chrysler+Building+in+New+York+City+was+finished+in+1930.+It+was+the+first+structure+to+reach+a+height+of+300+metres.+Due+to+the+addition+of+a+broadcasting+aerial+at+the+top+of+the+tower+in+1957%2C+it+is+now+taller+than+the+Chrysler+Building+by+5.2+metres+%2817+ft%29'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 5346}, page_content='.2+metres+%2817+ft%29.+Excluding+transmitters%2C+the+Eiffel+Tower+is+the+second+tallest+free-standing+structure+in+France+after+the+Millau+Viaduct)'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 5494}, page_content='- [用 DistilBERT 做问答](https://huggingface'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 5534}, page_content='.co/distilbert-base-uncased-distilled-squad?text=Which+name+is+also+used+to+describe+the+Amazon+rainforest+in+English%3F&context=The+Amazon+rainforest+%28Portuguese%3A+Floresta+Amaz%C3%B4nica+or+Amaz%C3%B4nia%3B+Spanish%3A+Selva+Amaz%C3%B3nica%2C+Amazon%C3%ADa+or+usually+Amazonia%3B+French%3A+For%C3%AAt+amazonienne%3B+Dutch%3A+Amazoneregenwoud%29%2C+also+known+in+English+as+Amazonia+or+the+Amazon+Jungle%2C+is+a+moist+broadleaf+forest+that+covers+most+of+the+Amazon+basin+of+South+America'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 6025}, page_content='.+This+basin+encompasses+7%2C000%2C000+square+kilometres+%282%2C700%2C000+sq+mi%29%2C+of+which+5%2C500%2C000+square+kilometres+%282%2C100%2C000+sq+mi%29+are+covered+by+the+rainforest.+This+region+includes+territory+belonging+to+nine+nations.+The+majority+of+the+forest+is+contained+within+Brazil%2C+with+60%25+of+the+rainforest%2C+followed+by+Peru+with+13%25%2C+Colombia+with+10%25%2C+and+with+minor+amounts+in+Venezuela%2C+Ecuador%2C+Bolivia%2C+Guyana%2C+Suriname+and+French+Guiana'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 6507}, page_content='.+States+or+departments+in+four+nations+contain+%22Amazonas%22+in+their+names.+The+Amazon+represents+over+half+of+the+planet%27s+remaining+rainforests%2C+and+comprises+the+largest+and+most+biodiverse+tract+of+tropical+rainforest+in+the+world%2C+with+an+estimated+390+billion+individual+trees+divided+into+16%2C000+species)'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 6830}, page_content='- [用 T5 做翻译](https://huggingface.co/t5-base?text=My+name+is+Wolfgang+and+I+live+in+Berlin)'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 6922}, page_content='**[Write With Transformer](https://transformer.huggingface.co)**，由抱抱脸团队打造，是一个文本生成的官方 demo。\\n\\n## 如果你在寻找由抱抱脸团队提供的定制化支持服务\\n\\n<a target=\"_blank\" href=\"https://huggingface.co/support\">\\n    <img alt=\"HuggingFace Expert Acceleration Program\" src=\"https://huggingface.co/front/thumbnails/support.png\" style=\"max-width: 600px; border: 1px solid #eee; border-radius: 4px; box-shadow: 0 1px 2px 0 rgba(0, 0, 0, 0.05);\">\\n</a><br>\\n\\n## 快速上手'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 7338}, page_content=\"## 快速上手\\n\\n我们为快速使用模型提供了 `pipeline` （流水线）API。流水线聚合了预训练模型和对应的文本预处理。下面是一个快速使用流水线去判断正负面情绪的例子：\\n\\n```python\\n>>> from transformers import pipeline\\n\\n# 使用情绪分析流水线\\n>>> classifier = pipeline('sentiment-analysis')\\n>>> classifier('We are very happy to introduce pipeline to the transformers repository.')\\n[{'label': 'POSITIVE', 'score': 0.9996980428695679}]\\n```\\n\\n第二行代码下载并缓存了流水线使用的预训练模型，而第三行代码则在给定的文本上进行了评估。这里的答案“正面” (positive) 具有 99 的置信度。\\n\\n许多的 NLP 任务都有开箱即用的预训练流水线。比如说，我们可以轻松的从给定文本中抽取问题答案：\"),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 7761}, page_content=\"许多的 NLP 任务都有开箱即用的预训练流水线。比如说，我们可以轻松的从给定文本中抽取问题答案：\\n\\n``` python\\n>>> from transformers import pipeline\\n\\n# 使用问答流水线\\n>>> question_answerer = pipeline('question-answering')\\n>>> question_answerer({\\n...     'question': 'What is the name of the repository ?',\\n...     'context': 'Pipeline has been included in the huggingface/transformers repository'\\n... })\\n{'score': 0.30970096588134766, 'start': 34, 'end': 58, 'answer': 'huggingface/transformers'}\\n\\n```\"),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 8202}, page_content='```\\n\\n除了给出答案，预训练模型还给出了对应的置信度分数、答案在词符化 (tokenized) 后的文本中开始和结束的位置。你可以从[这个教程](https://huggingface.co/docs/transformers/task_summary)了解更多流水线API支持的任务。\\n\\n要在你的任务上下载和使用任意预训练模型也很简单，只需三行代码。这里是 PyTorch 版的示例：\\n```python\\n>>> from transformers import AutoTokenizer, AutoModel\\n\\n>>> tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\\n>>> model = AutoModel.from_pretrained(\"bert-base-uncased\")'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 8589}, page_content='>>> inputs = tokenizer(\"Hello world!\", return_tensors=\"pt\")\\n>>> outputs = model(**inputs)\\n```\\n这里是等效的 TensorFlow 代码：\\n```python\\n>>> from transformers import AutoTokenizer, TFAutoModel\\n\\n>>> tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\\n>>> model = TFAutoModel.from_pretrained(\"bert-base-uncased\")\\n\\n>>> inputs = tokenizer(\"Hello world!\", return_tensors=\"tf\")\\n>>> outputs = model(**inputs)\\n```'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 8996}, page_content='词符化器 (tokenizer) 为所有的预训练模型提供了预处理，并可以直接对单个字符串进行调用（比如上面的例子）或对列表 (list) 调用。它会输出一个你可以在下游代码里使用或直接通过 `**` 解包表达式传给模型的词典 (dict)。\\n\\n模型本身是一个常规的 [Pytorch `nn.Module`](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) 或 [TensorFlow `tf.keras.Model`](https://www.tensorflow.org/api_docs/python/tf/keras/Model)（取决于你的后端），可以常规方式使用。 [这个教程](https://huggingface.co/transformers/training.html)解释了如何将这样的模型整合到经典的 PyTorch 或 TensorFlow 训练循环中，或是如何使用我们的 `Trainer` 训练器）API 来在一个新的数据集上快速微调。\\n\\n## 为什么要用 transformers？'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 9465}, page_content='## 为什么要用 transformers？\\n\\n1. 便于使用的先进模型：\\n    - NLU 和 NLG 上表现优越\\n    - 对教学和实践友好且低门槛\\n    - 高级抽象，只需了解三个类\\n    - 对所有模型统一的API\\n\\n1. 更低计算开销，更少的碳排放：\\n    - 研究人员可以分享已训练的模型而非每次从头开始训练\\n    - 工程师可以减少计算用时和生产环境开销\\n    - 数十种模型架构、两千多个预训练模型、100多种语言支持\\n\\n1. 对于模型生命周期的每一个部分都面面俱到：\\n    - 训练先进的模型，只需 3 行代码\\n    - 模型在不同深度学习框架间任意转移，随你心意\\n    - 为训练、评估和生产选择最适合的框架，衔接无缝\\n\\n1. 为你的需求轻松定制专属模型和用例：\\n    - 我们为每种模型架构提供了多个用例来复现原论文结果\\n    - 模型内部结构保持透明一致\\n    - 模型文件可单独使用，方便魔改和快速实验\\n\\n## 什么情况下我不该用 transformers？'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 9894}, page_content='## 什么情况下我不该用 transformers？\\n\\n- 本库并不是模块化的神经网络工具箱。模型文件中的代码特意呈若璞玉，未经额外抽象封装，以便研究人员快速迭代魔改而不致溺于抽象和文件跳转之中。\\n- `Trainer` API 并非兼容任何模型，只为本库之模型优化。若是在寻找适用于通用机器学习的训练循环实现，请另觅他库。\\n- 尽管我们已尽力而为，[examples 目录](https://github.com/huggingface/transformers/tree/main/examples)中的脚本也仅为用例而已。对于你的特定问题，它们并不一定开箱即用，可能需要改几行代码以适之。\\n\\n## 安装\\n\\n### 使用 pip\\n\\n这个仓库已在 Python 3.8+、Flax 0.4.1+、PyTorch 1.10+ 和 TensorFlow 2.6+ 下经过测试。'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 10283}, page_content='你可以在[虚拟环境](https://docs.python.org/3/library/venv.html)中安装 🤗 Transformers。如果你还不熟悉 Python 的虚拟环境，请阅此[用户说明](https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/)。\\n\\n首先，用你打算使用的版本的 Python 创建一个虚拟环境并激活。\\n\\n然后，你需要安装 Flax、PyTorch 或 TensorFlow 其中之一。关于在你使用的平台上安装这些框架，请参阅 [TensorFlow 安装页](https://www.tensorflow.org/install/), [PyTorch 安装页](https://pytorch.org/get-started/locally/#start-locally) 或 [Flax 安装页](https://github.com/google/flax#quick-install)。'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 10756}, page_content='当这些后端之一安装成功后， 🤗 Transformers 可依此安装：\\n\\n```bash\\npip install transformers\\n```\\n\\n如果你想要试试用例或者想在正式发布前使用最新的开发中代码，你得[从源代码安装](https://huggingface.co/docs/transformers/installation#installing-from-source)。\\n\\n### 使用 conda\\n\\n自 Transformers 4.0.0 版始，我们有了一个 conda 频道： `huggingface`。\\n\\n🤗 Transformers 可以通过 conda 依此安装：\\n\\n```shell script\\nconda install -c huggingface transformers\\n```\\n\\n要通过 conda 安装 Flax、PyTorch 或 TensorFlow 其中之一，请参阅它们各自安装页的说明。\\n\\n## 模型架构'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 11178}, page_content='## 模型架构\\n\\n🤗 Transformers 支持的[**所有的模型检查点**](https://huggingface.co/models)由[用户](https://huggingface.co/users)和[组织](https://huggingface.co/organizations)上传，均与 huggingface.co [model hub](https://huggingface.co) 无缝整合。\\n\\n目前的检查点数量： ![](https://img.shields.io/endpoint?url=https://huggingface.co/api/shields/models&color=brightgreen)\\n\\n🤗 Transformers 目前支持如下的架构（模型概述请阅[这里](https://huggingface.co/docs/transformers/model_summary)）：'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 11599}, page_content='1. **[ALBERT](https://huggingface.co/docs/transformers/model_doc/albert)** (来自 Google Research and the Toyota Technological Institute at Chicago) 伴随论文 [ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://arxiv.org/abs/1909.11942), 由 Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut 发布。'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 11959}, page_content='1. **[ALIGN](https://huggingface.co/docs/transformers/model_doc/align)** (来自 Google Research) 伴随论文 [Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision](https://arxiv.org/abs/2102.05918) 由 Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V. Le, Yunhsuan Sung, Zhen Li, Tom Duerig 发布。'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 12308}, page_content='1. **[AltCLIP](https://huggingface.co/docs/transformers/model_doc/altclip)** (来自 BAAI) 伴随论文 [AltCLIP: Altering the Language Encoder in CLIP for Extended Language Capabilities](https://arxiv.org/abs/2211.06679) 由 Chen, Zhongzhi and Liu, Guang and Zhang, Bo-Wen and Ye, Fulong and Yang, Qinghong and Wu, Ledell 发布。'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 12621}, page_content='1. **[Audio Spectrogram Transformer](https://huggingface.co/docs/transformers/model_doc/audio-spectrogram-transformer)** (来自 MIT) 伴随论文 [AST: Audio Spectrogram Transformer](https://arxiv.org/abs/2104.01778) 由 Yuan Gong, Yu-An Chung, James Glass 发布。'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 12869}, page_content='1. **[Autoformer](https://huggingface.co/docs/transformers/model_doc/autoformer)** (from Tsinghua University) released with the paper [Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting](https://arxiv.org/abs/2106.13008) by Haixu Wu, Jiehui Xu, Jianmin Wang, Mingsheng Long.\\n1. **[Bark](https://huggingface.co/docs/transformers/model_doc/bark)** (from Suno) released in the repository [suno-ai/bark](https://github.com/suno-ai/bark) by Suno AI team.'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 13362}, page_content='1. **[BART](https://huggingface.co/docs/transformers/model_doc/bart)** (来自 Facebook) 伴随论文 [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/pdf/1910.13461.pdf) 由 Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov and Luke Zettlemoyer 发布。'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 13737}, page_content='1. **[BARThez](https://huggingface.co/docs/transformers/model_doc/barthez)** (来自 École polytechnique) 伴随论文 [BARThez: a Skilled Pretrained French Sequence-to-Sequence Model](https://arxiv.org/abs/2010.12321) 由 Moussa Kamal Eddine, Antoine J.-P. Tixier, Michalis Vazirgiannis 发布。'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 14015}, page_content='1. **[BARTpho](https://huggingface.co/docs/transformers/model_doc/bartpho)** (来自 VinAI Research) 伴随论文 [BARTpho: Pre-trained Sequence-to-Sequence Models for Vietnamese](https://arxiv.org/abs/2109.09701) 由 Nguyen Luong Tran, Duong Minh Le and Dat Quoc Nguyen 发布。\\n1. **[BEiT](https://huggingface.co/docs/transformers/model_doc/beit)** (来自 Microsoft) 伴随论文 [BEiT: BERT Pre-Training of Image Transformers](https://arxiv.org/abs/2106.08254) 由 Hangbo Bao, Li Dong, Furu Wei 发布。'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 14485}, page_content='1. **[BERT](https://huggingface.co/docs/transformers/model_doc/bert)** (来自 Google) 伴随论文 [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) 由 Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova 发布。'),\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(docs_processed[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F35XsaQb4sBB",
        "outputId": "f42797d1-ef74-4757-de8b-125cfde28604"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "page_content='!--Copyright 2023 The HuggingFace Team. All rights reserved.\n",
            "\n",
            "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n",
            "the License. You may obtain a copy of the License at\n",
            "\n",
            "http://www.apache.org/licenses/LICENSE-2.0' metadata={'source': 'transformers', 'start_index': 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class RetrievalTool(Tool):\n",
        "    name = \"retriever\"\n",
        "    description = (\n",
        "        \"Uses semantic search to retrieve the parts of transformers documentation \"\n",
        "        \"that could be most relevant to answer your query.\"\n",
        "    )\n",
        "    inputs = {  # This must be a class attribute, not a type hint.\n",
        "        \"query\": {\n",
        "            \"type\": \"string\",\n",
        "            \"description\": (\n",
        "                \"The query to perform. This should be semantically close to your \"\n",
        "                \"target documents. Use the affirmative form rather than a question.\"\n",
        "            ),\n",
        "        }\n",
        "    }\n",
        "    output_type = \"string\"\n",
        "\n",
        "    def __init__(self, docs, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.retriever = BM25Retriever.from_documents(docs, k=10)\n",
        "\n",
        "    def forward(self, query: str) -> str:\n",
        "        assert isinstance(query, str), \"query must be a string\"\n",
        "        docs = self.retriever.invoke(query)\n",
        "        return \"\\nRetrieved documents:\\n\" + \"\".join(\n",
        "            [\n",
        "                f\"\\n\\n======Document {str(i)} =======\\n\" + doc.page_content\n",
        "                for i, doc in enumerate(docs)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "\n",
        "# Assuming `docs_processed` is a valid input (list of documents):\n",
        "retriever_tool = RetrievalTool(docs=docs_processed)"
      ],
      "metadata": {
        "id": "gJrJxjIZ5U0d"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever_tool.forward(\"How to download a model from Huggingface?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "id": "hIHOgwGO7BTk",
        "outputId": "cb6c2af7-ba32-4116-f5d7-90b59bb2369e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nRetrieved documents:\\n\\n\\n======Document 0 =======\\n2. Use the [`hf_hub_download`](https://huggingface.co/docs/hub/adding-a-library#download-files-from-the-hub) function to download a file to a specific path. For example, the following command downloads the `config.json` file from the [T0](https://huggingface.co/bigscience/T0_3B) model to your desired path:\\n\\n    ```py\\n    >>> from huggingface_hub import hf_hub_download\\n\\n    >>> hf_hub_download(repo_id=\"bigscience/T0_3B\", filename=\"config.json\", cache_dir=\"./your/path/bigscience_t0\")\\n    ```\\n\\n======Document 1 =======\\nWe demonstrate how to evaluate retrieval against DPR evaluation data. You can download respective files from links listed [here](https://github.com/facebookresearch/DPR/blob/master/data/download_data.py#L39-L45).\\n\\n======Document 2 =======\\n- How to fine-tune the XLSR model\\n- How to upload the model\\n- How to share your evaluation results & training/eval script\\n- What are the training/evaluation rules\\n\\ncan be found in the sections below. If something is still unclear, feel free to drop a message in the Slack channel.\\n\\n## How to fine tune XLSR Wav2Vec2\\n\\n======Document 3 =======\\nIn many cases you will want to download the full repository. Here is an example downloading all the files from a repo. You can even specify specific revisions!\\n\\n```\\nfrom huggingface_hub import snapshot_download\\nlocal_path = snapshot_download(\"flax-community/roberta-base-als\");\\n```\\n\\n======Document 4 =======\\n|[Expand and Fine Tune Sci-BERT](https://github.com/lordtt13/word-embeddings/blob/master/COVID-19%20Research%20Data/COVID-SciBERT.ipynb)| How to increase vocabulary of a pretrained SciBERT model from AllenAI on the CORD dataset and pipeline it. | [Tanmay Thakur](https://github.com/lordtt13) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1rqAR40goxbAfez1xvF3hBJphSCsvXmh8)|\\n\\n======Document 5 =======\\nFor more details about troubleshooting and getting help, take a look at [Chapter 8](https://huggingface.co/course/chapter8/1?fw=pt) of the Hugging Face course.\\n\\n\\n## Firewalled environments\\n\\nSome GPU instances on cloud and intranet setups are firewalled to external connections, resulting in a connection error. When your script attempts to download model weights or datasets, the download will hang and then timeout with the following message:\\n\\n======Document 6 =======\\n🤗 Transformers provides APIs and tools to easily download and train state-of-the-art pretrained models. Using pretrained models can reduce your compute costs, carbon footprint, and save you the time and resources required to train a model from scratch. These models support common tasks in different modalities, such as:\\n\\n======Document 7 =======\\n## How to use this project?\\n\\nTo fine-tune a transformer model with IGF on a language modeling task, use the following script:\\n\\n======Document 8 =======\\n| [Fine-tuning a pretrained model](https://github.com/huggingface/notebooks/blob/main/transformers_doc/en/training.ipynb)  | How to use the Trainer to fine-tune a pretrained model |[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/training.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker\\n\\n======Document 9 =======\\n| [Fine-tuning a pretrained model](https://github.com/huggingface/notebooks/blob/main/transformers_doc/en/training.ipynb)  | How to use the Trainer to fine-tune a pretrained model |[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/training.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from smolagents import HfApiModel, CodeAgent\n",
        "agent = CodeAgent(\n",
        "    tools=[retriever_tool],\n",
        "    model=HfApiModel(),\n",
        "    max_iterations=4,\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "NRgC9hY77ZLq"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent.run(\"How to download a model from huggingface?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6K0QUVUM7vFX",
        "outputId": "254ac50a-0393-433e-e57f-50eb98a857c5"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[38;2;212;183;2m╭─\u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╮\u001b[0m\n",
              "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
              "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mHow to download a model from huggingface?\u001b[0m                                                                       \u001b[38;2;212;183;2m│\u001b[0m\n",
              "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
              "\u001b[38;2;212;183;2m╰─\u001b[0m\u001b[38;2;212;183;2m HfApiModel - Qwen/Qwen2.5-Coder-32B-Instruct \u001b[0m\u001b[38;2;212;183;2m─────────────────────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╯\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">╭──────────────────────────────────────────────────── </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ────────────────────────────────────────────────────╮</span>\n",
              "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
              "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">How to download a model from huggingface?</span>                                                                       <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
              "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
              "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">╰─ HfApiModel - Qwen/Qwen2.5-Coder-32B-Instruct ──────────────────────────────────────────────────────────────────╯</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m0\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[3mOutput message of the LLM:\u001b[0m ────────────────────────────────────────────────────────────────────────────────────────\n",
              "\u001b[38;2;230;237;243;48;2;13;17;23mThought:\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mTo\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mdownload\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23ma\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mmodel\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mfrom\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mHugging\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mFace,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mI\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mneed\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mto\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23muse\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mthe\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mHugging\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mFace's\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;165;214;255;48;2;13;17;23m`transformers`\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mlibrary.\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mI'll\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mfirst\u001b[0m\n",
              "\u001b[38;2;230;237;243;48;2;13;17;23mretrieve\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mthe\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mrelevant\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mdocumentation\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mfrom\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mthe\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;165;214;255;48;2;13;17;23m`retriever`\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mtool\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mto\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23munderstand\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mthe\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mprocess.\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mThen,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mI'll\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mwrite\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mthe\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mcode\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\n",
              "\u001b[38;2;230;237;243;48;2;13;17;23mto\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mdownload\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23ma\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mmodel\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23musing\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mthat\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23minformation.\u001b[0m\u001b[48;2;13;17;23m                                                                        \u001b[0m\n",
              "\u001b[48;2;13;17;23m                                                                                                                   \u001b[0m\n",
              "\u001b[38;2;230;237;243;48;2;13;17;23mCode:\u001b[0m\u001b[48;2;13;17;23m                                                                                                              \u001b[0m\n",
              "\u001b[38;2;230;237;243;48;2;13;17;23m```py\u001b[0m\u001b[48;2;13;17;23m                                                                                                              \u001b[0m\n",
              "\u001b[38;2;230;237;243;48;2;13;17;23mdoc\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m=\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mretriever(query=\"how\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mto\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mdownload\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23ma\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mmodel\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mfrom\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mhuggingface\")\u001b[0m\u001b[48;2;13;17;23m                                                  \u001b[0m\n",
              "\u001b[38;2;230;237;243;48;2;13;17;23mprint(doc)\u001b[0m\u001b[48;2;13;17;23m                                                                                                         \u001b[0m\n",
              "\u001b[38;2;230;237;243;48;2;13;17;23m```<end_code>\u001b[0m\u001b[48;2;13;17;23m                                                                                                      \u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">Output message of the LLM:</span> ────────────────────────────────────────────────────────────────────────────────────────\n",
              "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">Thought: To download a model from Hugging Face, I need to use the Hugging Face's </span><span style=\"color: #a5d6ff; text-decoration-color: #a5d6ff; background-color: #0d1117\">`transformers`</span><span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\"> library. I'll first</span>\n",
              "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">retrieve the relevant documentation from the </span><span style=\"color: #a5d6ff; text-decoration-color: #a5d6ff; background-color: #0d1117\">`retriever`</span><span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\"> tool to understand the process. Then, I'll write the code </span>\n",
              "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">to download a model using that information.</span><span style=\"background-color: #0d1117\">                                                                        </span>\n",
              "<span style=\"background-color: #0d1117\">                                                                                                                   </span>\n",
              "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">Code:</span><span style=\"background-color: #0d1117\">                                                                                                              </span>\n",
              "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">```py</span><span style=\"background-color: #0d1117\">                                                                                                              </span>\n",
              "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">doc = retriever(query=\"how to download a model from huggingface\")</span><span style=\"background-color: #0d1117\">                                                  </span>\n",
              "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">print(doc)</span><span style=\"background-color: #0d1117\">                                                                                                         </span>\n",
              "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">```&lt;end_code&gt;</span><span style=\"background-color: #0d1117\">                                                                                                      </span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "╭─ \u001b[1mExecuting this code:\u001b[0m ──────────────────────────────────────────────────────────────────────────────────────────╮\n",
              "│ \u001b[1;38;2;227;227;221;48;2;39;40;34m  \u001b[0m\u001b[38;2;101;102;96;48;2;39;40;34m1 \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mdoc\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mretriever\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mquery\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mhow to download a model from huggingface\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                          \u001b[0m │\n",
              "│ \u001b[1;38;2;227;227;221;48;2;39;40;34m  \u001b[0m\u001b[38;2;101;102;96;48;2;39;40;34m2 \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mprint\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mdoc\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                                 \u001b[0m │\n",
              "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─ <span style=\"font-weight: bold\">Executing this code:</span> ──────────────────────────────────────────────────────────────────────────────────────────╮\n",
              "│ <span style=\"color: #e3e3dd; text-decoration-color: #e3e3dd; background-color: #272822; font-weight: bold\">  </span><span style=\"color: #656660; text-decoration-color: #656660; background-color: #272822\">1 </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">doc </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> retriever(query</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"how to download a model from huggingface\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">                                          </span> │\n",
              "│ <span style=\"color: #e3e3dd; text-decoration-color: #e3e3dd; background-color: #272822; font-weight: bold\">  </span><span style=\"color: #656660; text-decoration-color: #656660; background-color: #272822\">2 </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">print(doc)</span><span style=\"background-color: #272822\">                                                                                                 </span> │\n",
              "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mExecution logs:\u001b[0m\n",
              "\n",
              "Retrieved documents:\n",
              "\n",
              "\n",
              "======Document 0 =======\n",
              "We demonstrate how to evaluate retrieval against DPR evaluation data. You can download respective files from links \n",
              "listed [here](https://github.com/facebookresearch/DPR/blob/master/data/download_data.py#L39-L45).\n",
              "\n",
              "======Document 1 =======\n",
              "2. Use the [`hf_hub_download`](https://huggingface.co/docs/hub/adding-a-library#download-files-from-the-hub) \n",
              "function to download a file to a specific path. For example, the following command downloads the `config.json` file\n",
              "from the [T0](https://huggingface.co/bigscience/T0_3B) model to your desired path:\n",
              "\n",
              "    ```py\n",
              "    >>> from huggingface_hub import hf_hub_download\n",
              "\n",
              "    >>> hf_hub_download(repo_id=\"bigscience/T0_3B\", filename=\"config.json\", cache_dir=\"./your/path/bigscience_t0\")\n",
              "    ```\n",
              "\n",
              "======Document 2 =======\n",
              "As an alternative to the [`Graphcore/vqa` dataset](https://huggingface.co/datasets/Graphcore/vqa), you can download\n",
              "the \n",
              "same data manually from the official [VQA dataset page](https://visualqa.org/download.html). If you prefer to \n",
              "follow the \n",
              "tutorial with your custom data, check out how to [Create an image \n",
              "dataset](https://huggingface.co/docs/datasets/image_dataset#loading-script)\n",
              "guide in the 🤗 Datasets documentation.\n",
              "\n",
              "======Document 3 =======\n",
              "`run_summarization_flax.py` is a lightweight example of how to download and preprocess a dataset from the 🤗 \n",
              "Datasets library or use your own files (jsonlines or csv), then fine-tune one of the architectures above on it.\n",
              "\n",
              "For custom datasets in `jsonlines` format please see: \n",
              "https://huggingface.co/docs/datasets/loading_datasets#json-files and you also will find examples of these below.\n",
              "\n",
              "### Train the model\n",
              "Next we can run the example script to train the model:\n",
              "\n",
              "======Document 4 =======\n",
              "`run_translation.py` is a lightweight examples of how to download and preprocess a dataset from the [🤗 \n",
              "Datasets](https://github.com/huggingface/datasets) library or use your own files (jsonlines or csv), then fine-tune\n",
              "one of the architectures above on it.\n",
              "\n",
              "For custom datasets in `jsonlines` format please see: \n",
              "https://huggingface.co/docs/datasets/loading_datasets#json-files\n",
              "and you also will find examples of these below.\n",
              "\n",
              "\n",
              "## With Trainer\n",
              "\n",
              "======Document 5 =======\n",
              "`run_summarization.py` is a lightweight example of how to download and preprocess a dataset from the [🤗 \n",
              "Datasets](https://github.com/huggingface/datasets) library or use your own files (jsonlines or csv), then fine-tune\n",
              "one of the architectures above on it.\n",
              "\n",
              "For custom datasets in `jsonlines` format please see: \n",
              "https://huggingface.co/docs/datasets/loading_datasets#json-files\n",
              "and you also will find examples of these below.\n",
              "\n",
              "## With Trainer\n",
              "\n",
              "======Document 6 =======\n",
              "JAX/Flax allows you to trace pure functions and compile them into efficient, fused accelerator code on both GPU and\n",
              "TPU.\n",
              "Models written in JAX/Flax are **immutable** and updated in a purely functional\n",
              "way which enables simple and efficient model parallelism.\n",
              "\n",
              "`run_wav2vec2_pretrain_flax.py` is a lightweight example of how to download and preprocess a dataset from the 🤗 \n",
              "Datasets library or use your own files (jsonlines or csv), then pretrain the wav2vec2 architectures above on it.\n",
              "\n",
              "======Document 7 =======\n",
              "* Fine-tune a pretrained model with 🤗 Transformers [`Trainer`].\n",
              "* Fine-tune a pretrained model in TensorFlow with Keras.\n",
              "* Fine-tune a pretrained model in native PyTorch.\n",
              "\n",
              "<a id='data-processing'></a>\n",
              "\n",
              "## Prepare a dataset\n",
              "\n",
              "<Youtube id=\"_BZearw7f0w\"/>\n",
              "\n",
              "Before you can fine-tune a pretrained model, download a dataset and prepare it for training. The previous tutorial \n",
              "showed you how to process data for training, and now you get an opportunity to put those skills to the test!\n",
              "\n",
              "======Document 8 =======\n",
              "JAX/Flax allows you to trace pure functions and compile them into efficient, fused accelerator code on both GPU and\n",
              "TPU.\n",
              "Models written in JAX/Flax are **immutable** and updated in a purely functional\n",
              "way which enables simple and efficient model parallelism.\n",
              "\n",
              "`run_image_captioning_flax.py` is a lightweight example of how to download and preprocess a dataset from the 🤗 \n",
              "Datasets\n",
              "library or use your own files (jsonlines or csv), then fine-tune one of the architectures above on it.\n",
              "\n",
              "======Document 9 =======\n",
              "In many cases you will want to download the full repository. Here is an example downloading all the files from a \n",
              "repo. You can even specify specific revisions!\n",
              "\n",
              "```\n",
              "from huggingface_hub import snapshot_download\n",
              "local_path = snapshot_download(\"flax-community/roberta-base-als\");\n",
              "```\n",
              "\n",
              "Out: None\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Execution logs:</span>\n",
              "\n",
              "Retrieved documents:\n",
              "\n",
              "\n",
              "======Document 0 =======\n",
              "We demonstrate how to evaluate retrieval against DPR evaluation data. You can download respective files from links \n",
              "listed [here](https://github.com/facebookresearch/DPR/blob/master/data/download_data.py#L39-L45).\n",
              "\n",
              "======Document 1 =======\n",
              "2. Use the [`hf_hub_download`](https://huggingface.co/docs/hub/adding-a-library#download-files-from-the-hub) \n",
              "function to download a file to a specific path. For example, the following command downloads the `config.json` file\n",
              "from the [T0](https://huggingface.co/bigscience/T0_3B) model to your desired path:\n",
              "\n",
              "    ```py\n",
              "    &gt;&gt;&gt; from huggingface_hub import hf_hub_download\n",
              "\n",
              "    &gt;&gt;&gt; hf_hub_download(repo_id=\"bigscience/T0_3B\", filename=\"config.json\", cache_dir=\"./your/path/bigscience_t0\")\n",
              "    ```\n",
              "\n",
              "======Document 2 =======\n",
              "As an alternative to the [`Graphcore/vqa` dataset](https://huggingface.co/datasets/Graphcore/vqa), you can download\n",
              "the \n",
              "same data manually from the official [VQA dataset page](https://visualqa.org/download.html). If you prefer to \n",
              "follow the \n",
              "tutorial with your custom data, check out how to [Create an image \n",
              "dataset](https://huggingface.co/docs/datasets/image_dataset#loading-script)\n",
              "guide in the 🤗 Datasets documentation.\n",
              "\n",
              "======Document 3 =======\n",
              "`run_summarization_flax.py` is a lightweight example of how to download and preprocess a dataset from the 🤗 \n",
              "Datasets library or use your own files (jsonlines or csv), then fine-tune one of the architectures above on it.\n",
              "\n",
              "For custom datasets in `jsonlines` format please see: \n",
              "https://huggingface.co/docs/datasets/loading_datasets#json-files and you also will find examples of these below.\n",
              "\n",
              "### Train the model\n",
              "Next we can run the example script to train the model:\n",
              "\n",
              "======Document 4 =======\n",
              "`run_translation.py` is a lightweight examples of how to download and preprocess a dataset from the [🤗 \n",
              "Datasets](https://github.com/huggingface/datasets) library or use your own files (jsonlines or csv), then fine-tune\n",
              "one of the architectures above on it.\n",
              "\n",
              "For custom datasets in `jsonlines` format please see: \n",
              "https://huggingface.co/docs/datasets/loading_datasets#json-files\n",
              "and you also will find examples of these below.\n",
              "\n",
              "\n",
              "## With Trainer\n",
              "\n",
              "======Document 5 =======\n",
              "`run_summarization.py` is a lightweight example of how to download and preprocess a dataset from the [🤗 \n",
              "Datasets](https://github.com/huggingface/datasets) library or use your own files (jsonlines or csv), then fine-tune\n",
              "one of the architectures above on it.\n",
              "\n",
              "For custom datasets in `jsonlines` format please see: \n",
              "https://huggingface.co/docs/datasets/loading_datasets#json-files\n",
              "and you also will find examples of these below.\n",
              "\n",
              "## With Trainer\n",
              "\n",
              "======Document 6 =======\n",
              "JAX/Flax allows you to trace pure functions and compile them into efficient, fused accelerator code on both GPU and\n",
              "TPU.\n",
              "Models written in JAX/Flax are **immutable** and updated in a purely functional\n",
              "way which enables simple and efficient model parallelism.\n",
              "\n",
              "`run_wav2vec2_pretrain_flax.py` is a lightweight example of how to download and preprocess a dataset from the 🤗 \n",
              "Datasets library or use your own files (jsonlines or csv), then pretrain the wav2vec2 architectures above on it.\n",
              "\n",
              "======Document 7 =======\n",
              "* Fine-tune a pretrained model with 🤗 Transformers [`Trainer`].\n",
              "* Fine-tune a pretrained model in TensorFlow with Keras.\n",
              "* Fine-tune a pretrained model in native PyTorch.\n",
              "\n",
              "&lt;a id='data-processing'&gt;&lt;/a&gt;\n",
              "\n",
              "## Prepare a dataset\n",
              "\n",
              "&lt;Youtube id=\"_BZearw7f0w\"/&gt;\n",
              "\n",
              "Before you can fine-tune a pretrained model, download a dataset and prepare it for training. The previous tutorial \n",
              "showed you how to process data for training, and now you get an opportunity to put those skills to the test!\n",
              "\n",
              "======Document 8 =======\n",
              "JAX/Flax allows you to trace pure functions and compile them into efficient, fused accelerator code on both GPU and\n",
              "TPU.\n",
              "Models written in JAX/Flax are **immutable** and updated in a purely functional\n",
              "way which enables simple and efficient model parallelism.\n",
              "\n",
              "`run_image_captioning_flax.py` is a lightweight example of how to download and preprocess a dataset from the 🤗 \n",
              "Datasets\n",
              "library or use your own files (jsonlines or csv), then fine-tune one of the architectures above on it.\n",
              "\n",
              "======Document 9 =======\n",
              "In many cases you will want to download the full repository. Here is an example downloading all the files from a \n",
              "repo. You can even specify specific revisions!\n",
              "\n",
              "```\n",
              "from huggingface_hub import snapshot_download\n",
              "local_path = snapshot_download(\"flax-community/roberta-base-als\");\n",
              "```\n",
              "\n",
              "Out: None\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2m[Step 0: Duration 2.96 seconds| Input tokens: 2,061 | Output tokens: 91]\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 0: Duration 2.96 seconds| Input tokens: 2,061 | Output tokens: 91]</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[3mOutput message of the LLM:\u001b[0m ────────────────────────────────────────────────────────────────────────────────────────\n",
              "\u001b[38;2;230;237;243;48;2;13;17;23mThought:\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mFrom\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mthe\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mretrieved\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mdocuments,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mit\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mseems\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mthat\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mthe\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mdocumentation\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mfor\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mdownloading\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23ma\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mmodel\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mfrom\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mHugging\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mFace\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mis\u001b[0m\n",
              "\u001b[38;2;230;237;243;48;2;13;17;23mspread\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23macross\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mseveral\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mentries.\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mDocument\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m1\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mprovides\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mthe\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;165;214;255;48;2;13;17;23m`hf_hub_download`\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mfunction,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mwhich\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mwe\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mcan\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23muse\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mto\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mdownload\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23ma\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[48;2;13;17;23m \u001b[0m\n",
              "\u001b[38;2;230;237;243;48;2;13;17;23mmodel.\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mDocument\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m9\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mprovides\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23man\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mexample\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mof\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;165;214;255;48;2;13;17;23m`snapshot_download`\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mto\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mdownload\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mthe\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mentire\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mrepository.\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mSince\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mthe\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mtask\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mis\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[48;2;13;17;23m \u001b[0m\n",
              "\u001b[38;2;230;237;243;48;2;13;17;23mto\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mdownload\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23ma\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mmodel,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mI'll\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23muse\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mthe\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;165;214;255;48;2;13;17;23m`snapshot_download`\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mfunction\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mto\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mensure\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mwe\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mdownload\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mthe\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mentire\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mmodel\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mdirectory.\u001b[0m\u001b[48;2;13;17;23m   \u001b[0m\n",
              "\u001b[48;2;13;17;23m                                                                                                                   \u001b[0m\n",
              "\u001b[38;2;230;237;243;48;2;13;17;23mCode:\u001b[0m\u001b[48;2;13;17;23m                                                                                                              \u001b[0m\n",
              "\u001b[38;2;230;237;243;48;2;13;17;23m```py\u001b[0m\u001b[48;2;13;17;23m                                                                                                              \u001b[0m\n",
              "\u001b[38;2;230;237;243;48;2;13;17;23mfrom\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mhuggingface_hub\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mimport\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23msnapshot_download\u001b[0m\u001b[48;2;13;17;23m                                                                      \u001b[0m\n",
              "\u001b[38;2;230;237;243;48;2;13;17;23mlocal_path\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m=\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23msnapshot_download(repo_id=\"bigscience/T0_3B\",\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mcache_dir=\"./models\")\u001b[0m\u001b[48;2;13;17;23m                                   \u001b[0m\n",
              "\u001b[38;2;230;237;243;48;2;13;17;23mprint(local_path)\u001b[0m\u001b[48;2;13;17;23m                                                                                                  \u001b[0m\n",
              "\u001b[38;2;230;237;243;48;2;13;17;23m```<end_code>\u001b[0m\u001b[48;2;13;17;23m                                                                                                      \u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">Output message of the LLM:</span> ────────────────────────────────────────────────────────────────────────────────────────\n",
              "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">Thought: From the retrieved documents, it seems that the documentation for downloading a model from Hugging Face is</span>\n",
              "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">spread across several entries. Document 1 provides the </span><span style=\"color: #a5d6ff; text-decoration-color: #a5d6ff; background-color: #0d1117\">`hf_hub_download`</span><span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\"> function, which we can use to download a </span><span style=\"background-color: #0d1117\"> </span>\n",
              "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">model. Document 9 provides an example of </span><span style=\"color: #a5d6ff; text-decoration-color: #a5d6ff; background-color: #0d1117\">`snapshot_download`</span><span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\"> to download the entire repository. Since the task is </span><span style=\"background-color: #0d1117\"> </span>\n",
              "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">to download a model, I'll use the </span><span style=\"color: #a5d6ff; text-decoration-color: #a5d6ff; background-color: #0d1117\">`snapshot_download`</span><span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\"> function to ensure we download the entire model directory.</span><span style=\"background-color: #0d1117\">   </span>\n",
              "<span style=\"background-color: #0d1117\">                                                                                                                   </span>\n",
              "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">Code:</span><span style=\"background-color: #0d1117\">                                                                                                              </span>\n",
              "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">```py</span><span style=\"background-color: #0d1117\">                                                                                                              </span>\n",
              "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">from huggingface_hub import snapshot_download</span><span style=\"background-color: #0d1117\">                                                                      </span>\n",
              "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">local_path = snapshot_download(repo_id=\"bigscience/T0_3B\", cache_dir=\"./models\")</span><span style=\"background-color: #0d1117\">                                   </span>\n",
              "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">print(local_path)</span><span style=\"background-color: #0d1117\">                                                                                                  </span>\n",
              "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">```&lt;end_code&gt;</span><span style=\"background-color: #0d1117\">                                                                                                      </span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "╭─ \u001b[1mExecuting this code:\u001b[0m ──────────────────────────────────────────────────────────────────────────────────────────╮\n",
              "│ \u001b[1;38;2;227;227;221;48;2;39;40;34m  \u001b[0m\u001b[38;2;101;102;96;48;2;39;40;34m1 \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34mfrom\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mhuggingface_hub\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34mimport\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34msnapshot_download\u001b[0m\u001b[48;2;39;40;34m                                                              \u001b[0m │\n",
              "│ \u001b[1;38;2;227;227;221;48;2;39;40;34m  \u001b[0m\u001b[38;2;101;102;96;48;2;39;40;34m2 \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mlocal_path\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34msnapshot_download\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mrepo_id\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mbigscience/T0_3B\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mcache_dir\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m./models\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                           \u001b[0m │\n",
              "│ \u001b[1;38;2;227;227;221;48;2;39;40;34m  \u001b[0m\u001b[38;2;101;102;96;48;2;39;40;34m3 \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mprint\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mlocal_path\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                          \u001b[0m │\n",
              "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─ <span style=\"font-weight: bold\">Executing this code:</span> ──────────────────────────────────────────────────────────────────────────────────────────╮\n",
              "│ <span style=\"color: #e3e3dd; text-decoration-color: #e3e3dd; background-color: #272822; font-weight: bold\">  </span><span style=\"color: #656660; text-decoration-color: #656660; background-color: #272822\">1 </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">from</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> huggingface_hub </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">import</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> snapshot_download</span><span style=\"background-color: #272822\">                                                              </span> │\n",
              "│ <span style=\"color: #e3e3dd; text-decoration-color: #e3e3dd; background-color: #272822; font-weight: bold\">  </span><span style=\"color: #656660; text-decoration-color: #656660; background-color: #272822\">2 </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">local_path </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> snapshot_download(repo_id</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"bigscience/T0_3B\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">, cache_dir</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"./models\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">                           </span> │\n",
              "│ <span style=\"color: #e3e3dd; text-decoration-color: #e3e3dd; background-color: #272822; font-weight: bold\">  </span><span style=\"color: #656660; text-decoration-color: #656660; background-color: #272822\">3 </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">print(local_path)</span><span style=\"background-color: #272822\">                                                                                          </span> │\n",
              "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;31mCode execution failed: Code execution failed at line \u001b[0m\u001b[1;31m'from huggingface_hub import snapshot_download'\u001b[0m\u001b[1;31m because of the\u001b[0m\n",
              "\u001b[1;31mfollowing error:\u001b[0m\n",
              "\u001b[1;31mImport from huggingface_hub is not allowed.\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Code execution failed: Code execution failed at line </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">'from huggingface_hub import snapshot_download'</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\"> because of the</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">following error:</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Import from huggingface_hub is not allowed.</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2m[Step 1: Duration 3.45 seconds| Input tokens: 5,335 | Output tokens: 227]\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 3.45 seconds| Input tokens: 5,335 | Output tokens: 227]</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m2\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[3mOutput message of the LLM:\u001b[0m ────────────────────────────────────────────────────────────────────────────────────────\n",
              "\u001b[38;2;230;237;243;48;2;13;17;23mThought:\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mSince\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mimporting\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;165;214;255;48;2;13;17;23m`huggingface_hub`\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mdirectly\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mis\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mnot\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mallowed,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mI\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mwill\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23muse\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mthe\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;165;214;255;48;2;13;17;23m`transformers`\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mlibrary,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mwhich\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mis\u001b[0m\n",
              "\u001b[38;2;230;237;243;48;2;13;17;23mcommonly\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mused\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mto\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mdownload\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mmodels\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mfrom\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mHugging\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mFace.\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mI'll\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23muse\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mthe\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;165;214;255;48;2;13;17;23m`AutoModel`\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mand\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;165;214;255;48;2;13;17;23m`AutoTokenizer`\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mclasses\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mto\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[48;2;13;17;23m       \u001b[0m\n",
              "\u001b[38;2;230;237;243;48;2;13;17;23mdownload\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23ma\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mmodel\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mand\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mits\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mtokenizer.\u001b[0m\u001b[48;2;13;17;23m                                                                                \u001b[0m\n",
              "\u001b[48;2;13;17;23m                                                                                                                   \u001b[0m\n",
              "\u001b[38;2;230;237;243;48;2;13;17;23mCode:\u001b[0m\u001b[48;2;13;17;23m                                                                                                              \u001b[0m\n",
              "\u001b[38;2;230;237;243;48;2;13;17;23m```py\u001b[0m\u001b[48;2;13;17;23m                                                                                                              \u001b[0m\n",
              "\u001b[38;2;230;237;243;48;2;13;17;23mfrom\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mtransformers\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mimport\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mAutoModel,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mAutoTokenizer\u001b[0m\u001b[48;2;13;17;23m                                                                  \u001b[0m\n",
              "\u001b[48;2;13;17;23m                                                                                                                   \u001b[0m\n",
              "\u001b[38;2;230;237;243;48;2;13;17;23mmodel_id\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m=\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m\"bigscience/T0_3B\"\u001b[0m\u001b[48;2;13;17;23m                                                                                      \u001b[0m\n",
              "\u001b[38;2;230;237;243;48;2;13;17;23mtokenizer\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m=\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mAutoTokenizer.from_pretrained(model_id)\u001b[0m\u001b[48;2;13;17;23m                                                                \u001b[0m\n",
              "\u001b[38;2;230;237;243;48;2;13;17;23mmodel\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m=\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mAutoModel.from_pretrained(model_id)\u001b[0m\u001b[48;2;13;17;23m                                                                        \u001b[0m\n",
              "\u001b[48;2;13;17;23m                                                                                                                   \u001b[0m\n",
              "\u001b[38;2;230;237;243;48;2;13;17;23mprint(f\"Model\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mand\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mtokenizer\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mdownloaded\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mand\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23msaved\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mto\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mthe\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mcache\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mdirectory:\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m{model.cache_dir}\")\u001b[0m\u001b[48;2;13;17;23m                       \u001b[0m\n",
              "\u001b[38;2;230;237;243;48;2;13;17;23m```<end_code>\u001b[0m\u001b[48;2;13;17;23m                                                                                                      \u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">Output message of the LLM:</span> ────────────────────────────────────────────────────────────────────────────────────────\n",
              "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">Thought: Since importing </span><span style=\"color: #a5d6ff; text-decoration-color: #a5d6ff; background-color: #0d1117\">`huggingface_hub`</span><span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\"> directly is not allowed, I will use the </span><span style=\"color: #a5d6ff; text-decoration-color: #a5d6ff; background-color: #0d1117\">`transformers`</span><span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\"> library, which is</span>\n",
              "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">commonly used to download models from Hugging Face. I'll use the </span><span style=\"color: #a5d6ff; text-decoration-color: #a5d6ff; background-color: #0d1117\">`AutoModel`</span><span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\"> and </span><span style=\"color: #a5d6ff; text-decoration-color: #a5d6ff; background-color: #0d1117\">`AutoTokenizer`</span><span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\"> classes to </span><span style=\"background-color: #0d1117\">       </span>\n",
              "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">download a model and its tokenizer.</span><span style=\"background-color: #0d1117\">                                                                                </span>\n",
              "<span style=\"background-color: #0d1117\">                                                                                                                   </span>\n",
              "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">Code:</span><span style=\"background-color: #0d1117\">                                                                                                              </span>\n",
              "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">```py</span><span style=\"background-color: #0d1117\">                                                                                                              </span>\n",
              "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">from transformers import AutoModel, AutoTokenizer</span><span style=\"background-color: #0d1117\">                                                                  </span>\n",
              "<span style=\"background-color: #0d1117\">                                                                                                                   </span>\n",
              "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">model_id = \"bigscience/T0_3B\"</span><span style=\"background-color: #0d1117\">                                                                                      </span>\n",
              "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">tokenizer = AutoTokenizer.from_pretrained(model_id)</span><span style=\"background-color: #0d1117\">                                                                </span>\n",
              "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">model = AutoModel.from_pretrained(model_id)</span><span style=\"background-color: #0d1117\">                                                                        </span>\n",
              "<span style=\"background-color: #0d1117\">                                                                                                                   </span>\n",
              "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">print(f\"Model and tokenizer downloaded and saved to the cache directory: {model.cache_dir}\")</span><span style=\"background-color: #0d1117\">                       </span>\n",
              "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">```&lt;end_code&gt;</span><span style=\"background-color: #0d1117\">                                                                                                      </span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "╭─ \u001b[1mExecuting this code:\u001b[0m ──────────────────────────────────────────────────────────────────────────────────────────╮\n",
              "│ \u001b[1;38;2;227;227;221;48;2;39;40;34m  \u001b[0m\u001b[38;2;101;102;96;48;2;39;40;34m1 \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34mfrom\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mtransformers\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34mimport\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mAutoModel\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mAutoTokenizer\u001b[0m\u001b[48;2;39;40;34m                                                          \u001b[0m │\n",
              "│ \u001b[1;38;2;227;227;221;48;2;39;40;34m  \u001b[0m\u001b[38;2;101;102;96;48;2;39;40;34m2 \u001b[0m\u001b[48;2;39;40;34m                                                                                                           \u001b[0m │\n",
              "│ \u001b[1;38;2;227;227;221;48;2;39;40;34m  \u001b[0m\u001b[38;2;101;102;96;48;2;39;40;34m3 \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mmodel_id\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mbigscience/T0_3B\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[48;2;39;40;34m                                                                              \u001b[0m │\n",
              "│ \u001b[1;38;2;227;227;221;48;2;39;40;34m  \u001b[0m\u001b[38;2;101;102;96;48;2;39;40;34m4 \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mtokenizer\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mAutoTokenizer\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mfrom_pretrained\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mmodel_id\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                        \u001b[0m │\n",
              "│ \u001b[1;38;2;227;227;221;48;2;39;40;34m  \u001b[0m\u001b[38;2;101;102;96;48;2;39;40;34m5 \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mmodel\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mAutoModel\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mfrom_pretrained\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mmodel_id\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                \u001b[0m │\n",
              "│ \u001b[1;38;2;227;227;221;48;2;39;40;34m  \u001b[0m\u001b[38;2;101;102;96;48;2;39;40;34m6 \u001b[0m\u001b[48;2;39;40;34m                                                                                                           \u001b[0m │\n",
              "│ \u001b[1;38;2;227;227;221;48;2;39;40;34m  \u001b[0m\u001b[38;2;101;102;96;48;2;39;40;34m7 \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mprint\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mf\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mModel and tokenizer downloaded and saved to the cache directory: \u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m{\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mmodel\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mcache_dir\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m}\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m               \u001b[0m │\n",
              "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─ <span style=\"font-weight: bold\">Executing this code:</span> ──────────────────────────────────────────────────────────────────────────────────────────╮\n",
              "│ <span style=\"color: #e3e3dd; text-decoration-color: #e3e3dd; background-color: #272822; font-weight: bold\">  </span><span style=\"color: #656660; text-decoration-color: #656660; background-color: #272822\">1 </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">from</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> transformers </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">import</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> AutoModel, AutoTokenizer</span><span style=\"background-color: #272822\">                                                          </span> │\n",
              "│ <span style=\"color: #e3e3dd; text-decoration-color: #e3e3dd; background-color: #272822; font-weight: bold\">  </span><span style=\"color: #656660; text-decoration-color: #656660; background-color: #272822\">2 </span><span style=\"background-color: #272822\">                                                                                                           </span> │\n",
              "│ <span style=\"color: #e3e3dd; text-decoration-color: #e3e3dd; background-color: #272822; font-weight: bold\">  </span><span style=\"color: #656660; text-decoration-color: #656660; background-color: #272822\">3 </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">model_id </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> </span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"bigscience/T0_3B\"</span><span style=\"background-color: #272822\">                                                                              </span> │\n",
              "│ <span style=\"color: #e3e3dd; text-decoration-color: #e3e3dd; background-color: #272822; font-weight: bold\">  </span><span style=\"color: #656660; text-decoration-color: #656660; background-color: #272822\">4 </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">tokenizer </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> AutoTokenizer</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">from_pretrained(model_id)</span><span style=\"background-color: #272822\">                                                        </span> │\n",
              "│ <span style=\"color: #e3e3dd; text-decoration-color: #e3e3dd; background-color: #272822; font-weight: bold\">  </span><span style=\"color: #656660; text-decoration-color: #656660; background-color: #272822\">5 </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">model </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> AutoModel</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">from_pretrained(model_id)</span><span style=\"background-color: #272822\">                                                                </span> │\n",
              "│ <span style=\"color: #e3e3dd; text-decoration-color: #e3e3dd; background-color: #272822; font-weight: bold\">  </span><span style=\"color: #656660; text-decoration-color: #656660; background-color: #272822\">6 </span><span style=\"background-color: #272822\">                                                                                                           </span> │\n",
              "│ <span style=\"color: #e3e3dd; text-decoration-color: #e3e3dd; background-color: #272822; font-weight: bold\">  </span><span style=\"color: #656660; text-decoration-color: #656660; background-color: #272822\">7 </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">print(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">f\"Model and tokenizer downloaded and saved to the cache directory: {</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">model</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">cache_dir</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">}\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">               </span> │\n",
              "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;31mCode execution failed: Code execution failed at line \u001b[0m\u001b[1;31m'from transformers import AutoModel, AutoTokenizer'\u001b[0m\u001b[1;31m because of\u001b[0m\n",
              "\u001b[1;31mthe following error:\u001b[0m\n",
              "\u001b[1;31mImport from transformers is not allowed.\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Code execution failed: Code execution failed at line </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">'from transformers import AutoModel, AutoTokenizer'</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\"> because of</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">the following error:</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Import from transformers is not allowed.</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2m[Step 2: Duration 3.83 seconds| Input tokens: 8,894 | Output tokens: 357]\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 2: Duration 3.83 seconds| Input tokens: 8,894 | Output tokens: 357]</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m3\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[3mOutput message of the LLM:\u001b[0m ────────────────────────────────────────────────────────────────────────────────────────\n",
              "\u001b[38;2;230;237;243;48;2;13;17;23mThought:\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mSince\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mwe\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mcannot\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23muse\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mthe\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;165;214;255;48;2;13;17;23m`transformers`\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mlibrary\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mdirectly,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mI\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mwill\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23muse\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mthe\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;165;214;255;48;2;13;17;23m`retriever`\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mtool\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23magain\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mto\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mfind\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23many\u001b[0m\n",
              "\u001b[38;2;230;237;243;48;2;13;17;23mother\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mmethods\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mor\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23minstructions\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mfor\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mdownloading\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mmodels\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mfrom\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mHugging\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mFace\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mthat\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mdo\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mnot\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mrely\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mon\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mdirect\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mlibrary\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mimports.\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\n",
              "\u001b[38;2;230;237;243;48;2;13;17;23mLet's\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mlook\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mfor\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23malternative\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mmethods\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mdescribed\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23min\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mthe\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mdocumentation.\u001b[0m\u001b[48;2;13;17;23m                                                 \u001b[0m\n",
              "\u001b[48;2;13;17;23m                                                                                                                   \u001b[0m\n",
              "\u001b[38;2;230;237;243;48;2;13;17;23mCode:\u001b[0m\u001b[48;2;13;17;23m                                                                                                              \u001b[0m\n",
              "\u001b[38;2;230;237;243;48;2;13;17;23m```py\u001b[0m\u001b[48;2;13;17;23m                                                                                                              \u001b[0m\n",
              "\u001b[38;2;230;237;243;48;2;13;17;23mdoc\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m=\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mretriever(query=\"download\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mmodel\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mhuggingface\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mwithout\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mimport\")\u001b[0m\u001b[48;2;13;17;23m                                                 \u001b[0m\n",
              "\u001b[38;2;230;237;243;48;2;13;17;23mprint(doc)\u001b[0m\u001b[48;2;13;17;23m                                                                                                         \u001b[0m\n",
              "\u001b[38;2;230;237;243;48;2;13;17;23m```<end_code>\u001b[0m\u001b[48;2;13;17;23m                                                                                                      \u001b[0m\n",
              "\u001b[38;2;230;237;243;48;2;13;17;23matically:\u001b[0m\u001b[48;2;13;17;23m                                                                                                          \u001b[0m\n",
              "\u001b[38;2;230;237;243;48;2;13;17;23m[{'id':\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m'call_5',\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m'type':\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m'function',\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m'function':\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m{'name':\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m'retriever',\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m'arguments':\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m'query=\"download\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mmodel\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[48;2;13;17;23m       \u001b[0m\n",
              "\u001b[38;2;230;237;243;48;2;13;17;23mhuggingface\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mwithout\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mimport\"'}}]\u001b[0m\u001b[48;2;13;17;23m                                                                                    \u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">Output message of the LLM:</span> ────────────────────────────────────────────────────────────────────────────────────────\n",
              "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">Thought: Since we cannot use the </span><span style=\"color: #a5d6ff; text-decoration-color: #a5d6ff; background-color: #0d1117\">`transformers`</span><span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\"> library directly, I will use the </span><span style=\"color: #a5d6ff; text-decoration-color: #a5d6ff; background-color: #0d1117\">`retriever`</span><span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\"> tool again to find any</span>\n",
              "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">other methods or instructions for downloading models from Hugging Face that do not rely on direct library imports. </span>\n",
              "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">Let's look for alternative methods described in the documentation.</span><span style=\"background-color: #0d1117\">                                                 </span>\n",
              "<span style=\"background-color: #0d1117\">                                                                                                                   </span>\n",
              "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">Code:</span><span style=\"background-color: #0d1117\">                                                                                                              </span>\n",
              "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">```py</span><span style=\"background-color: #0d1117\">                                                                                                              </span>\n",
              "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">doc = retriever(query=\"download model huggingface without import\")</span><span style=\"background-color: #0d1117\">                                                 </span>\n",
              "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">print(doc)</span><span style=\"background-color: #0d1117\">                                                                                                         </span>\n",
              "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">```&lt;end_code&gt;</span><span style=\"background-color: #0d1117\">                                                                                                      </span>\n",
              "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">atically:</span><span style=\"background-color: #0d1117\">                                                                                                          </span>\n",
              "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">[{'id': 'call_5', 'type': 'function', 'function': {'name': 'retriever', 'arguments': 'query=\"download model </span><span style=\"background-color: #0d1117\">       </span>\n",
              "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">huggingface without import\"'}}]</span><span style=\"background-color: #0d1117\">                                                                                    </span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "╭─ \u001b[1mExecuting this code:\u001b[0m ──────────────────────────────────────────────────────────────────────────────────────────╮\n",
              "│ \u001b[1;38;2;227;227;221;48;2;39;40;34m  \u001b[0m\u001b[38;2;101;102;96;48;2;39;40;34m1 \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mdoc\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mretriever\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mquery\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mdownload model huggingface without import\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                         \u001b[0m │\n",
              "│ \u001b[1;38;2;227;227;221;48;2;39;40;34m  \u001b[0m\u001b[38;2;101;102;96;48;2;39;40;34m2 \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mprint\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mdoc\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                                 \u001b[0m │\n",
              "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─ <span style=\"font-weight: bold\">Executing this code:</span> ──────────────────────────────────────────────────────────────────────────────────────────╮\n",
              "│ <span style=\"color: #e3e3dd; text-decoration-color: #e3e3dd; background-color: #272822; font-weight: bold\">  </span><span style=\"color: #656660; text-decoration-color: #656660; background-color: #272822\">1 </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">doc </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> retriever(query</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"download model huggingface without import\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">                                         </span> │\n",
              "│ <span style=\"color: #e3e3dd; text-decoration-color: #e3e3dd; background-color: #272822; font-weight: bold\">  </span><span style=\"color: #656660; text-decoration-color: #656660; background-color: #272822\">2 </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">print(doc)</span><span style=\"background-color: #272822\">                                                                                                 </span> │\n",
              "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mExecution logs:\u001b[0m\n",
              "\n",
              "Retrieved documents:\n",
              "\n",
              "\n",
              "======Document 0 =======\n",
              "All of the following examples make use of [dataset \n",
              "streaming](https://huggingface.co/docs/datasets/master/dataset_streaming), therefore allowing to train models on \n",
              "massive datasets\\\n",
              "without ever having to download the full dataset.\n",
              "\n",
              "## Masked language modeling\n",
              "\n",
              "======Document 1 =======\n",
              "```py\n",
              ">>> from transformers import AutoModelForSequenceClassification\n",
              ">>> import torch\n",
              "\n",
              ">>> model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
              ">>> model.config.pad_token_id\n",
              "0\n",
              "```\n",
              "\n",
              "The following example shows the output without masking the padding tokens:\n",
              "\n",
              "======Document 2 =======\n",
              "<Tip>\n",
              "\n",
              "Add [🤗 Datasets](https://huggingface.co/docs/datasets/) to your offline training workflow with the environment \n",
              "variable `HF_DATASETS_OFFLINE=1`.\n",
              "\n",
              "</Tip>\n",
              "\n",
              "```bash\n",
              "HF_DATASETS_OFFLINE=1 TRANSFORMERS_OFFLINE=1 \\\n",
              "python examples/pytorch/translation/run_translation.py --model_name_or_path t5-small --dataset_name wmt16 \n",
              "--dataset_config ro-en ...\n",
              "```\n",
              "\n",
              "This script should run without hanging or waiting to timeout because it won't attempt to download the model from \n",
              "the Hub.\n",
              "\n",
              "======Document 3 =======\n",
              "2. Use the [`hf_hub_download`](https://huggingface.co/docs/hub/adding-a-library#download-files-from-the-hub) \n",
              "function to download a file to a specific path. For example, the following command downloads the `config.json` file\n",
              "from the [T0](https://huggingface.co/bigscience/T0_3B) model to your desired path:\n",
              "\n",
              "    ```py\n",
              "    >>> from huggingface_hub import hf_hub_download\n",
              "\n",
              "    >>> hf_hub_download(repo_id=\"bigscience/T0_3B\", filename=\"config.json\", cache_dir=\"./your/path/bigscience_t0\")\n",
              "    ```\n",
              "\n",
              "======Document 4 =======\n",
              "Reload a quantized model with the [`~PreTrainedModel.from_pretrained`] method, and set `device_map=\"auto\"` to \n",
              "automatically distribute the model on all available GPUs to load the model faster without using more memory than \n",
              "needed.\n",
              "\n",
              "```py\n",
              "from transformers import AutoModelForCausalLM\n",
              "\n",
              "model = AutoModelForCausalLM.from_pretrained(\"{your_username}/opt-125m-gptq\", device_map=\"auto\")\n",
              "```\n",
              "\n",
              "### ExLlama\n",
              "\n",
              "======Document 5 =======\n",
              "```bash\n",
              "pip install transformers\n",
              "```\n",
              "\n",
              "もしサンプルを試したい、またはコードの最先端が必要で、新しいリリースを待てない場合は、[ライブラリをソースからインス\n",
              "トール](https://huggingface.co/docs/transformers/installation#installing-from-source)する必要があります。\n",
              "\n",
              "### condaにて\n",
              "\n",
              "Transformersバージョン4.0.0から、condaチャンネルを搭載しました: `huggingface`。\n",
              "\n",
              "🤗Transformersは以下のようにcondaを使って設置することができます:\n",
              "\n",
              "```shell script\n",
              "conda install -c huggingface transformers\n",
              "```\n",
              "\n",
              "Flax、PyTorch、TensorFlowをcondaでインストールする方法は、それぞれのインストールページに従ってください。\n",
              "\n",
              "======Document 6 =======\n",
              "In many cases you will want to download the full repository. Here is an example downloading all the files from a \n",
              "repo. You can even specify specific revisions!\n",
              "\n",
              "```\n",
              "from huggingface_hub import snapshot_download\n",
              "local_path = snapshot_download(\"flax-community/roberta-base-als\");\n",
              "```\n",
              "\n",
              "======Document 7 =======\n",
              "```py\n",
              "print(model.get_memory_footprint())\n",
              "```\n",
              "\n",
              "Quantized models can be loaded from the [`~PreTrainedModel.from_pretrained`] method without needing to specify the \n",
              "`load_in_8bit` or `load_in_4bit` parameters:\n",
              "\n",
              "```py\n",
              "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
              "\n",
              "model = AutoModelForCausalLM.from_pretrained(\"{your_username}/bloom-560m-8bit\", device_map=\"auto\")\n",
              "```\n",
              "\n",
              "### 8-bit\n",
              "\n",
              "<Tip>\n",
              "\n",
              "======Document 8 =======\n",
              "For more details about troubleshooting and getting help, take a look at [Chapter \n",
              "8](https://huggingface.co/course/chapter8/1?fw=pt) of the Hugging Face course.\n",
              "\n",
              "\n",
              "## Firewalled environments\n",
              "\n",
              "Some GPU instances on cloud and intranet setups are firewalled to external connections, resulting in a connection \n",
              "error. When your script attempts to download model weights or datasets, the download will hang and then timeout \n",
              "with the following message:\n",
              "\n",
              "======Document 9 =======\n",
              "```shell\n",
              "# download validation-dataset first\n",
              "mkdir natural-questions-validation\n",
              "wget \n",
              "https://huggingface.co/datasets/vasudevgupta/natural-questions-validation/resolve/main/natural_questions-validation\n",
              ".arrow -P natural-questions-validation\n",
              "wget https://huggingface.co/datasets/vasudevgupta/natural-questions-validation/resolve/main/dataset_info.json -P \n",
              "natural-questions-validation\n",
              "\n",
              "Out: None\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Execution logs:</span>\n",
              "\n",
              "Retrieved documents:\n",
              "\n",
              "\n",
              "======Document 0 =======\n",
              "All of the following examples make use of [dataset \n",
              "streaming](https://huggingface.co/docs/datasets/master/dataset_streaming), therefore allowing to train models on \n",
              "massive datasets\\\n",
              "without ever having to download the full dataset.\n",
              "\n",
              "## Masked language modeling\n",
              "\n",
              "======Document 1 =======\n",
              "```py\n",
              "&gt;&gt;&gt; from transformers import AutoModelForSequenceClassification\n",
              "&gt;&gt;&gt; import torch\n",
              "\n",
              "&gt;&gt;&gt; model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
              "&gt;&gt;&gt; model.config.pad_token_id\n",
              "0\n",
              "```\n",
              "\n",
              "The following example shows the output without masking the padding tokens:\n",
              "\n",
              "======Document 2 =======\n",
              "&lt;Tip&gt;\n",
              "\n",
              "Add [🤗 Datasets](https://huggingface.co/docs/datasets/) to your offline training workflow with the environment \n",
              "variable `HF_DATASETS_OFFLINE=1`.\n",
              "\n",
              "&lt;/Tip&gt;\n",
              "\n",
              "```bash\n",
              "HF_DATASETS_OFFLINE=1 TRANSFORMERS_OFFLINE=1 \\\n",
              "python examples/pytorch/translation/run_translation.py --model_name_or_path t5-small --dataset_name wmt16 \n",
              "--dataset_config ro-en ...\n",
              "```\n",
              "\n",
              "This script should run without hanging or waiting to timeout because it won't attempt to download the model from \n",
              "the Hub.\n",
              "\n",
              "======Document 3 =======\n",
              "2. Use the [`hf_hub_download`](https://huggingface.co/docs/hub/adding-a-library#download-files-from-the-hub) \n",
              "function to download a file to a specific path. For example, the following command downloads the `config.json` file\n",
              "from the [T0](https://huggingface.co/bigscience/T0_3B) model to your desired path:\n",
              "\n",
              "    ```py\n",
              "    &gt;&gt;&gt; from huggingface_hub import hf_hub_download\n",
              "\n",
              "    &gt;&gt;&gt; hf_hub_download(repo_id=\"bigscience/T0_3B\", filename=\"config.json\", cache_dir=\"./your/path/bigscience_t0\")\n",
              "    ```\n",
              "\n",
              "======Document 4 =======\n",
              "Reload a quantized model with the [`~PreTrainedModel.from_pretrained`] method, and set `device_map=\"auto\"` to \n",
              "automatically distribute the model on all available GPUs to load the model faster without using more memory than \n",
              "needed.\n",
              "\n",
              "```py\n",
              "from transformers import AutoModelForCausalLM\n",
              "\n",
              "model = AutoModelForCausalLM.from_pretrained(\"{your_username}/opt-125m-gptq\", device_map=\"auto\")\n",
              "```\n",
              "\n",
              "### ExLlama\n",
              "\n",
              "======Document 5 =======\n",
              "```bash\n",
              "pip install transformers\n",
              "```\n",
              "\n",
              "もしサンプルを試したい、またはコードの最先端が必要で、新しいリリースを待てない場合は、[ライブラリをソースからインス\n",
              "トール](https://huggingface.co/docs/transformers/installation#installing-from-source)する必要があります。\n",
              "\n",
              "### condaにて\n",
              "\n",
              "Transformersバージョン4.0.0から、condaチャンネルを搭載しました: `huggingface`。\n",
              "\n",
              "🤗Transformersは以下のようにcondaを使って設置することができます:\n",
              "\n",
              "```shell script\n",
              "conda install -c huggingface transformers\n",
              "```\n",
              "\n",
              "Flax、PyTorch、TensorFlowをcondaでインストールする方法は、それぞれのインストールページに従ってください。\n",
              "\n",
              "======Document 6 =======\n",
              "In many cases you will want to download the full repository. Here is an example downloading all the files from a \n",
              "repo. You can even specify specific revisions!\n",
              "\n",
              "```\n",
              "from huggingface_hub import snapshot_download\n",
              "local_path = snapshot_download(\"flax-community/roberta-base-als\");\n",
              "```\n",
              "\n",
              "======Document 7 =======\n",
              "```py\n",
              "print(model.get_memory_footprint())\n",
              "```\n",
              "\n",
              "Quantized models can be loaded from the [`~PreTrainedModel.from_pretrained`] method without needing to specify the \n",
              "`load_in_8bit` or `load_in_4bit` parameters:\n",
              "\n",
              "```py\n",
              "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
              "\n",
              "model = AutoModelForCausalLM.from_pretrained(\"{your_username}/bloom-560m-8bit\", device_map=\"auto\")\n",
              "```\n",
              "\n",
              "### 8-bit\n",
              "\n",
              "&lt;Tip&gt;\n",
              "\n",
              "======Document 8 =======\n",
              "For more details about troubleshooting and getting help, take a look at [Chapter \n",
              "8](https://huggingface.co/course/chapter8/1?fw=pt) of the Hugging Face course.\n",
              "\n",
              "\n",
              "## Firewalled environments\n",
              "\n",
              "Some GPU instances on cloud and intranet setups are firewalled to external connections, resulting in a connection \n",
              "error. When your script attempts to download model weights or datasets, the download will hang and then timeout \n",
              "with the following message:\n",
              "\n",
              "======Document 9 =======\n",
              "```shell\n",
              "# download validation-dataset first\n",
              "mkdir natural-questions-validation\n",
              "wget \n",
              "https://huggingface.co/datasets/vasudevgupta/natural-questions-validation/resolve/main/natural_questions-validation\n",
              ".arrow -P natural-questions-validation\n",
              "wget https://huggingface.co/datasets/vasudevgupta/natural-questions-validation/resolve/main/dataset_info.json -P \n",
              "natural-questions-validation\n",
              "\n",
              "Out: None\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2m[Step 3: Duration 3.98 seconds| Input tokens: 12,759 | Output tokens: 489]\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 3: Duration 3.98 seconds| Input tokens: 12,759 | Output tokens: 489]</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;31mReached max iterations.\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Reached max iterations.</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Final answer: Given the constraints and the information retrieved, it seems that direct imports of the \n",
              "`transformers` or `huggingface_hub` libraries are not allowed. However, you can manually download the model by \n",
              "using the links provided on the Hugging Face model page. Here's how you can do it:\n",
              "\n",
              "1. **Navigate to the Hugging Face Model Page**:\n",
              "   - Go to the [Hugging Face Models Hub](https://huggingface.co/models).\n",
              "   - Search for the model you want to download, for example, `bigscience/T0_3B`.\n",
              "   - Click on the model you wish to download.\n",
              "\n",
              "2. **Download the Model Files**:\n",
              "   - On the model's page, you'll see a list of files associated with the model.\n",
              "   - Click on the file you want to download (or select multiple files if needed).\n",
              "   - Click the file name to expand it and get a raw link to download it.\n",
              "\n",
              "3. **Download Using `wget` or `curl`**:\n",
              "   - Use `wget` or `curl` in your terminal to download the files manually. For example:\n",
              "\n",
              "```bash\n",
              "wget https://huggingface.co/bigscience/T0_3B/resolve/main/config.json -P ./path/to/save\n",
              "wget https://huggingface.co/bigscience/T0_3B/resolve/main/pytorch_model.bin -P ./path/to/save\n",
              "```\n",
              "\n",
              "4. **Automate Downloading**:\n",
              "   - If you need to download the entire model, you can use `git` to clone the repository, but this might not always\n",
              "be supported for large models due to download limits or server restrictions.\n",
              "\n",
              "```bash\n",
              "git clone https://huggingface.co/bigscience/T0_3B ./path/to/save\n",
              "```\n",
              "\n",
              "5. **Check the `HF_DATASETS_OFFLINE` and `TRANSFORMERS_OFFLINE` Environment Variables**:\n",
              "   - If you have the model files already downloaded, make sure to set these environment variables to avoid \n",
              "redownloads:\n",
              "\n",
              "```bash\n",
              "export HF_DATASETS_OFFLINE=1\n",
              "export TRANSFORMERS_OFFLINE=1\n",
              "```\n",
              "\n",
              "By following these steps, you should be able to download the model files manually without importing the \n",
              "`transformers` or `huggingface_hub` libraries in your code.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Final answer: Given the constraints and the information retrieved, it seems that direct imports of the \n",
              "`transformers` or `huggingface_hub` libraries are not allowed. However, you can manually download the model by \n",
              "using the links provided on the Hugging Face model page. Here's how you can do it:\n",
              "\n",
              "1. **Navigate to the Hugging Face Model Page**:\n",
              "   - Go to the [Hugging Face Models Hub](https://huggingface.co/models).\n",
              "   - Search for the model you want to download, for example, `bigscience/T0_3B`.\n",
              "   - Click on the model you wish to download.\n",
              "\n",
              "2. **Download the Model Files**:\n",
              "   - On the model's page, you'll see a list of files associated with the model.\n",
              "   - Click on the file you want to download (or select multiple files if needed).\n",
              "   - Click the file name to expand it and get a raw link to download it.\n",
              "\n",
              "3. **Download Using `wget` or `curl`**:\n",
              "   - Use `wget` or `curl` in your terminal to download the files manually. For example:\n",
              "\n",
              "```bash\n",
              "wget https://huggingface.co/bigscience/T0_3B/resolve/main/config.json -P ./path/to/save\n",
              "wget https://huggingface.co/bigscience/T0_3B/resolve/main/pytorch_model.bin -P ./path/to/save\n",
              "```\n",
              "\n",
              "4. **Automate Downloading**:\n",
              "   - If you need to download the entire model, you can use `git` to clone the repository, but this might not always\n",
              "be supported for large models due to download limits or server restrictions.\n",
              "\n",
              "```bash\n",
              "git clone https://huggingface.co/bigscience/T0_3B ./path/to/save\n",
              "```\n",
              "\n",
              "5. **Check the `HF_DATASETS_OFFLINE` and `TRANSFORMERS_OFFLINE` Environment Variables**:\n",
              "   - If you have the model files already downloaded, make sure to set these environment variables to avoid \n",
              "redownloads:\n",
              "\n",
              "```bash\n",
              "export HF_DATASETS_OFFLINE=1\n",
              "export TRANSFORMERS_OFFLINE=1\n",
              "```\n",
              "\n",
              "By following these steps, you should be able to download the model files manually without importing the \n",
              "`transformers` or `huggingface_hub` libraries in your code.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2m[Step 4: Duration 0.00 seconds| Input tokens: 15,886 | Output tokens: 958]\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 4: Duration 0.00 seconds| Input tokens: 15,886 | Output tokens: 958]</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Given the constraints and the information retrieved, it seems that direct imports of the `transformers` or `huggingface_hub` libraries are not allowed. However, you can manually download the model by using the links provided on the Hugging Face model page. Here's how you can do it:\\n\\n1. **Navigate to the Hugging Face Model Page**:\\n   - Go to the [Hugging Face Models Hub](https://huggingface.co/models).\\n   - Search for the model you want to download, for example, `bigscience/T0_3B`.\\n   - Click on the model you wish to download.\\n\\n2. **Download the Model Files**:\\n   - On the model's page, you'll see a list of files associated with the model.\\n   - Click on the file you want to download (or select multiple files if needed).\\n   - Click the file name to expand it and get a raw link to download it.\\n\\n3. **Download Using `wget` or `curl`**:\\n   - Use `wget` or `curl` in your terminal to download the files manually. For example:\\n\\n```bash\\nwget https://huggingface.co/bigscience/T0_3B/resolve/main/config.json -P ./path/to/save\\nwget https://huggingface.co/bigscience/T0_3B/resolve/main/pytorch_model.bin -P ./path/to/save\\n```\\n\\n4. **Automate Downloading**:\\n   - If you need to download the entire model, you can use `git` to clone the repository, but this might not always be supported for large models due to download limits or server restrictions.\\n\\n```bash\\ngit clone https://huggingface.co/bigscience/T0_3B ./path/to/save\\n```\\n\\n5. **Check the `HF_DATASETS_OFFLINE` and `TRANSFORMERS_OFFLINE` Environment Variables**:\\n   - If you have the model files already downloaded, make sure to set these environment variables to avoid redownloads:\\n\\n```bash\\nexport HF_DATASETS_OFFLINE=1\\nexport TRANSFORMERS_OFFLINE=1\\n```\\n\\nBy following these steps, you should be able to download the model files manually without importing the `transformers` or `huggingface_hub` libraries in your code.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent_output = agent.run(\"For a transformers model training, which is slower, the forward or the backward pass?\")\n",
        "print(\"Final Output:\")\n",
        "print(agent_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zsT2lpK97_TY",
        "outputId": "16ddd21f-9763-45f4-96c3-f047f846e773"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[38;2;212;183;2m╭─\u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╮\u001b[0m\n",
              "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
              "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mFor a transformers model training, which is slower, the forward or the backward pass?\u001b[0m                           \u001b[38;2;212;183;2m│\u001b[0m\n",
              "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
              "\u001b[38;2;212;183;2m╰─\u001b[0m\u001b[38;2;212;183;2m HfApiModel - Qwen/Qwen2.5-Coder-32B-Instruct \u001b[0m\u001b[38;2;212;183;2m─────────────────────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╯\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">╭──────────────────────────────────────────────────── </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ────────────────────────────────────────────────────╮</span>\n",
              "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
              "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">For a transformers model training, which is slower, the forward or the backward pass?</span>                           <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
              "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
              "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">╰─ HfApiModel - Qwen/Qwen2.5-Coder-32B-Instruct ──────────────────────────────────────────────────────────────────╯</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m0\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[3mOutput message of the LLM:\u001b[0m ────────────────────────────────────────────────────────────────────────────────────────\n",
              "\u001b[38;2;230;237;243;48;2;13;17;23mThought:\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mTo\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mdetermine\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mwhether\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mthe\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mforward\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mpass\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mor\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mthe\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mbackward\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mpass\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mis\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mslower\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23min\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23ma\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mtransformers\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mmodel\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mtraining,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[48;2;13;17;23m   \u001b[0m\n",
              "\u001b[38;2;230;237;243;48;2;13;17;23mI'll\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23msearch\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mthe\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mtransformers\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mdocumentation\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mthat\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mmight\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mprovide\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23minsights\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mon\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mthis\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mtopic.\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mThe\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;165;214;255;48;2;13;17;23m`retriever`\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mtool\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mwill\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mbe\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\n",
              "\u001b[38;2;230;237;243;48;2;13;17;23museful\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mfor\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mthis\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mpurpose.\u001b[0m\u001b[48;2;13;17;23m                                                                                           \u001b[0m\n",
              "\u001b[48;2;13;17;23m                                                                                                                   \u001b[0m\n",
              "\u001b[38;2;230;237;243;48;2;13;17;23mCode:\u001b[0m\u001b[48;2;13;17;23m                                                                                                              \u001b[0m\n",
              "\u001b[38;2;230;237;243;48;2;13;17;23m```py\u001b[0m\u001b[48;2;13;17;23m                                                                                                              \u001b[0m\n",
              "\u001b[38;2;230;237;243;48;2;13;17;23mresult\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m=\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mretriever(query=\"comparison\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mof\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mforward\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mand\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mbackward\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mpass\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23min\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mtransformers\")\u001b[0m\u001b[48;2;13;17;23m                                \u001b[0m\n",
              "\u001b[38;2;230;237;243;48;2;13;17;23mprint(result)\u001b[0m\u001b[48;2;13;17;23m                                                                                                      \u001b[0m\n",
              "\u001b[38;2;230;237;243;48;2;13;17;23m```<end_code>\u001b[0m\u001b[48;2;13;17;23m                                                                                                      \u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">Output message of the LLM:</span> ────────────────────────────────────────────────────────────────────────────────────────\n",
              "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">Thought: To determine whether the forward pass or the backward pass is slower in a transformers model training, </span><span style=\"background-color: #0d1117\">   </span>\n",
              "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">I'll search the transformers documentation that might provide insights on this topic. The </span><span style=\"color: #a5d6ff; text-decoration-color: #a5d6ff; background-color: #0d1117\">`retriever`</span><span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\"> tool will be </span>\n",
              "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">useful for this purpose.</span><span style=\"background-color: #0d1117\">                                                                                           </span>\n",
              "<span style=\"background-color: #0d1117\">                                                                                                                   </span>\n",
              "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">Code:</span><span style=\"background-color: #0d1117\">                                                                                                              </span>\n",
              "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">```py</span><span style=\"background-color: #0d1117\">                                                                                                              </span>\n",
              "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">result = retriever(query=\"comparison of forward and backward pass in transformers\")</span><span style=\"background-color: #0d1117\">                                </span>\n",
              "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">print(result)</span><span style=\"background-color: #0d1117\">                                                                                                      </span>\n",
              "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">```&lt;end_code&gt;</span><span style=\"background-color: #0d1117\">                                                                                                      </span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "╭─ \u001b[1mExecuting this code:\u001b[0m ──────────────────────────────────────────────────────────────────────────────────────────╮\n",
              "│ \u001b[1;38;2;227;227;221;48;2;39;40;34m  \u001b[0m\u001b[38;2;101;102;96;48;2;39;40;34m1 \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mresult\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mretriever\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mquery\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mcomparison of forward and backward pass in transformers\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                        \u001b[0m │\n",
              "│ \u001b[1;38;2;227;227;221;48;2;39;40;34m  \u001b[0m\u001b[38;2;101;102;96;48;2;39;40;34m2 \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mprint\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mresult\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                              \u001b[0m │\n",
              "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─ <span style=\"font-weight: bold\">Executing this code:</span> ──────────────────────────────────────────────────────────────────────────────────────────╮\n",
              "│ <span style=\"color: #e3e3dd; text-decoration-color: #e3e3dd; background-color: #272822; font-weight: bold\">  </span><span style=\"color: #656660; text-decoration-color: #656660; background-color: #272822\">1 </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">result </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> retriever(query</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"comparison of forward and backward pass in transformers\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">                        </span> │\n",
              "│ <span style=\"color: #e3e3dd; text-decoration-color: #e3e3dd; background-color: #272822; font-weight: bold\">  </span><span style=\"color: #656660; text-decoration-color: #656660; background-color: #272822\">2 </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">print(result)</span><span style=\"background-color: #272822\">                                                                                              </span> │\n",
              "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mExecution logs:\u001b[0m\n",
              "\n",
              "Retrieved documents:\n",
              "\n",
              "\n",
              "======Document 0 =======\n",
              "Saving all activations from the forward pass in order to compute the gradients during the backward pass can result \n",
              "in \n",
              "significant memory overhead. The alternative approach of discarding the activations and recalculating them when \n",
              "needed \n",
              "during the backward pass, would introduce a considerable computational overhead and slow down the training process.\n",
              "\n",
              "======Document 1 =======\n",
              "- A train step function which combines the loss function and optimizer update, does the forward and backward pass \n",
              "and returns the updated parameters.\n",
              "\n",
              "======Document 2 =======\n",
              "For convolutions and linear layers there are 2x flops in the backward compared to the forward, which generally \n",
              "translates \n",
              "into ~2x slower (sometimes more, because sizes in the backward tend to be more awkward). Activations are usually \n",
              "bandwidth-limited, and it’s typical for an activation to have to read more data in the backward than in the forward\n",
              "(e.g. activation forward reads once, writes once, activation backward reads twice, gradOutput and output of the \n",
              "forward,\n",
              "\n",
              "======Document 3 =======\n",
              "overhead. This is super helpful when you have activation checkpointing enabled, where we do a forward recompute and\n",
              "backward passes a single layer granularity and want to keep the parameter in the forward recompute till the \n",
              "backward\n",
              "\n",
              "======Document 4 =======\n",
              "## How to benchmark 🤗 Transformers models\n",
              "\n",
              "The classes [`PyTorchBenchmark`] and [`TensorFlowBenchmark`] allow to flexibly benchmark 🤗 Transformers models. \n",
              "The benchmark classes allow us to measure the _peak memory usage_ and _required time_ for both _inference_ and \n",
              "_training_.\n",
              "\n",
              "<Tip>\n",
              "\n",
              "Hereby, _inference_ is defined by a single forward pass, and _training_ is defined by a single forward pass and\n",
              "backward pass.\n",
              "\n",
              "</Tip>\n",
              "\n",
              "======Document 5 =======\n",
              "than 10 seconds. In case only very large checkpoints are available,\n",
              "    it might make more sense to create a dummy model in the new\n",
              "    environment with randomly initialized weights and save those weights\n",
              "    for comparison with the 🤗 Transformers version of your model\n",
              "-   Make sure you are using the easiest way of calling a forward pass in\n",
              "    the original repository. Ideally, you want to find the function in\n",
              "    the original repository that **only** calls a single forward pass,\n",
              "\n",
              "======Document 6 =======\n",
              "than 10 seconds. In case only very large checkpoints are available,\n",
              "    it might make more sense to create a dummy model in the new\n",
              "    environment with randomly initialized weights and save those weights\n",
              "    for comparison with the 🤗 Transformers version of your model\n",
              "-   Make sure you are using the easiest way of calling a forward pass in\n",
              "    the original repository. Ideally, you want to find the function in\n",
              "    the original repository that **only** calls a single forward pass,\n",
              "\n",
              "======Document 7 =======\n",
              "In case only very large checkpoints are available, it might make more sense to create a dummy model in the new\n",
              "  environment with randomly initialized weights and save those weights for comparison with the 🤗 Transformers \n",
              "version\n",
              "  of your model\n",
              "- Make sure you are using the easiest way of calling a forward pass in the original repository. Ideally, you want \n",
              "to\n",
              "  find the function in the original repository that **only** calls a single forward pass, *i.e.* that is often \n",
              "called\n",
              "\n",
              "======Document 8 =======\n",
              "**Forward Activations**\n",
              "\n",
              "- size depends on many factors, the key ones being sequence length, hidden size and batch size.\n",
              "\n",
              "There are the input and output that are being passed and returned by the forward and the backward functions and the\n",
              "forward activations saved for gradient computation.\n",
              "\n",
              "**Temporary Memory**\n",
              "\n",
              "======Document 9 =======\n",
              "This way each of the 3 GPUs gets the full tensors reconstructed and makes a forward pass with its own mini-batch.\n",
              "As soon as the calculation is done, the data that is no longer needed gets dropped - it's only used during the \n",
              "calculation. \n",
              "The reconstruction is done efficiently via a pre-fetch.\n",
              "\n",
              "Then the whole process is repeated for layer Lb, then Lc forward-wise, and then backward Lc -> Lb -> La.\n",
              "\n",
              "<Tip>\n",
              "\n",
              "Out: None\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Execution logs:</span>\n",
              "\n",
              "Retrieved documents:\n",
              "\n",
              "\n",
              "======Document 0 =======\n",
              "Saving all activations from the forward pass in order to compute the gradients during the backward pass can result \n",
              "in \n",
              "significant memory overhead. The alternative approach of discarding the activations and recalculating them when \n",
              "needed \n",
              "during the backward pass, would introduce a considerable computational overhead and slow down the training process.\n",
              "\n",
              "======Document 1 =======\n",
              "- A train step function which combines the loss function and optimizer update, does the forward and backward pass \n",
              "and returns the updated parameters.\n",
              "\n",
              "======Document 2 =======\n",
              "For convolutions and linear layers there are 2x flops in the backward compared to the forward, which generally \n",
              "translates \n",
              "into ~2x slower (sometimes more, because sizes in the backward tend to be more awkward). Activations are usually \n",
              "bandwidth-limited, and it’s typical for an activation to have to read more data in the backward than in the forward\n",
              "(e.g. activation forward reads once, writes once, activation backward reads twice, gradOutput and output of the \n",
              "forward,\n",
              "\n",
              "======Document 3 =======\n",
              "overhead. This is super helpful when you have activation checkpointing enabled, where we do a forward recompute and\n",
              "backward passes a single layer granularity and want to keep the parameter in the forward recompute till the \n",
              "backward\n",
              "\n",
              "======Document 4 =======\n",
              "## How to benchmark 🤗 Transformers models\n",
              "\n",
              "The classes [`PyTorchBenchmark`] and [`TensorFlowBenchmark`] allow to flexibly benchmark 🤗 Transformers models. \n",
              "The benchmark classes allow us to measure the _peak memory usage_ and _required time_ for both _inference_ and \n",
              "_training_.\n",
              "\n",
              "&lt;Tip&gt;\n",
              "\n",
              "Hereby, _inference_ is defined by a single forward pass, and _training_ is defined by a single forward pass and\n",
              "backward pass.\n",
              "\n",
              "&lt;/Tip&gt;\n",
              "\n",
              "======Document 5 =======\n",
              "than 10 seconds. In case only very large checkpoints are available,\n",
              "    it might make more sense to create a dummy model in the new\n",
              "    environment with randomly initialized weights and save those weights\n",
              "    for comparison with the 🤗 Transformers version of your model\n",
              "-   Make sure you are using the easiest way of calling a forward pass in\n",
              "    the original repository. Ideally, you want to find the function in\n",
              "    the original repository that **only** calls a single forward pass,\n",
              "\n",
              "======Document 6 =======\n",
              "than 10 seconds. In case only very large checkpoints are available,\n",
              "    it might make more sense to create a dummy model in the new\n",
              "    environment with randomly initialized weights and save those weights\n",
              "    for comparison with the 🤗 Transformers version of your model\n",
              "-   Make sure you are using the easiest way of calling a forward pass in\n",
              "    the original repository. Ideally, you want to find the function in\n",
              "    the original repository that **only** calls a single forward pass,\n",
              "\n",
              "======Document 7 =======\n",
              "In case only very large checkpoints are available, it might make more sense to create a dummy model in the new\n",
              "  environment with randomly initialized weights and save those weights for comparison with the 🤗 Transformers \n",
              "version\n",
              "  of your model\n",
              "- Make sure you are using the easiest way of calling a forward pass in the original repository. Ideally, you want \n",
              "to\n",
              "  find the function in the original repository that **only** calls a single forward pass, *i.e.* that is often \n",
              "called\n",
              "\n",
              "======Document 8 =======\n",
              "**Forward Activations**\n",
              "\n",
              "- size depends on many factors, the key ones being sequence length, hidden size and batch size.\n",
              "\n",
              "There are the input and output that are being passed and returned by the forward and the backward functions and the\n",
              "forward activations saved for gradient computation.\n",
              "\n",
              "**Temporary Memory**\n",
              "\n",
              "======Document 9 =======\n",
              "This way each of the 3 GPUs gets the full tensors reconstructed and makes a forward pass with its own mini-batch.\n",
              "As soon as the calculation is done, the data that is no longer needed gets dropped - it's only used during the \n",
              "calculation. \n",
              "The reconstruction is done efficiently via a pre-fetch.\n",
              "\n",
              "Then the whole process is repeated for layer Lb, then Lc forward-wise, and then backward Lc -&gt; Lb -&gt; La.\n",
              "\n",
              "&lt;Tip&gt;\n",
              "\n",
              "Out: None\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2m[Step 0: Duration 2.35 seconds| Input tokens: 2,068 | Output tokens: 77]\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 0: Duration 2.35 seconds| Input tokens: 2,068 | Output tokens: 77]</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[3mOutput message of the LLM:\u001b[0m ────────────────────────────────────────────────────────────────────────────────────────\n",
              "\u001b[38;2;230;237;243;48;2;13;17;23mThought:\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mFrom\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mthe\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mretrieved\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mdocuments,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mit's\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mevident\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mthat\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mthe\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mbackward\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mpass\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23min\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mtransformers\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mmodels\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mgenerally\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mtakes\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[48;2;13;17;23m \u001b[0m\n",
              "\u001b[38;2;230;237;243;48;2;13;17;23mmore\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mtime\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mand\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mcomputational\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mresources\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mcompared\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mto\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mthe\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mforward\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mpass.\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mDocument\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m2\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mspecifically\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mmentions\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mthat\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mthere\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mare\u001b[0m\n",
              "\u001b[38;2;230;237;243;48;2;13;17;23mtypically\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m2x\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mmore\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mfloating-point\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23moperations\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m(flops)\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23min\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mthe\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mbackward\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mpass\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mcompared\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mto\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mthe\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mforward\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mpass,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mcausing\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mit\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[48;2;13;17;23m \u001b[0m\n",
              "\u001b[38;2;230;237;243;48;2;13;17;23mto\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mbe\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mapproximately\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m2x\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mslower.\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[48;2;13;17;23m                                                                                    \u001b[0m\n",
              "\u001b[48;2;13;17;23m                                                                                                                   \u001b[0m\n",
              "\u001b[38;2;230;237;243;48;2;13;17;23mGiven\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mthis\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23minformation,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mI\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mcan\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mnow\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mconfidently\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mstate\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mwhich\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mpass\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mis\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mslower.\u001b[0m\u001b[48;2;13;17;23m                                          \u001b[0m\n",
              "\u001b[48;2;13;17;23m                                                                                                                   \u001b[0m\n",
              "\u001b[38;2;230;237;243;48;2;13;17;23mCode:\u001b[0m\u001b[48;2;13;17;23m                                                                                                              \u001b[0m\n",
              "\u001b[38;2;230;237;243;48;2;13;17;23m```py\u001b[0m\u001b[48;2;13;17;23m                                                                                                              \u001b[0m\n",
              "\u001b[38;2;230;237;243;48;2;13;17;23mfinal_answer(\"backward\")\u001b[0m\u001b[48;2;13;17;23m                                                                                           \u001b[0m\n",
              "\u001b[38;2;230;237;243;48;2;13;17;23m```<end_code>\u001b[0m\u001b[48;2;13;17;23m                                                                                                      \u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">Output message of the LLM:</span> ────────────────────────────────────────────────────────────────────────────────────────\n",
              "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">Thought: From the retrieved documents, it's evident that the backward pass in transformers models generally takes </span><span style=\"background-color: #0d1117\"> </span>\n",
              "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">more time and computational resources compared to the forward pass. Document 2 specifically mentions that there are</span>\n",
              "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">typically 2x more floating-point operations (flops) in the backward pass compared to the forward pass, causing it </span><span style=\"background-color: #0d1117\"> </span>\n",
              "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">to be approximately 2x slower. </span><span style=\"background-color: #0d1117\">                                                                                    </span>\n",
              "<span style=\"background-color: #0d1117\">                                                                                                                   </span>\n",
              "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">Given this information, I can now confidently state which pass is slower.</span><span style=\"background-color: #0d1117\">                                          </span>\n",
              "<span style=\"background-color: #0d1117\">                                                                                                                   </span>\n",
              "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">Code:</span><span style=\"background-color: #0d1117\">                                                                                                              </span>\n",
              "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">```py</span><span style=\"background-color: #0d1117\">                                                                                                              </span>\n",
              "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">final_answer(\"backward\")</span><span style=\"background-color: #0d1117\">                                                                                           </span>\n",
              "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">```&lt;end_code&gt;</span><span style=\"background-color: #0d1117\">                                                                                                      </span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "╭─ \u001b[1mExecuting this code:\u001b[0m ──────────────────────────────────────────────────────────────────────────────────────────╮\n",
              "│ \u001b[1;38;2;227;227;221;48;2;39;40;34m  \u001b[0m\u001b[38;2;101;102;96;48;2;39;40;34m1 \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mfinal_answer\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mbackward\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                   \u001b[0m │\n",
              "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─ <span style=\"font-weight: bold\">Executing this code:</span> ──────────────────────────────────────────────────────────────────────────────────────────╮\n",
              "│ <span style=\"color: #e3e3dd; text-decoration-color: #e3e3dd; background-color: #272822; font-weight: bold\">  </span><span style=\"color: #656660; text-decoration-color: #656660; background-color: #272822\">1 </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">final_answer(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"backward\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">                                                                                   </span> │\n",
              "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;2;212;183;2mOut - Final answer: backward\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Out - Final answer: backward</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2m[Step 1: Duration 3.30 seconds| Input tokens: 5,143 | Output tokens: 178]\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 3.30 seconds| Input tokens: 5,143 | Output tokens: 178]</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Output:\n",
            "backward\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from smolagents import HfApiModel, CodeAgent\n",
        "agent = CodeAgent(\n",
        "    tools=[retriever_tool, DuckDuckGoSearchTool],\n",
        "    model=HfApiModel(),\n",
        "    max_iterations=4,\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "D2HwsreI8mnF"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent.run(\"How to download a model from Huggingface?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YywLGi_B9GLY",
        "outputId": "0247a21b-8edd-4b61-bcaf-a42cc4f86f2a"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[38;2;212;183;2m╭─\u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╮\u001b[0m\n",
              "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
              "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mHow to download a model from Huggingface?\u001b[0m                                                                       \u001b[38;2;212;183;2m│\u001b[0m\n",
              "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
              "\u001b[38;2;212;183;2m╰─\u001b[0m\u001b[38;2;212;183;2m HfApiModel - Qwen/Qwen2.5-Coder-32B-Instruct \u001b[0m\u001b[38;2;212;183;2m─────────────────────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╯\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">╭──────────────────────────────────────────────────── </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ────────────────────────────────────────────────────╮</span>\n",
              "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
              "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">How to download a model from Huggingface?</span>                                                                       <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
              "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
              "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">╰─ HfApiModel - Qwen/Qwen2.5-Coder-32B-Instruct ──────────────────────────────────────────────────────────────────╯</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m0\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[3mOutput message of the LLM:\u001b[0m ────────────────────────────────────────────────────────────────────────────────────────\n",
              "\u001b[38;2;230;237;243;48;2;13;17;23mThought:\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mTo\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mdownload\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23ma\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mmodel\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mfrom\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mHugging\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mFace,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mI\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mneed\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mto\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23muse\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mthe\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mappropriate\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mlibrary\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mand\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mfollow\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mthe\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mcorrect\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23msteps.\u001b[0m\n",
              "\u001b[38;2;230;237;243;48;2;13;17;23mI\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mwill\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23muse\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mthe\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;165;214;255;48;2;13;17;23m`retriever`\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mtool\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mto\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mfind\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mthe\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mrelevant\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mdocumentation\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mand\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mthen\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mfollow\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mthe\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23minstructions\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mto\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mperform\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mthe\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\n",
              "\u001b[38;2;230;237;243;48;2;13;17;23mdownload.\u001b[0m\u001b[48;2;13;17;23m                                                                                                          \u001b[0m\n",
              "\u001b[48;2;13;17;23m                                                                                                                   \u001b[0m\n",
              "\u001b[38;2;230;237;243;48;2;13;17;23mCode:\u001b[0m\u001b[48;2;13;17;23m                                                                                                              \u001b[0m\n",
              "\u001b[38;2;230;237;243;48;2;13;17;23m```py\u001b[0m\u001b[48;2;13;17;23m                                                                                                              \u001b[0m\n",
              "\u001b[38;2;230;237;243;48;2;13;17;23mhuggingface_docs\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m=\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mretriever(query=\"how\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mto\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mdownload\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23ma\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mmodel\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mfrom\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mHugging\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mFace\")\u001b[0m\u001b[48;2;13;17;23m                                    \u001b[0m\n",
              "\u001b[38;2;230;237;243;48;2;13;17;23mprint(huggingface_docs)\u001b[0m\u001b[48;2;13;17;23m                                                                                            \u001b[0m\n",
              "\u001b[38;2;230;237;243;48;2;13;17;23m```<end_code>\u001b[0m\u001b[48;2;13;17;23m                                                                                                      \u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">Output message of the LLM:</span> ────────────────────────────────────────────────────────────────────────────────────────\n",
              "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">Thought: To download a model from Hugging Face, I need to use the appropriate library and follow the correct steps.</span>\n",
              "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">I will use the </span><span style=\"color: #a5d6ff; text-decoration-color: #a5d6ff; background-color: #0d1117\">`retriever`</span><span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\"> tool to find the relevant documentation and then follow the instructions to perform the </span>\n",
              "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">download.</span><span style=\"background-color: #0d1117\">                                                                                                          </span>\n",
              "<span style=\"background-color: #0d1117\">                                                                                                                   </span>\n",
              "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">Code:</span><span style=\"background-color: #0d1117\">                                                                                                              </span>\n",
              "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">```py</span><span style=\"background-color: #0d1117\">                                                                                                              </span>\n",
              "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">huggingface_docs = retriever(query=\"how to download a model from Hugging Face\")</span><span style=\"background-color: #0d1117\">                                    </span>\n",
              "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">print(huggingface_docs)</span><span style=\"background-color: #0d1117\">                                                                                            </span>\n",
              "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">```&lt;end_code&gt;</span><span style=\"background-color: #0d1117\">                                                                                                      </span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "╭─ \u001b[1mExecuting this code:\u001b[0m ──────────────────────────────────────────────────────────────────────────────────────────╮\n",
              "│ \u001b[1;38;2;227;227;221;48;2;39;40;34m  \u001b[0m\u001b[38;2;101;102;96;48;2;39;40;34m1 \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mhuggingface_docs\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mretriever\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mquery\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mhow to download a model from Hugging Face\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                            \u001b[0m │\n",
              "│ \u001b[1;38;2;227;227;221;48;2;39;40;34m  \u001b[0m\u001b[38;2;101;102;96;48;2;39;40;34m2 \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mprint\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mhuggingface_docs\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                    \u001b[0m │\n",
              "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─ <span style=\"font-weight: bold\">Executing this code:</span> ──────────────────────────────────────────────────────────────────────────────────────────╮\n",
              "│ <span style=\"color: #e3e3dd; text-decoration-color: #e3e3dd; background-color: #272822; font-weight: bold\">  </span><span style=\"color: #656660; text-decoration-color: #656660; background-color: #272822\">1 </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">huggingface_docs </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> retriever(query</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"how to download a model from Hugging Face\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">                            </span> │\n",
              "│ <span style=\"color: #e3e3dd; text-decoration-color: #e3e3dd; background-color: #272822; font-weight: bold\">  </span><span style=\"color: #656660; text-decoration-color: #656660; background-color: #272822\">2 </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">print(huggingface_docs)</span><span style=\"background-color: #272822\">                                                                                    </span> │\n",
              "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mExecution logs:\u001b[0m\n",
              "\n",
              "Retrieved documents:\n",
              "\n",
              "\n",
              "======Document 0 =======\n",
              "🚀 **Deploy**\n",
              "- A blog post on how to [Convert Transformers to ONNX with Hugging Face \n",
              "Optimum](https://www.philschmid.de/convert-transformers-to-onnx).\n",
              "- A blog post on how to [Setup Deep Learning environment for Hugging Face Transformers with Habana Gaudi on \n",
              "AWS](https://www.philschmid.de/getting-started-habana-gaudi#conclusion).\n",
              "\n",
              "======Document 1 =======\n",
              "- A blog post on how to [Fine-Tune ViT for Image Classification with Hugging Face \n",
              "Transformers](https://huggingface.co/blog/fine-tune-vit)\n",
              "- A blog post on [Image Classification with Hugging Face Transformers and \n",
              "`Keras`](https://www.philschmid.de/image-classification-huggingface-transformers-keras)\n",
              "- A notebook on [Fine-tuning for Image Classification with Hugging Face \n",
              "Transformers](https://github.com/huggingface/notebooks/blob/main/examples/image_classification.ipynb)\n",
              "\n",
              "======Document 2 =======\n",
              "- An example of how to train [`BartForConditionalGeneration`] with a Hugging Face `datasets` object can be found in\n",
              "this [forum \n",
              "discussion](https://discuss.huggingface.co/t/train-bart-for-conditional-generation-e-g-summarization/1904)\n",
              "- [Summarization](https://huggingface.co/course/chapter7/5?fw=pt#summarization) chapter of the 🤗 Hugging Face \n",
              "course.\n",
              "- [Summarization task guide](../tasks/summarization)\n",
              "\n",
              "======Document 3 =======\n",
              "For more details about troubleshooting and getting help, take a look at [Chapter \n",
              "8](https://huggingface.co/course/chapter8/1?fw=pt) of the Hugging Face course.\n",
              "\n",
              "\n",
              "## Firewalled environments\n",
              "\n",
              "Some GPU instances on cloud and intranet setups are firewalled to external connections, resulting in a connection \n",
              "error. When your script attempts to download model weights or datasets, the download will hang and then timeout \n",
              "with the following message:\n",
              "\n",
              "======Document 4 =======\n",
              "Alternatively, you can directly download the checkpoints using:\n",
              "\n",
              "```bash\n",
              "wget --content-disposition https://api.ngc.nvidia.com/v2/models/nvidia/megatron_lm_345m/versions/v0.0/zip -O\n",
              "megatron_gpt2_345m_v0_0.zip\n",
              "```\n",
              "\n",
              "Once you have obtained the checkpoint from NVIDIA GPU Cloud (NGC), you have to convert it to a format that will \n",
              "easily\n",
              "be loaded by Hugging Face Transformers GPT2 implementation.\n",
              "\n",
              "======Document 5 =======\n",
              "- A blog post on [fine-tuning\n",
              "  LayoutLM for document-understanding using Keras & Hugging Face\n",
              "  Transformers](https://www.philschmid.de/fine-tuning-layoutlm-keras).\n",
              "\n",
              "- A blog post on how to [fine-tune LayoutLM for document-understanding using only Hugging Face \n",
              "Transformers](https://www.philschmid.de/fine-tuning-layoutlm).\n",
              "\n",
              "======Document 6 =======\n",
              "In this example, we're going to demonstrate how to train a TensorFlow model from 🤗 Transformers from scratch. If \n",
              "you're interested in some background theory on training Hugging Face models with TensorFlow on TPU, please check \n",
              "out our \n",
              "[tutorial doc](https://huggingface.co/docs/transformers/main/perf_train_tpu_tf) on this topic!\n",
              "\n",
              "======Document 7 =======\n",
              "We will now login to Hugging Face Hub so we can push our model to the Hugging Face Hub through the `Trainer`. \n",
              "\n",
              "```python\n",
              "from huggingface_hub import notebook_login\n",
              "\n",
              "notebook_login()\n",
              "```\n",
              "\n",
              "Let's set the `TrainingArguments`, the teacher model and the student model. \n",
              "\n",
              "```python\n",
              "from transformers import AutoModelForImageClassification, MobileNetV2Config, MobileNetV2ForImageClassification\n",
              "\n",
              "======Document 8 =======\n",
              "the community to add some *Tips* to show how the model should be used. Don't hesitate to ping the Hugging Face team\n",
              "regarding the docstrings.\n",
              "\n",
              "======Document 9 =======\n",
              "- A notebook on how to [Fine-tune the Vision Transformer on CIFAR-10 with the Hugging Face \n",
              "Trainer](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/VisionTransformer/Fine_tuning_the_Vision_\n",
              "Transformer_on_CIFAR_10_with_the_%F0%9F%A4%97_Trainer.ipynb)\n",
              "\n",
              "Out: None\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Execution logs:</span>\n",
              "\n",
              "Retrieved documents:\n",
              "\n",
              "\n",
              "======Document 0 =======\n",
              "🚀 **Deploy**\n",
              "- A blog post on how to [Convert Transformers to ONNX with Hugging Face \n",
              "Optimum](https://www.philschmid.de/convert-transformers-to-onnx).\n",
              "- A blog post on how to [Setup Deep Learning environment for Hugging Face Transformers with Habana Gaudi on \n",
              "AWS](https://www.philschmid.de/getting-started-habana-gaudi#conclusion).\n",
              "\n",
              "======Document 1 =======\n",
              "- A blog post on how to [Fine-Tune ViT for Image Classification with Hugging Face \n",
              "Transformers](https://huggingface.co/blog/fine-tune-vit)\n",
              "- A blog post on [Image Classification with Hugging Face Transformers and \n",
              "`Keras`](https://www.philschmid.de/image-classification-huggingface-transformers-keras)\n",
              "- A notebook on [Fine-tuning for Image Classification with Hugging Face \n",
              "Transformers](https://github.com/huggingface/notebooks/blob/main/examples/image_classification.ipynb)\n",
              "\n",
              "======Document 2 =======\n",
              "- An example of how to train [`BartForConditionalGeneration`] with a Hugging Face `datasets` object can be found in\n",
              "this [forum \n",
              "discussion](https://discuss.huggingface.co/t/train-bart-for-conditional-generation-e-g-summarization/1904)\n",
              "- [Summarization](https://huggingface.co/course/chapter7/5?fw=pt#summarization) chapter of the 🤗 Hugging Face \n",
              "course.\n",
              "- [Summarization task guide](../tasks/summarization)\n",
              "\n",
              "======Document 3 =======\n",
              "For more details about troubleshooting and getting help, take a look at [Chapter \n",
              "8](https://huggingface.co/course/chapter8/1?fw=pt) of the Hugging Face course.\n",
              "\n",
              "\n",
              "## Firewalled environments\n",
              "\n",
              "Some GPU instances on cloud and intranet setups are firewalled to external connections, resulting in a connection \n",
              "error. When your script attempts to download model weights or datasets, the download will hang and then timeout \n",
              "with the following message:\n",
              "\n",
              "======Document 4 =======\n",
              "Alternatively, you can directly download the checkpoints using:\n",
              "\n",
              "```bash\n",
              "wget --content-disposition https://api.ngc.nvidia.com/v2/models/nvidia/megatron_lm_345m/versions/v0.0/zip -O\n",
              "megatron_gpt2_345m_v0_0.zip\n",
              "```\n",
              "\n",
              "Once you have obtained the checkpoint from NVIDIA GPU Cloud (NGC), you have to convert it to a format that will \n",
              "easily\n",
              "be loaded by Hugging Face Transformers GPT2 implementation.\n",
              "\n",
              "======Document 5 =======\n",
              "- A blog post on [fine-tuning\n",
              "  LayoutLM for document-understanding using Keras &amp; Hugging Face\n",
              "  Transformers](https://www.philschmid.de/fine-tuning-layoutlm-keras).\n",
              "\n",
              "- A blog post on how to [fine-tune LayoutLM for document-understanding using only Hugging Face \n",
              "Transformers](https://www.philschmid.de/fine-tuning-layoutlm).\n",
              "\n",
              "======Document 6 =======\n",
              "In this example, we're going to demonstrate how to train a TensorFlow model from 🤗 Transformers from scratch. If \n",
              "you're interested in some background theory on training Hugging Face models with TensorFlow on TPU, please check \n",
              "out our \n",
              "[tutorial doc](https://huggingface.co/docs/transformers/main/perf_train_tpu_tf) on this topic!\n",
              "\n",
              "======Document 7 =======\n",
              "We will now login to Hugging Face Hub so we can push our model to the Hugging Face Hub through the `Trainer`. \n",
              "\n",
              "```python\n",
              "from huggingface_hub import notebook_login\n",
              "\n",
              "notebook_login()\n",
              "```\n",
              "\n",
              "Let's set the `TrainingArguments`, the teacher model and the student model. \n",
              "\n",
              "```python\n",
              "from transformers import AutoModelForImageClassification, MobileNetV2Config, MobileNetV2ForImageClassification\n",
              "\n",
              "======Document 8 =======\n",
              "the community to add some *Tips* to show how the model should be used. Don't hesitate to ping the Hugging Face team\n",
              "regarding the docstrings.\n",
              "\n",
              "======Document 9 =======\n",
              "- A notebook on how to [Fine-tune the Vision Transformer on CIFAR-10 with the Hugging Face \n",
              "Trainer](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/VisionTransformer/Fine_tuning_the_Vision_\n",
              "Transformer_on_CIFAR_10_with_the_%F0%9F%A4%97_Trainer.ipynb)\n",
              "\n",
              "Out: None\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2m[Step 0: Duration 2.77 seconds| Input tokens: 2,143 | Output tokens: 85]\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 0: Duration 2.77 seconds| Input tokens: 2,143 | Output tokens: 85]</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[3mOutput message of the LLM:\u001b[0m ────────────────────────────────────────────────────────────────────────────────────────\n",
              "\u001b[38;2;230;237;243;48;2;13;17;23mThought:\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mThe\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mretrieved\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mdocuments\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mdo\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mnot\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mdirectly\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23manswer\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mhow\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mto\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mdownload\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23ma\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mmodel\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mfrom\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mHugging\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mFace.\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mI\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mwill\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23muse\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mthe\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[48;2;13;17;23m \u001b[0m\n",
              "\u001b[38;2;165;214;255;48;2;13;17;23m`web_search`\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mtool\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mto\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mfind\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mmore\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mrelevant\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23minformation\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23min\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23man\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23meffort\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mto\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23midentify\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mthe\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mcorrect\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mprocess.\u001b[0m\u001b[48;2;13;17;23m                  \u001b[0m\n",
              "\u001b[48;2;13;17;23m                                                                                                                   \u001b[0m\n",
              "\u001b[38;2;230;237;243;48;2;13;17;23mCode:\u001b[0m\u001b[48;2;13;17;23m                                                                                                              \u001b[0m\n",
              "\u001b[38;2;230;237;243;48;2;13;17;23m```py\u001b[0m\u001b[48;2;13;17;23m                                                                                                              \u001b[0m\n",
              "\u001b[38;2;230;237;243;48;2;13;17;23msearch_results\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m=\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mweb_search(query=\"how\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mto\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mdownload\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23ma\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mmodel\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mfrom\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mHugging\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mFace\")\u001b[0m\u001b[48;2;13;17;23m                                     \u001b[0m\n",
              "\u001b[38;2;230;237;243;48;2;13;17;23mprint(search_results)\u001b[0m\u001b[48;2;13;17;23m                                                                                              \u001b[0m\n",
              "\u001b[38;2;230;237;243;48;2;13;17;23m```<end_code>\u001b[0m\u001b[48;2;13;17;23m                                                                                                      \u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">Output message of the LLM:</span> ────────────────────────────────────────────────────────────────────────────────────────\n",
              "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">Thought: The retrieved documents do not directly answer how to download a model from Hugging Face. I will use the </span><span style=\"background-color: #0d1117\"> </span>\n",
              "<span style=\"color: #a5d6ff; text-decoration-color: #a5d6ff; background-color: #0d1117\">`web_search`</span><span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\"> tool to find more relevant information in an effort to identify the correct process.</span><span style=\"background-color: #0d1117\">                  </span>\n",
              "<span style=\"background-color: #0d1117\">                                                                                                                   </span>\n",
              "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">Code:</span><span style=\"background-color: #0d1117\">                                                                                                              </span>\n",
              "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">```py</span><span style=\"background-color: #0d1117\">                                                                                                              </span>\n",
              "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">search_results = web_search(query=\"how to download a model from Hugging Face\")</span><span style=\"background-color: #0d1117\">                                     </span>\n",
              "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">print(search_results)</span><span style=\"background-color: #0d1117\">                                                                                              </span>\n",
              "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">```&lt;end_code&gt;</span><span style=\"background-color: #0d1117\">                                                                                                      </span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "╭─ \u001b[1mExecuting this code:\u001b[0m ──────────────────────────────────────────────────────────────────────────────────────────╮\n",
              "│ \u001b[1;38;2;227;227;221;48;2;39;40;34m  \u001b[0m\u001b[38;2;101;102;96;48;2;39;40;34m1 \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34msearch_results\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mweb_search\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mquery\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mhow to download a model from Hugging Face\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                             \u001b[0m │\n",
              "│ \u001b[1;38;2;227;227;221;48;2;39;40;34m  \u001b[0m\u001b[38;2;101;102;96;48;2;39;40;34m2 \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mprint\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34msearch_results\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                      \u001b[0m │\n",
              "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─ <span style=\"font-weight: bold\">Executing this code:</span> ──────────────────────────────────────────────────────────────────────────────────────────╮\n",
              "│ <span style=\"color: #e3e3dd; text-decoration-color: #e3e3dd; background-color: #272822; font-weight: bold\">  </span><span style=\"color: #656660; text-decoration-color: #656660; background-color: #272822\">1 </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">search_results </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> web_search(query</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"how to download a model from Hugging Face\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">                             </span> │\n",
              "│ <span style=\"color: #e3e3dd; text-decoration-color: #e3e3dd; background-color: #272822; font-weight: bold\">  </span><span style=\"color: #656660; text-decoration-color: #656660; background-color: #272822\">2 </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">print(search_results)</span><span style=\"background-color: #272822\">                                                                                      </span> │\n",
              "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mExecution logs:\u001b[0m\n",
              "<smolagents.default_tools.DuckDuckGoSearchTool object at 0x789b98065c90>\n",
              "\n",
              "Out: None\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Execution logs:</span>\n",
              "&lt;smolagents.default_tools.DuckDuckGoSearchTool object at 0x789b98065c90&gt;\n",
              "\n",
              "Out: None\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2m[Step 1: Duration 2.49 seconds| Input tokens: 5,397 | Output tokens: 159]\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 2.49 seconds| Input tokens: 5,397 | Output tokens: 159]</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m2\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[3mOutput message of the LLM:\u001b[0m ────────────────────────────────────────────────────────────────────────────────────────\n",
              "\u001b[38;2;230;237;243;48;2;13;17;23mThought:\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mIt\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mseems\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mthe\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;165;214;255;48;2;13;17;23m`web_search`\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mfunction\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mis\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mnot\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mdirectly\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mreturning\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mthe\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mexpected\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mformatted\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23msearch\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mresults.\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mI'll\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[48;2;13;17;23m \u001b[0m\n",
              "\u001b[38;2;230;237;243;48;2;13;17;23mtry\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mto\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mrefine\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mthe\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23msearch\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mquery\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mand\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mcall\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mit\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23magain\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mto\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mensure\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mI\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mget\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mthe\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mrelevant\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mresults.\u001b[0m\u001b[48;2;13;17;23m                             \u001b[0m\n",
              "\u001b[48;2;13;17;23m                                                                                                                   \u001b[0m\n",
              "\u001b[38;2;230;237;243;48;2;13;17;23mCode:\u001b[0m\u001b[48;2;13;17;23m                                                                                                              \u001b[0m\n",
              "\u001b[38;2;230;237;243;48;2;13;17;23m```py\u001b[0m\u001b[48;2;13;17;23m                                                                                                              \u001b[0m\n",
              "\u001b[38;2;230;237;243;48;2;13;17;23msearch_results\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m=\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mweb_search(query=\"how\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mto\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mdownload\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23ma\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mmodel\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mfrom\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mHugging\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mFace\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mtransformers\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mlibrary\")\u001b[0m\u001b[48;2;13;17;23m                \u001b[0m\n",
              "\u001b[38;2;230;237;243;48;2;13;17;23mprint(search_results)\u001b[0m\u001b[48;2;13;17;23m                                                                                              \u001b[0m\n",
              "\u001b[38;2;230;237;243;48;2;13;17;23m```<end_code>\u001b[0m\u001b[48;2;13;17;23m                                                                                                      \u001b[0m\n",
              "\u001b[48;2;13;17;23m                                                                                                                   \u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">Output message of the LLM:</span> ────────────────────────────────────────────────────────────────────────────────────────\n",
              "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">Thought: It seems the </span><span style=\"color: #a5d6ff; text-decoration-color: #a5d6ff; background-color: #0d1117\">`web_search`</span><span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\"> function is not directly returning the expected formatted search results. I'll </span><span style=\"background-color: #0d1117\"> </span>\n",
              "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">try to refine the search query and call it again to ensure I get the relevant results.</span><span style=\"background-color: #0d1117\">                             </span>\n",
              "<span style=\"background-color: #0d1117\">                                                                                                                   </span>\n",
              "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">Code:</span><span style=\"background-color: #0d1117\">                                                                                                              </span>\n",
              "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">```py</span><span style=\"background-color: #0d1117\">                                                                                                              </span>\n",
              "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">search_results = web_search(query=\"how to download a model from Hugging Face transformers library\")</span><span style=\"background-color: #0d1117\">                </span>\n",
              "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">print(search_results)</span><span style=\"background-color: #0d1117\">                                                                                              </span>\n",
              "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">```&lt;end_code&gt;</span><span style=\"background-color: #0d1117\">                                                                                                      </span>\n",
              "<span style=\"background-color: #0d1117\">                                                                                                                   </span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "╭─ \u001b[1mExecuting this code:\u001b[0m ──────────────────────────────────────────────────────────────────────────────────────────╮\n",
              "│ \u001b[1;38;2;227;227;221;48;2;39;40;34m  \u001b[0m\u001b[38;2;101;102;96;48;2;39;40;34m1 \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34msearch_results\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mweb_search\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mquery\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mhow to download a model from Hugging Face transformers library\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m        \u001b[0m │\n",
              "│ \u001b[1;38;2;227;227;221;48;2;39;40;34m  \u001b[0m\u001b[38;2;101;102;96;48;2;39;40;34m2 \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mprint\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34msearch_results\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                      \u001b[0m │\n",
              "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─ <span style=\"font-weight: bold\">Executing this code:</span> ──────────────────────────────────────────────────────────────────────────────────────────╮\n",
              "│ <span style=\"color: #e3e3dd; text-decoration-color: #e3e3dd; background-color: #272822; font-weight: bold\">  </span><span style=\"color: #656660; text-decoration-color: #656660; background-color: #272822\">1 </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">search_results </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> web_search(query</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"how to download a model from Hugging Face transformers library\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">        </span> │\n",
              "│ <span style=\"color: #e3e3dd; text-decoration-color: #e3e3dd; background-color: #272822; font-weight: bold\">  </span><span style=\"color: #656660; text-decoration-color: #656660; background-color: #272822\">2 </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">print(search_results)</span><span style=\"background-color: #272822\">                                                                                      </span> │\n",
              "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mExecution logs:\u001b[0m\n",
              "<smolagents.default_tools.DuckDuckGoSearchTool object at 0x789b98067220>\n",
              "\n",
              "Out: None\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Execution logs:</span>\n",
              "&lt;smolagents.default_tools.DuckDuckGoSearchTool object at 0x789b98067220&gt;\n",
              "\n",
              "Out: None\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2m[Step 2: Duration 2.64 seconds| Input tokens: 8,841 | Output tokens: 233]\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 2: Duration 2.64 seconds| Input tokens: 8,841 | Output tokens: 233]</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m3\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[3mOutput message of the LLM:\u001b[0m ────────────────────────────────────────────────────────────────────────────────────────\n",
              "\u001b[38;2;230;237;243;48;2;13;17;23mThought:\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mIt\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mappears\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mthat\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mthe\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;165;214;255;48;2;13;17;23m`web_search`\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mfunction\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mis\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mnot\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mreturning\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23many\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mstructured\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23msearch\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mresults.\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mTo\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mproceed,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mI\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[48;2;13;17;23m  \u001b[0m\n",
              "\u001b[38;2;230;237;243;48;2;13;17;23mwill\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mmanually\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mcheck\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mthe\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mHugging\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mFace\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mdocumentation\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mfor\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23minstructions\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mon\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mdownloading\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23ma\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mmodel.\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mI\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mwill\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23muse\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mthe\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[48;2;13;17;23m        \u001b[0m\n",
              "\u001b[38;2;165;214;255;48;2;13;17;23m`retriever`\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mtool\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mto\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mget\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mthe\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mrelevant\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23minformation\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mfrom\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mthe\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mHugging\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mFace\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mTransformers\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mdocumentation.\u001b[0m\u001b[48;2;13;17;23m                 \u001b[0m\n",
              "\u001b[48;2;13;17;23m                                                                                                                   \u001b[0m\n",
              "\u001b[38;2;230;237;243;48;2;13;17;23mCode:\u001b[0m\u001b[48;2;13;17;23m                                                                                                              \u001b[0m\n",
              "\u001b[38;2;230;237;243;48;2;13;17;23m```py\u001b[0m\u001b[48;2;13;17;23m                                                                                                              \u001b[0m\n",
              "\u001b[38;2;230;237;243;48;2;13;17;23mhuggingface_docs\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m=\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mretriever(query=\"how\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mto\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mdownload\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23ma\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mmodel\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mfrom\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mHugging\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mFace\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mTransformers\")\u001b[0m\u001b[48;2;13;17;23m                       \u001b[0m\n",
              "\u001b[38;2;230;237;243;48;2;13;17;23mprint(huggingface_docs)\u001b[0m\u001b[48;2;13;17;23m                                                                                            \u001b[0m\n",
              "\u001b[38;2;230;237;243;48;2;13;17;23m```<end_code>\u001b[0m\u001b[48;2;13;17;23m                                                                                                      \u001b[0m\n",
              "\u001b[48;2;13;17;23m                                                                                                                   \u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">Output message of the LLM:</span> ────────────────────────────────────────────────────────────────────────────────────────\n",
              "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">Thought: It appears that the </span><span style=\"color: #a5d6ff; text-decoration-color: #a5d6ff; background-color: #0d1117\">`web_search`</span><span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\"> function is not returning any structured search results. To proceed, I </span><span style=\"background-color: #0d1117\">  </span>\n",
              "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">will manually check the Hugging Face documentation for instructions on downloading a model. I will use the </span><span style=\"background-color: #0d1117\">        </span>\n",
              "<span style=\"color: #a5d6ff; text-decoration-color: #a5d6ff; background-color: #0d1117\">`retriever`</span><span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\"> tool to get the relevant information from the Hugging Face Transformers documentation.</span><span style=\"background-color: #0d1117\">                 </span>\n",
              "<span style=\"background-color: #0d1117\">                                                                                                                   </span>\n",
              "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">Code:</span><span style=\"background-color: #0d1117\">                                                                                                              </span>\n",
              "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">```py</span><span style=\"background-color: #0d1117\">                                                                                                              </span>\n",
              "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">huggingface_docs = retriever(query=\"how to download a model from Hugging Face Transformers\")</span><span style=\"background-color: #0d1117\">                       </span>\n",
              "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">print(huggingface_docs)</span><span style=\"background-color: #0d1117\">                                                                                            </span>\n",
              "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">```&lt;end_code&gt;</span><span style=\"background-color: #0d1117\">                                                                                                      </span>\n",
              "<span style=\"background-color: #0d1117\">                                                                                                                   </span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "╭─ \u001b[1mExecuting this code:\u001b[0m ──────────────────────────────────────────────────────────────────────────────────────────╮\n",
              "│ \u001b[1;38;2;227;227;221;48;2;39;40;34m  \u001b[0m\u001b[38;2;101;102;96;48;2;39;40;34m1 \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mhuggingface_docs\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mretriever\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mquery\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mhow to download a model from Hugging Face Transformers\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m               \u001b[0m │\n",
              "│ \u001b[1;38;2;227;227;221;48;2;39;40;34m  \u001b[0m\u001b[38;2;101;102;96;48;2;39;40;34m2 \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mprint\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mhuggingface_docs\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                    \u001b[0m │\n",
              "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─ <span style=\"font-weight: bold\">Executing this code:</span> ──────────────────────────────────────────────────────────────────────────────────────────╮\n",
              "│ <span style=\"color: #e3e3dd; text-decoration-color: #e3e3dd; background-color: #272822; font-weight: bold\">  </span><span style=\"color: #656660; text-decoration-color: #656660; background-color: #272822\">1 </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">huggingface_docs </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> retriever(query</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"how to download a model from Hugging Face Transformers\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">               </span> │\n",
              "│ <span style=\"color: #e3e3dd; text-decoration-color: #e3e3dd; background-color: #272822; font-weight: bold\">  </span><span style=\"color: #656660; text-decoration-color: #656660; background-color: #272822\">2 </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">print(huggingface_docs)</span><span style=\"background-color: #272822\">                                                                                    </span> │\n",
              "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mExecution logs:\u001b[0m\n",
              "\n",
              "Retrieved documents:\n",
              "\n",
              "\n",
              "======Document 0 =======\n",
              "🚀 **Deploy**\n",
              "- A blog post on how to [Convert Transformers to ONNX with Hugging Face \n",
              "Optimum](https://www.philschmid.de/convert-transformers-to-onnx).\n",
              "- A blog post on how to [Setup Deep Learning environment for Hugging Face Transformers with Habana Gaudi on \n",
              "AWS](https://www.philschmid.de/getting-started-habana-gaudi#conclusion).\n",
              "\n",
              "======Document 1 =======\n",
              "- A blog post on how to [Fine-Tune ViT for Image Classification with Hugging Face \n",
              "Transformers](https://huggingface.co/blog/fine-tune-vit)\n",
              "- A blog post on [Image Classification with Hugging Face Transformers and \n",
              "`Keras`](https://www.philschmid.de/image-classification-huggingface-transformers-keras)\n",
              "- A notebook on [Fine-tuning for Image Classification with Hugging Face \n",
              "Transformers](https://github.com/huggingface/notebooks/blob/main/examples/image_classification.ipynb)\n",
              "\n",
              "======Document 2 =======\n",
              "⚡️ **Inference**\n",
              "- A blog post on how to [Accelerate BERT inference with Hugging Face Transformers and AWS \n",
              "Inferentia](https://huggingface.co/blog/bert-inferentia-sagemaker).\n",
              "- A blog post on how to [Accelerate BERT inference with DeepSpeed-Inference on \n",
              "GPUs](https://www.philschmid.de/bert-deepspeed-inference).\n",
              "\n",
              "⚙️ **Pretraining**\n",
              "- A blog post on [Pre-Training BERT with Hugging Face Transformers and Habana \n",
              "Gaudi](https://www.philschmid.de/pre-training-bert-habana).\n",
              "\n",
              "======Document 3 =======\n",
              "Alternatively, you can directly download the checkpoints using:\n",
              "\n",
              "```bash\n",
              "wget --content-disposition https://api.ngc.nvidia.com/v2/models/nvidia/megatron_lm_345m/versions/v0.0/zip -O\n",
              "megatron_gpt2_345m_v0_0.zip\n",
              "```\n",
              "\n",
              "Once you have obtained the checkpoint from NVIDIA GPU Cloud (NGC), you have to convert it to a format that will \n",
              "easily\n",
              "be loaded by Hugging Face Transformers GPT2 implementation.\n",
              "\n",
              "======Document 4 =======\n",
              "In this example, we're going to demonstrate how to train a TensorFlow model from 🤗 Transformers from scratch. If \n",
              "you're interested in some background theory on training Hugging Face models with TensorFlow on TPU, please check \n",
              "out our \n",
              "[tutorial doc](https://huggingface.co/docs/transformers/main/perf_train_tpu_tf) on this topic!\n",
              "\n",
              "======Document 5 =======\n",
              "⚗️ Optimization\n",
              "\n",
              "- A blog post on how to [quantize DistilBERT with 🤗 Optimum and Intel](https://huggingface.co/blog/intel).\n",
              "- A blog post on how [Optimizing Transformers for GPUs with 🤗 \n",
              "Optimum](https://www.philschmid.de/optimizing-transformers-with-optimum-gpu).\n",
              "- A blog post on [Optimizing Transformers with Hugging Face \n",
              "Optimum](https://www.philschmid.de/optimizing-transformers-with-optimum).\n",
              "\n",
              "⚡️ Inference\n",
              "\n",
              "======Document 6 =======\n",
              "⚡️ Inference\n",
              "\n",
              "- A blog post on how to [Accelerate BERT inference with Hugging Face Transformers and AWS \n",
              "Inferentia](https://huggingface.co/blog/bert-inferentia-sagemaker) with DistilBERT.\n",
              "- A blog post on [Serverless Inference with Hugging Face's Transformers, DistilBERT and Amazon \n",
              "SageMaker](https://www.philschmid.de/sagemaker-serverless-huggingface-distilbert).\n",
              "\n",
              "🚀 Deploy\n",
              "\n",
              "======Document 7 =======\n",
              "- An example of how to train [`BartForConditionalGeneration`] with a Hugging Face `datasets` object can be found in\n",
              "this [forum \n",
              "discussion](https://discuss.huggingface.co/t/train-bart-for-conditional-generation-e-g-summarization/1904)\n",
              "- [Summarization](https://huggingface.co/course/chapter7/5?fw=pt#summarization) chapter of the 🤗 Hugging Face \n",
              "course.\n",
              "- [Summarization task guide](../tasks/summarization)\n",
              "\n",
              "======Document 8 =======\n",
              "For more details about troubleshooting and getting help, take a look at [Chapter \n",
              "8](https://huggingface.co/course/chapter8/1?fw=pt) of the Hugging Face course.\n",
              "\n",
              "\n",
              "## Firewalled environments\n",
              "\n",
              "Some GPU instances on cloud and intranet setups are firewalled to external connections, resulting in a connection \n",
              "error. When your script attempts to download model weights or datasets, the download will hang and then timeout \n",
              "with the following message:\n",
              "\n",
              "======Document 9 =======\n",
              "- A blog post on [fine-tuning\n",
              "  LayoutLM for document-understanding using Keras & Hugging Face\n",
              "  Transformers](https://www.philschmid.de/fine-tuning-layoutlm-keras).\n",
              "\n",
              "- A blog post on how to [fine-tune LayoutLM for document-understanding using only Hugging Face \n",
              "Transformers](https://www.philschmid.de/fine-tuning-layoutlm).\n",
              "\n",
              "Out: None\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Execution logs:</span>\n",
              "\n",
              "Retrieved documents:\n",
              "\n",
              "\n",
              "======Document 0 =======\n",
              "🚀 **Deploy**\n",
              "- A blog post on how to [Convert Transformers to ONNX with Hugging Face \n",
              "Optimum](https://www.philschmid.de/convert-transformers-to-onnx).\n",
              "- A blog post on how to [Setup Deep Learning environment for Hugging Face Transformers with Habana Gaudi on \n",
              "AWS](https://www.philschmid.de/getting-started-habana-gaudi#conclusion).\n",
              "\n",
              "======Document 1 =======\n",
              "- A blog post on how to [Fine-Tune ViT for Image Classification with Hugging Face \n",
              "Transformers](https://huggingface.co/blog/fine-tune-vit)\n",
              "- A blog post on [Image Classification with Hugging Face Transformers and \n",
              "`Keras`](https://www.philschmid.de/image-classification-huggingface-transformers-keras)\n",
              "- A notebook on [Fine-tuning for Image Classification with Hugging Face \n",
              "Transformers](https://github.com/huggingface/notebooks/blob/main/examples/image_classification.ipynb)\n",
              "\n",
              "======Document 2 =======\n",
              "⚡️ **Inference**\n",
              "- A blog post on how to [Accelerate BERT inference with Hugging Face Transformers and AWS \n",
              "Inferentia](https://huggingface.co/blog/bert-inferentia-sagemaker).\n",
              "- A blog post on how to [Accelerate BERT inference with DeepSpeed-Inference on \n",
              "GPUs](https://www.philschmid.de/bert-deepspeed-inference).\n",
              "\n",
              "⚙️ **Pretraining**\n",
              "- A blog post on [Pre-Training BERT with Hugging Face Transformers and Habana \n",
              "Gaudi](https://www.philschmid.de/pre-training-bert-habana).\n",
              "\n",
              "======Document 3 =======\n",
              "Alternatively, you can directly download the checkpoints using:\n",
              "\n",
              "```bash\n",
              "wget --content-disposition https://api.ngc.nvidia.com/v2/models/nvidia/megatron_lm_345m/versions/v0.0/zip -O\n",
              "megatron_gpt2_345m_v0_0.zip\n",
              "```\n",
              "\n",
              "Once you have obtained the checkpoint from NVIDIA GPU Cloud (NGC), you have to convert it to a format that will \n",
              "easily\n",
              "be loaded by Hugging Face Transformers GPT2 implementation.\n",
              "\n",
              "======Document 4 =======\n",
              "In this example, we're going to demonstrate how to train a TensorFlow model from 🤗 Transformers from scratch. If \n",
              "you're interested in some background theory on training Hugging Face models with TensorFlow on TPU, please check \n",
              "out our \n",
              "[tutorial doc](https://huggingface.co/docs/transformers/main/perf_train_tpu_tf) on this topic!\n",
              "\n",
              "======Document 5 =======\n",
              "⚗️ Optimization\n",
              "\n",
              "- A blog post on how to [quantize DistilBERT with 🤗 Optimum and Intel](https://huggingface.co/blog/intel).\n",
              "- A blog post on how [Optimizing Transformers for GPUs with 🤗 \n",
              "Optimum](https://www.philschmid.de/optimizing-transformers-with-optimum-gpu).\n",
              "- A blog post on [Optimizing Transformers with Hugging Face \n",
              "Optimum](https://www.philschmid.de/optimizing-transformers-with-optimum).\n",
              "\n",
              "⚡️ Inference\n",
              "\n",
              "======Document 6 =======\n",
              "⚡️ Inference\n",
              "\n",
              "- A blog post on how to [Accelerate BERT inference with Hugging Face Transformers and AWS \n",
              "Inferentia](https://huggingface.co/blog/bert-inferentia-sagemaker) with DistilBERT.\n",
              "- A blog post on [Serverless Inference with Hugging Face's Transformers, DistilBERT and Amazon \n",
              "SageMaker](https://www.philschmid.de/sagemaker-serverless-huggingface-distilbert).\n",
              "\n",
              "🚀 Deploy\n",
              "\n",
              "======Document 7 =======\n",
              "- An example of how to train [`BartForConditionalGeneration`] with a Hugging Face `datasets` object can be found in\n",
              "this [forum \n",
              "discussion](https://discuss.huggingface.co/t/train-bart-for-conditional-generation-e-g-summarization/1904)\n",
              "- [Summarization](https://huggingface.co/course/chapter7/5?fw=pt#summarization) chapter of the 🤗 Hugging Face \n",
              "course.\n",
              "- [Summarization task guide](../tasks/summarization)\n",
              "\n",
              "======Document 8 =======\n",
              "For more details about troubleshooting and getting help, take a look at [Chapter \n",
              "8](https://huggingface.co/course/chapter8/1?fw=pt) of the Hugging Face course.\n",
              "\n",
              "\n",
              "## Firewalled environments\n",
              "\n",
              "Some GPU instances on cloud and intranet setups are firewalled to external connections, resulting in a connection \n",
              "error. When your script attempts to download model weights or datasets, the download will hang and then timeout \n",
              "with the following message:\n",
              "\n",
              "======Document 9 =======\n",
              "- A blog post on [fine-tuning\n",
              "  LayoutLM for document-understanding using Keras &amp; Hugging Face\n",
              "  Transformers](https://www.philschmid.de/fine-tuning-layoutlm-keras).\n",
              "\n",
              "- A blog post on how to [fine-tune LayoutLM for document-understanding using only Hugging Face \n",
              "Transformers](https://www.philschmid.de/fine-tuning-layoutlm).\n",
              "\n",
              "Out: None\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2m[Step 3: Duration 3.18 seconds| Input tokens: 12,477 | Output tokens: 331]\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 3: Duration 3.18 seconds| Input tokens: 12,477 | Output tokens: 331]</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;31mReached max iterations.\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Reached max iterations.</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Final answer: To download a model from Hugging Face, you typically use the `transformers` library from Hugging \n",
              "Face, which provides a simple and efficient way to download pre-trained models. Here's a step-by-step guide on how \n",
              "to do it:\n",
              "\n",
              "1. **Install the Transformers Library**: If you haven't already, you need to install the `transformers` library. \n",
              "You can install it using pip:\n",
              "\n",
              "   ```bash\n",
              "   pip install transformers\n",
              "   ```\n",
              "\n",
              "2. **Import the Necessary Classes or Functions**: Depending on the type of model you want to download, you'll need \n",
              "to import the appropriate classes. Here are some common ones:\n",
              "\n",
              "   - For a pre-trained model: `AutoModel`\n",
              "   - For a pre-trained tokenizer: `AutoTokenizer`\n",
              "   - For a pre-trained configuration: `AutoConfig`\n",
              "   - For pre-trained models that require images, audio, or other inputs: You'll need to import the specific model \n",
              "class, such as `ViTModel` for Vision Transformers, `AutoModelForCausalLM` for causal language models, etc.\n",
              "\n",
              "3. **Download the Model**: Use the `from_pretrained` method to download the model. You need to specify the name of \n",
              "the model you want to download. This is usually the name of the repository on the Hugging Face Model Hub.\n",
              "\n",
              "   Here's an example of how to download a pre-trained BERT model and its tokenizer:\n",
              "\n",
              "   ```python\n",
              "   from transformers import AutoTokenizer, AutoModel\n",
              "\n",
              "   # Load pre-trained model and tokenizer\n",
              "   model_name = \"bert-base-uncased\"  # You can change this to any model you want to download\n",
              "   model = AutoModel.from_pretrained(model_name)\n",
              "   tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
              "   ```\n",
              "\n",
              "   After running this code, the model and tokenizer will be automatically downloaded and cached locally in the \n",
              "`~/.cache/huggingface/transformers` directory. Subsequent uses of `from_pretrained` with the same model name will \n",
              "load the model from the cache instead of downloading it again.\n",
              "\n",
              "4. **Using the Model**: Once downloaded, you can use the model for various tasks such as text classification, \n",
              "translation, summarization, etc. Make sure to refer to the Hugging Face documentation for specific instructions on \n",
              "how to use the model for your intended task.\n",
              "\n",
              "5. **Updating the Code**: If you need a newer version of the model, you can update the code to download the latest \n",
              "version from the Hugging Face Model Hub by specifying the model name as shown above.\n",
              "\n",
              "For more detailed instructions and examples, you can refer to the [Hugging Face Transformers \n",
              "documentation](https://huggingface.co/transformers/).\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Final answer: To download a model from Hugging Face, you typically use the `transformers` library from Hugging \n",
              "Face, which provides a simple and efficient way to download pre-trained models. Here's a step-by-step guide on how \n",
              "to do it:\n",
              "\n",
              "1. **Install the Transformers Library**: If you haven't already, you need to install the `transformers` library. \n",
              "You can install it using pip:\n",
              "\n",
              "   ```bash\n",
              "   pip install transformers\n",
              "   ```\n",
              "\n",
              "2. **Import the Necessary Classes or Functions**: Depending on the type of model you want to download, you'll need \n",
              "to import the appropriate classes. Here are some common ones:\n",
              "\n",
              "   - For a pre-trained model: `AutoModel`\n",
              "   - For a pre-trained tokenizer: `AutoTokenizer`\n",
              "   - For a pre-trained configuration: `AutoConfig`\n",
              "   - For pre-trained models that require images, audio, or other inputs: You'll need to import the specific model \n",
              "class, such as `ViTModel` for Vision Transformers, `AutoModelForCausalLM` for causal language models, etc.\n",
              "\n",
              "3. **Download the Model**: Use the `from_pretrained` method to download the model. You need to specify the name of \n",
              "the model you want to download. This is usually the name of the repository on the Hugging Face Model Hub.\n",
              "\n",
              "   Here's an example of how to download a pre-trained BERT model and its tokenizer:\n",
              "\n",
              "   ```python\n",
              "   from transformers import AutoTokenizer, AutoModel\n",
              "\n",
              "   # Load pre-trained model and tokenizer\n",
              "   model_name = \"bert-base-uncased\"  # You can change this to any model you want to download\n",
              "   model = AutoModel.from_pretrained(model_name)\n",
              "   tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
              "   ```\n",
              "\n",
              "   After running this code, the model and tokenizer will be automatically downloaded and cached locally in the \n",
              "`~/.cache/huggingface/transformers` directory. Subsequent uses of `from_pretrained` with the same model name will \n",
              "load the model from the cache instead of downloading it again.\n",
              "\n",
              "4. **Using the Model**: Once downloaded, you can use the model for various tasks such as text classification, \n",
              "translation, summarization, etc. Make sure to refer to the Hugging Face documentation for specific instructions on \n",
              "how to use the model for your intended task.\n",
              "\n",
              "5. **Updating the Code**: If you need a newer version of the model, you can update the code to download the latest \n",
              "version from the Hugging Face Model Hub by specifying the model name as shown above.\n",
              "\n",
              "For more detailed instructions and examples, you can refer to the [Hugging Face Transformers \n",
              "documentation](https://huggingface.co/transformers/).\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2m[Step 4: Duration 0.00 seconds| Input tokens: 15,319 | Output tokens: 877]\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 4: Duration 0.00 seconds| Input tokens: 15,319 | Output tokens: 877]</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'To download a model from Hugging Face, you typically use the `transformers` library from Hugging Face, which provides a simple and efficient way to download pre-trained models. Here\\'s a step-by-step guide on how to do it:\\n\\n1. **Install the Transformers Library**: If you haven\\'t already, you need to install the `transformers` library. You can install it using pip:\\n\\n   ```bash\\n   pip install transformers\\n   ```\\n\\n2. **Import the Necessary Classes or Functions**: Depending on the type of model you want to download, you\\'ll need to import the appropriate classes. Here are some common ones:\\n\\n   - For a pre-trained model: `AutoModel`\\n   - For a pre-trained tokenizer: `AutoTokenizer`\\n   - For a pre-trained configuration: `AutoConfig`\\n   - For pre-trained models that require images, audio, or other inputs: You\\'ll need to import the specific model class, such as `ViTModel` for Vision Transformers, `AutoModelForCausalLM` for causal language models, etc.\\n\\n3. **Download the Model**: Use the `from_pretrained` method to download the model. You need to specify the name of the model you want to download. This is usually the name of the repository on the Hugging Face Model Hub.\\n\\n   Here\\'s an example of how to download a pre-trained BERT model and its tokenizer:\\n\\n   ```python\\n   from transformers import AutoTokenizer, AutoModel\\n\\n   # Load pre-trained model and tokenizer\\n   model_name = \"bert-base-uncased\"  # You can change this to any model you want to download\\n   model = AutoModel.from_pretrained(model_name)\\n   tokenizer = AutoTokenizer.from_pretrained(model_name)\\n   ```\\n\\n   After running this code, the model and tokenizer will be automatically downloaded and cached locally in the `~/.cache/huggingface/transformers` directory. Subsequent uses of `from_pretrained` with the same model name will load the model from the cache instead of downloading it again.\\n\\n4. **Using the Model**: Once downloaded, you can use the model for various tasks such as text classification, translation, summarization, etc. Make sure to refer to the Hugging Face documentation for specific instructions on how to use the model for your intended task.\\n\\n5. **Updating the Code**: If you need a newer version of the model, you can update the code to download the latest version from the Hugging Face Model Hub by specifying the model name as shown above.\\n\\nFor more detailed instructions and examples, you can refer to the [Hugging Face Transformers documentation](https://huggingface.co/transformers/).'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NUvMTN319LKQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
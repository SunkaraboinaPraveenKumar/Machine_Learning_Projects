{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPVzs4ioE6ovFTPCBbVtCb3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SunkaraboinaPraveenKumar/Machine_Learning_Projects/blob/main/Transcribe_YT_Videos_LangChain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2BPa9E_VDids",
        "outputId": "7b86ea7c-532f-4ab6-e0cc-fa87eb83be88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m177.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m68.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/454.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.3/454.3 kB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.6/411.6 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m40.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qU  \\\n",
        "  python-dotenv \\\n",
        "  langchain \\\n",
        "  langchain-community \\\n",
        "  openai \\\n",
        "  langchain-openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-groq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJDeouDbFxZS",
        "outputId": "a0eb926d-c1a2-4973-9f66-b56771275e32"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-groq\n",
            "  Downloading langchain_groq-0.2.2-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting groq<1,>=0.4.1 (from langchain-groq)\n",
            "  Downloading groq-0.13.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.27 in /usr/local/lib/python3.10/dist-packages (from langchain-groq) (0.3.28)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain-groq) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain-groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain-groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain-groq) (2.10.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain-groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain-groq) (4.12.2)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain-groq) (6.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain-groq) (1.33)\n",
            "Requirement already satisfied: langsmith<0.3,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain-groq) (0.2.3)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain-groq) (24.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain-groq) (9.0.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain-groq) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain-groq) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.27->langchain-groq) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain-groq) (3.10.12)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain-groq) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain-groq) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain-groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain-groq) (2.27.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain-groq) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain-groq) (2.2.3)\n",
            "Downloading langchain_groq-0.2.2-py3-none-any.whl (14 kB)\n",
            "Downloading groq-0.13.1-py3-none-any.whl (109 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.1/109.1 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: groq, langchain-groq\n",
            "Successfully installed groq-0.13.1 langchain-groq-0.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "from langchain_groq import ChatGroq\n",
        "GROQ_API_KEY=userdata.get('groq_api_key')\n",
        "os.environ['GROQ_API_KEY']=GROQ_API_KEY\n",
        "\n",
        "llm_gpt4=ChatGroq(model=\"llama-3.1-8b-instant\")\n",
        "# llm_gpt4 = ChatOpenAI(model=\"gpt-4\")\n",
        "\n",
        "# Verify that you can use the LLM\n",
        "llm_gpt4.invoke(\"What is a large language model?\").content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "id": "yp57G_72DxCy",
        "outputId": "c73887ce-c5fc-4994-90dd-7d0d57f0cb32"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"A large language model (LLM) is a type of artificial intelligence (AI) model that is trained on a massive corpus of text data to generate human-like language. These models are designed to process and understand vast amounts of language, including grammar, syntax, and semantics.\\n\\nCharacteristics of large language models:\\n\\n1. **Training data:** LLMs are trained on a massive dataset of text, often consisting of hundreds of billions of words.\\n2. **Complex architecture:** These models have a complex neural network architecture, with multiple layers of neural networks, to process and analyze language.\\n3. **Self-supervised learning:** LLMs learn by predicting missing words, phrases, or sentences in a text, rather than being explicitly programmed to perform specific tasks.\\n4. **Generative capabilities:** LLMs can generate new text based on the patterns and structures they've learned from the training data.\\n5. **Multitask capabilities:** Many LLMs are designed to perform multiple tasks simultaneously, such as language translation, question-answering, and text summarization.\\n\\nTypes of large language models:\\n\\n1. **Transformers:** These models use a specific type of neural network architecture called a transformer, which is particularly effective for natural language processing tasks.\\n2. **Recurrent Neural Networks (RNNs):** These models use a type of neural network that processes input sequences in a sequential manner.\\n3. **Hybrid models:** These models combine elements of both transformers and RNNs.\\n\\nExamples of large language models include:\\n\\n1. **BERT (Bidirectional Encoder Representations from Transformers):** Developed by Google, BERT is a widely used transformer-based LLM.\\n2. **RoBERTa (Robustly Optimized BERT Approach):** An improved version of BERT, developed by Facebook AI, that uses a different training approach.\\n3. **LLaMA (Large Language Model for All):** Developed by Meta AI, LLaMA is a large language model that can perform a wide range of tasks.\\n\\nThe applications of large language models are numerous, including:\\n\\n1. **Virtual assistants:** LLMs can be used to develop more human-like virtual assistants, like Siri, Alexa, and Google Assistant.\\n2. **Language translation:** LLMs can be used to develop more accurate and efficient language translation systems.\\n3. **Text summarization:** LLMs can be used to summarize long pieces of text into shorter, more digestible versions.\\n4. **Chatbots:** LLMs can be used to develop more sophisticated chatbots that can engage in natural-sounding conversations.\\n\\nHowever, large language models also raise concerns about:\\n\\n1. **Bias and fairness:** LLMs can perpetuate biases present in the training data, leading to unfair or discriminatory outcomes.\\n2. **Data quality:** The quality of the training data can impact the performance and reliability of LLMs.\\n3. **Explainability:** LLMs can be difficult to interpret and understand, making it challenging to explain their decisions.\\n\\nOverall, large language models have the potential to revolutionize the way we interact with language and information, but their development and deployment require careful consideration of these challenges.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt=\"\"\"\n",
        "You explain things to people like they are five year olds.\n",
        "\"\"\"\n",
        "user_prompt=f\"\"\"\n",
        "What is large language model?\n",
        "\"\"\"\n",
        "\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "import textwrap\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=system_prompt),\n",
        "    HumanMessage(content=user_prompt),\n",
        "]\n",
        "\n",
        "response=llm_gpt4.invoke(messages)\n",
        "answer = textwrap.fill(response.content, width=100)\n",
        "\n",
        "print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eFpy-lsQD8MG",
        "outputId": "8af36cb8-1236-4859-d04f-33fbc05c4931"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Imagine you have a super smart friend who knows everything you want to talk about. They can tell you\n",
            "stories, answer questions, and even write you a poem.  A Large Language Model is like that super\n",
            "smart friend, but instead of being a person, it's a computer program. It's made up of many tiny\n",
            "computers (called \"neurons\") that work together to understand what you say and give you a good\n",
            "answer.  When you ask the model a question or give it a task, it looks at all the words and\n",
            "sentences you gave it, and it tries to understand what you mean. It's like it's reading a big book\n",
            "with all the words and stories it's ever learned about.  The model uses this understanding to give\n",
            "you a good answer, and it can even talk back to you in a way that sounds like a real person. It's\n",
            "like having a conversation with your super smart friend!  But here's the really cool thing about\n",
            "Large Language Models: they can learn and get smarter over time. The more people use them and ask\n",
            "them questions, the more they can learn and understand about the world.  So, Large Language Models\n",
            "are like super smart friends that can help us with lots of things, from answering questions to\n",
            "writing stories and even creating art.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "prompt_template = \"\"\"\n",
        "You are a helpful assistant that explains AI topics. Given the following input:\n",
        "{topic}\n",
        "Provide an explanation of the given topic.\n",
        "\"\"\"\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"topic\"],\n",
        "    template=prompt_template,\n",
        ")\n",
        "# Assemble the chain using the pipe operator\n",
        "chain = prompt | llm_gpt4\n",
        "\n",
        "chain.invoke({\"topic\":\"What is large language model\"}).content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "id": "Qr03Pb6SEAv8",
        "outputId": "1603ba2f-a391-4d08-af16-8aea0f8545a7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'A large language model (LLM) is a type of artificial intelligence (AI) designed to process and understand human language. It\\'s called \"large\" because these models are typically trained on massive datasets, comprising hundreds of millions or even billions of words, and consist of millions or billions of parameters.\\n\\nHere\\'s a breakdown of how LLMs work:\\n\\n**Key Components:**\\n\\n1. **Training Data:** These models are trained on vast amounts of text data, which includes books, articles, websites, and even user-generated content. The training data is sourced from various places, such as the internet, books, and user feedback.\\n2. **Neural Network Architecture:** LLMs are based on neural networks, a type of machine learning model inspired by the structure and function of the human brain. These networks consist of multiple layers of interconnected nodes (neurons) that process and transmit information.\\n3. **Self-Supervised Learning:** LLMs use self-supervised learning, a type of unsupervised learning where the model is trained on unlabeled data. The model learns to predict the next word in a sentence based on the context, which helps it develop an understanding of language structures, grammar, and semantics.\\n\\n**How LLMs Work:**\\n\\n1. **Input:** When you ask an LLM a question or provide a prompt, it breaks down the text into a numerical representation, called a vector.\\n2. **Processing:** The LLM\\'s neural network processes the vector, considering the context, meaning, and relationships between words.\\n3. **Output:** The LLM generates a response, which is a sequence of words that are predicted based on the input and the model\\'s understanding of language.\\n\\n**Types of Large Language Models:**\\n\\n1. **Transformers:** Developed by Google, transformers are a type of LLM that uses self-attention mechanisms to process sequences of words. They\\'re widely used in natural language processing (NLP) tasks, such as language translation, text summarization, and question answering.\\n2. **Recurrent Neural Networks (RNNs):** RNNs are a type of LLM that use recurrent connections to process sequences of words. They\\'re often used in tasks like language modeling, text classification, and sentiment analysis.\\n\\n**Applications of Large Language Models:**\\n\\n1. **Chatbots:** LLMs are used in chatbots to generate human-like responses to user queries.\\n2. **Language Translation:** LLMs can translate text from one language to another with high accuracy.\\n3. **Text Summarization:** LLMs can summarize long documents, such as articles or books, into concise versions.\\n4. **Question Answering:** LLMs can answer questions based on the input text, making them useful for tasks like information retrieval and research.\\n\\nIn summary, large language models are powerful AI tools that can process and understand human language. They\\'re trained on massive datasets and use neural networks to generate human-like responses to user queries. Their applications are diverse, and they\\'re being used in various industries, including customer service, language translation, and research.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install --upgrade --quiet  youtube-transcript-api"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hyBwujDFEEeA",
        "outputId": "10d9f113-e115-45d5-d12f-93fb20fadf75"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/622.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m614.4/622.3 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m622.3/622.3 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import YoutubeLoader\n",
        "\n",
        "loader = YoutubeLoader.from_youtube_url(\n",
        "    \"https://youtu.be/h04DwdAkNZ4?si=C7MPK1mqvkBzUAAR\", add_video_info=False\n",
        ")\n",
        "\n",
        "\n",
        "# Load the video transcript as documents\n",
        "docs=loader.load()\n",
        "print(\"docs\",docs)\n",
        "transcript=docs[0].page_content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JVCenai2EOZn",
        "outputId": "b2639e64-d0af-4d39-84fa-ef0a46960877"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "docs [Document(metadata={'source': 'h04DwdAkNZ4'}, page_content=\"uh Hey guys so recently I have written this uh this article or you can call it a tutorial on how to create open source AI applications using Lang chain uh so we need to understand that uh LMS are not just enough to create your to build your eii applications you need to have a proper toolkit or a framework and uh um eii Frameworks like Lang chain and Lama andex uh uh help us build these a applications seamlessly uh so uh uh what is langin it's basically an open open source framework for eiml data Engineers to develop uh sophisticated Eid driven applications powered by llm and uh basically langin facilitates uh the integration of uh language models with all the required components including um external databases like vector databases logic reasoning apis and Etc so so these all are required to enance the capabilities of llm powered applications so Lan chain and then llama index they both provide uh provide this this toolkit so basically langin has six modules and U they include models chains uh prompts indexes memory and agents so all these six models um help us build uh llm powered applications uh seamlessly uh so in this tutorial what I've have done is um um uh we have used langin like uh like I said as a framework and then we have used publicly available uh PDF we have split the PDF uh and uh and and we have stored the chunks of the PDF uh into a vector database like single store um uh and uh we will ask the query at the end and retrieve the most relevant uh response uh for our query so uh we will use a single Stone notebook feature it's just like a Google collab or um uh you just like jupyter notebooks uh where uh you will build uh where we'll we'll run our application code and see how uh the things go forward okay so for that you need to uh activate your single store uh single store um account uh it's free for that you need to go to single store and then uh try for free it's completely free free forever no time bound and once you sign up you're going to get $600 worth free credits for the first time uh I already have my account so I'll just sign in um so let's go back to our article uh first things first you need to create a workspace uh for yourself and uh after creating a workspace you you need to create a database uh um uh let me show you so once you sign up you'll you you'll land on your single store dashboard like something like this if it's your first time it might uh look something little different for you because I have already I've been using this uh dashboard for uh uh for very long and I have created some notebooks already so if you go to deployments you can see um this is the this is this is my workspace I have already created and these are the databases attached to my workspace creating another workspace is very easy and this is how you can create a cloud workspace for yourself you just give it a name and then create a workspace I already have my have my workspace and then creating a database under your workspace is very easy just say create a database give it a name and then say Creator databas right so uh that's what we did and then we are going to use like I said uh single store notebooks uh where we will add all our notebook code and then we'll execute the code one by one to see how um how we can build our AI framework like so we should go to develop and once you go to this develop tab there is new notebook there is a notebook feature and uh you can say new notebook and then just name your notebook and then create a notebook you can create a blank notebook you can create a um there there's a personal notebook you can create and also you can create a shade notebook if you are working with your colleagues or somebody okay so once you create a notebook um this is how let me show you that to you I can say test create a notebook you will go to a dashboard of the notebook where you can add the code and then start just running can add the code and run the code okay so it's just uh spinning up yeah this is our this is how the notebook looks this is just for the example so you can add this code basically so what we are doing so basically we are installing the required libraries from lanin right we we are we are building this CI app using Lan chain so we need lch so we'll just copy this and add this here okay copy this thing then add it here and then we'll just run it so you will do the same thing next thing is load the PDF so we have this um publicly available PDF uh we will load that PDF and then what we'll do we'll split and uh read the content of the PDF we'll split that PDF into chunks and then what we'll do we'll set up a database to store the contents of a PDF okay so we are creating a database by name uh Lang DB and uh we are storing uh our content there and uh next thing is uh we need a open API key open API key and then we are we mentioning that and then we creating embeddings and inserting uh them into the database okay all the content along with the um vector embeddings and then we are asking a query like query text is um will object oriented database uh bases be commercially successful so this is a query um we are asking and it's going to give us the relevant answer and then we can also find the most similar text from the PDF okay and uh we are uh telling our we are using the model GPT 3.5 TBO and uh the role of this whole thing is like you know we are telling it like you are a helpful assistant so it'll act as our assistant okay and it'll for any query it's going to answer us back okay so that's how quickly you can build your AI application using uh L chain so notebook code is here uh you can just go to this uh I I'll share this notebook code to you in in the comments or in the video description and you can go and try this okay so yeah thank you\")]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We can now use the transcript in a chain\n",
        "prompt_template = \"\"\"\n",
        "You are a helpful assistant that explains YT videos. Given the following video transcript:\n",
        "{video_transcript}\n",
        "Give a summary.\n",
        "\"\"\"\n",
        "\n",
        "# Create the prompt\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"video_transcript\"],\n",
        "    template=prompt_template,\n",
        ")\n",
        "chain = prompt | llm_gpt4\n",
        "chain.invoke({\"video_transcript\":docs}).content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "id": "pej3Hb2YEUkN",
        "outputId": "85d5eebf-ab0a-4ad0-f637-d026fab674ad"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"The video transcript is about creating an open-source AI application using Lang Chain, a framework for developing AI-driven applications powered by Large Language Models (LLM). The creator of the tutorial uses Lang Chain to build a simple application that can answer questions based on a PDF document.\\n\\nHere's a summary of the steps involved:\\n\\n1. **Creating a Workspace and Database**: The creator sets up a workspace on Single Store, a cloud-based database service, and creates a database to store the content of the PDF document.\\n2. **Loading the PDF**: The creator loads a publicly available PDF document and splits it into chunks.\\n3. **Storing the Content**: The creator sets up a database to store the contents of the PDF document and creates embeddings to represent the content.\\n4. **Creating an Open AI API Key**: The creator creates an Open AI API key to use with the Lang Chain framework.\\n5. **Building the AI Application**: The creator uses Lang Chain to build a simple AI application that can answer questions based on the PDF document.\\n6. **Training the Model**: The creator uses the GPT-3.5-TBO model to train the AI application.\\n7. **Testing the Application**: The creator tests the application by asking it questions and seeing how it responds.\\n\\nThe video transcript provides a step-by-step guide on how to build a simple AI application using Lang Chain and Single Store. The creator shares the notebook code in the comments or video description, allowing viewers to try it out themselves.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "prompt_template = \"\"\"\n",
        "You are a helpful assistant that explains AI topics. Given the following context:\n",
        "{context}\n",
        "Summarize what LangChain can do.\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"context\"],\n",
        "    template=prompt_template,\n",
        ")\n",
        "chain = create_stuff_documents_chain(llm_gpt4, prompt)\n",
        "\n",
        "chain.invoke({\"context\": docs})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "id": "bKuGs7r9Ehfp",
        "outputId": "252eeebb-a2e4-4bdd-f41e-4e7c71a3c214"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'LangChain is an open-source framework for building AI applications. It facilitates the integration of language models with various components, including external databases like vector databases, logic reasoning APIs, and more. This enables the development of sophisticated AI-driven applications powered by Large Language Models (LLMs).\\n\\nLangChain has six modules: models, chains, prompts, indexes, memory, and agents. These modules help build LLM-powered applications seamlessly.\\n\\nWith LangChain, you can:\\n\\n1. Integrate LLMs with external databases and APIs.\\n2. Build and manage complex workflows and data flows.\\n3. Create and manage knowledge graphs and databases.\\n4. Develop and deploy AI applications using a variety of tools and frameworks.\\n\\nIn the context of the given tutorial, LangChain was used to:\\n\\n1. Load and process a publicly available PDF.\\n2. Split the PDF into chunks and store its content in a vector database.\\n3. Create a database to store the PDF content with vector embeddings.\\n4. Ask queries to retrieve relevant answers and find similar text in the PDF.\\n\\nOverall, LangChain provides a toolkit for building AI applications by simplifying the integration of LLMs with various components and enabling the development of sophisticated AI-driven applications.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "summarize_prompt_template = \"\"\"\n",
        "You are a helpful assistant that summarizes AI concepts:\n",
        "{context}\n",
        "Summarize the context\n",
        "\"\"\"\n",
        "\n",
        "summarize_prompt = PromptTemplate.from_template(summarize_prompt_template)"
      ],
      "metadata": {
        "id": "UqnCY23uEtBy"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(summarize_prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DALMxScqEuvF",
        "outputId": "818911ad-24d5-4327-edb6-648cc6657aa4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_variables=['context'] input_types={} partial_variables={} template='\\nYou are a helpful assistant that summarizes AI concepts:\\n{context}\\nSummarize the context\\n'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_parser = StrOutputParser()\n",
        "\n",
        "chain = summarize_prompt | llm_gpt4 | output_parser\n",
        "\n",
        "chain.invoke({\"context\": \"What is LangChain?\"})\n",
        "# Verify the type of the chain\n",
        "print(type(chain))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufrBaJT5EusS",
        "outputId": "65d5f85d-ad3d-429f-eb0a-39253f8f8d90"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'langchain_core.runnables.base.RunnableSequence'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inject python functions into a chain with RunnableLambda\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "summarize_chain = summarize_prompt | llm_gpt4 | output_parser\n",
        "\n",
        "# Define a custom lambda function and wrap it in RunnableLambda\n",
        "length_lambda = RunnableLambda(lambda summary: f\"Summary length: {len(summary)} characters\")\n",
        "\n",
        "lambda_chain = summarize_chain | length_lambda\n",
        "\n",
        "lambda_chain.invoke({\"context\": \"What is LangChain?\"})\n",
        "print(type(lambda_chain.steps[-1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Cg3dq2CE9Ej",
        "outputId": "20260247-a268-4c6c-ca37-7bd1da68cef6"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'langchain_core.runnables.base.RunnableLambda'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use function in chain without converting to RunnableLambda\n",
        "chain_with_function = summarize_chain |  (lambda summary: f\"Summary length: {len(summary)} characters\")\n",
        "print(type(chain_with_function.steps[-1]))\n",
        "chain_with_function.invoke({\"context\": \"What is LangChain?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "Zk2tcCoMFA_n",
        "outputId": "1854d66a-31dd-462c-a6cd-59487901232d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'langchain_core.runnables.base.RunnableLambda'>\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Summary length: 1248 characters'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=100,\n",
        "    chunk_overlap=20,\n",
        "    length_function=len,\n",
        "    is_separator_regex=False,\n",
        ")\n",
        "\n",
        "docs_split = text_splitter.split_documents(docs)\n",
        "docs_split"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZnpXIQs4FHgi",
        "outputId": "26419aff-dc61-461f-895a-1add42b2dc74"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': 'h04DwdAkNZ4'}, page_content='uh Hey guys so recently I have written this uh this article or you can call it a tutorial on how to'),\n",
              " Document(metadata={'source': 'h04DwdAkNZ4'}, page_content='tutorial on how to create open source AI applications using Lang chain uh so we need to understand'),\n",
              " Document(metadata={'source': 'h04DwdAkNZ4'}, page_content='need to understand that uh LMS are not just enough to create your to build your eii applications'),\n",
              " Document(metadata={'source': 'h04DwdAkNZ4'}, page_content='eii applications you need to have a proper toolkit or a framework and uh um eii Frameworks like'),\n",
              " Document(metadata={'source': 'h04DwdAkNZ4'}, page_content='eii Frameworks like Lang chain and Lama andex uh uh help us build these a applications seamlessly'),\n",
              " Document(metadata={'source': 'h04DwdAkNZ4'}, page_content=\"seamlessly uh so uh uh what is langin it's basically an open open source framework for eiml data\"),\n",
              " Document(metadata={'source': 'h04DwdAkNZ4'}, page_content='for eiml data Engineers to develop uh sophisticated Eid driven applications powered by llm and uh'),\n",
              " Document(metadata={'source': 'h04DwdAkNZ4'}, page_content='by llm and uh basically langin facilitates uh the integration of uh language models with all the'),\n",
              " Document(metadata={'source': 'h04DwdAkNZ4'}, page_content='models with all the required components including um external databases like vector databases logic'),\n",
              " Document(metadata={'source': 'h04DwdAkNZ4'}, page_content='databases logic reasoning apis and Etc so so these all are required to enance the capabilities of'),\n",
              " Document(metadata={'source': 'h04DwdAkNZ4'}, page_content='the capabilities of llm powered applications so Lan chain and then llama index they both provide uh'),\n",
              " Document(metadata={'source': 'h04DwdAkNZ4'}, page_content='both provide uh provide this this toolkit so basically langin has six modules and U they include'),\n",
              " Document(metadata={'source': 'h04DwdAkNZ4'}, page_content='and U they include models chains uh prompts indexes memory and agents so all these six models um'),\n",
              " Document(metadata={'source': 'h04DwdAkNZ4'}, page_content='these six models um help us build uh llm powered applications uh seamlessly uh so in this tutorial'),\n",
              " Document(metadata={'source': 'h04DwdAkNZ4'}, page_content=\"so in this tutorial what I've have done is um um uh we have used langin like uh like I said as a\"),\n",
              " Document(metadata={'source': 'h04DwdAkNZ4'}, page_content='uh like I said as a framework and then we have used publicly available uh PDF we have split the PDF'),\n",
              " Document(metadata={'source': 'h04DwdAkNZ4'}, page_content='have split the PDF uh and uh and and we have stored the chunks of the PDF uh into a vector database'),\n",
              " Document(metadata={'source': 'h04DwdAkNZ4'}, page_content='a vector database like single store um uh and uh we will ask the query at the end and retrieve the'),\n",
              " Document(metadata={'source': 'h04DwdAkNZ4'}, page_content='and retrieve the most relevant uh response uh for our query so uh we will use a single Stone'),\n",
              " Document(metadata={'source': 'h04DwdAkNZ4'}, page_content=\"use a single Stone notebook feature it's just like a Google collab or um uh you just like jupyter\"),\n",
              " Document(metadata={'source': 'h04DwdAkNZ4'}, page_content=\"just like jupyter notebooks uh where uh you will build uh where we'll we'll run our application\"),\n",
              " Document(metadata={'source': 'h04DwdAkNZ4'}, page_content='run our application code and see how uh the things go forward okay so for that you need to uh'),\n",
              " Document(metadata={'source': 'h04DwdAkNZ4'}, page_content=\"that you need to uh activate your single store uh single store um account uh it's free for that you\"),\n",
              " Document(metadata={'source': 'h04DwdAkNZ4'}, page_content=\"free for that you need to go to single store and then uh try for free it's completely free free\"),\n",
              " Document(metadata={'source': 'h04DwdAkNZ4'}, page_content=\"free free forever no time bound and once you sign up you're going to get $600 worth free credits\"),\n",
              " Document(metadata={'source': 'h04DwdAkNZ4'}, page_content=\"worth free credits for the first time uh I already have my account so I'll just sign in um so let's\"),\n",
              " Document(metadata={'source': 'h04DwdAkNZ4'}, page_content=\"sign in um so let's go back to our article uh first things first you need to create a workspace uh\"),\n",
              " Document(metadata={'source': 'h04DwdAkNZ4'}, page_content='a workspace uh for yourself and uh after creating a workspace you you need to create a database uh'),\n",
              " Document(metadata={'source': 'h04DwdAkNZ4'}, page_content=\"a database uh um uh let me show you so once you sign up you'll you you'll land on your single store\"),\n",
              " Document(metadata={'source': 'h04DwdAkNZ4'}, page_content=\"your single store dashboard like something like this if it's your first time it might uh look\"),\n",
              " Document(metadata={'source': 'h04DwdAkNZ4'}, page_content=\"it might uh look something little different for you because I have already I've been using this uh\"),\n",
              " Document(metadata={'source': 'h04DwdAkNZ4'}, page_content='been using this uh dashboard for uh uh for very long and I have created some notebooks already so'),\n",
              " Document(metadata={'source': 'h04DwdAkNZ4'}, page_content='already so if you go to deployments you can see um this is the this is this is my workspace I have'),\n",
              " Document(metadata={'source': 'h04DwdAkNZ4'}, page_content='my workspace I have already created and these are the databases attached to my workspace creating'),\n",
              " Document(metadata={'source': 'h04DwdAkNZ4'}, page_content='workspace creating another workspace is very easy and this is how you can create a cloud workspace'),\n",
              " Document(metadata={'source': 'h04DwdAkNZ4'}, page_content='a cloud workspace for yourself you just give it a name and then create a workspace I already have'),\n",
              " Document(metadata={'source': 'h04DwdAkNZ4'}, page_content='I already have my have my workspace and then creating a database under your workspace is very easy'),\n",
              " Document(metadata={'source': 'h04DwdAkNZ4'}, page_content='is very easy just say create a database give it a name and then say Creator databas right so uh'),\n",
              " Document(metadata={'source': 'h04DwdAkNZ4'}, page_content=\"databas right so uh that's what we did and then we are going to use like I said uh single store\"),\n",
              " Document(metadata={'source': 'h04DwdAkNZ4'}, page_content=\"uh single store notebooks uh where we will add all our notebook code and then we'll execute the\"),\n",
              " Document(metadata={'source': 'h04DwdAkNZ4'}, page_content=\"we'll execute the code one by one to see how um how we can build our AI framework like so we should\"),\n",
              " Document(metadata={'source': 'h04DwdAkNZ4'}, page_content='like so we should go to develop and once you go to this develop tab there is new notebook there is'),\n",
              " Document(metadata={'source': 'h04DwdAkNZ4'}, page_content='notebook there is a notebook feature and uh you can say new notebook and then just name your'),\n",
              " Document(metadata={'source': 'h04DwdAkNZ4'}, page_content='then just name your notebook and then create a notebook you can create a blank notebook you can'),\n",
              " Document(metadata={'source': 'h04DwdAkNZ4'}, page_content=\"notebook you can create a um there there's a personal notebook you can create and also you can\"),\n",
              " Document(metadata={'source': 'h04DwdAkNZ4'}, page_content='and also you can create a shade notebook if you are working with your colleagues or somebody okay'),\n",
              " Document(metadata={'source': 'h04DwdAkNZ4'}, page_content='or somebody okay so once you create a notebook um this is how let me show you that to you I can say'),\n",
              " Document(metadata={'source': 'h04DwdAkNZ4'}, page_content='to you I can say test create a notebook you will go to a dashboard of the notebook where you can'),\n",
              " Document(metadata={'source': 'h04DwdAkNZ4'}, page_content='where you can add the code and then start just running can add the code and run the code okay so'),\n",
              " Document(metadata={'source': 'h04DwdAkNZ4'}, page_content=\"the code okay so it's just uh spinning up yeah this is our this is how the notebook looks this is\"),\n",
              " Document(metadata={'source': 'h04DwdAkNZ4'}, page_content='looks this is just for the example so you can add this code basically so what we are doing so'),\n",
              " Document(metadata={'source': 'h04DwdAkNZ4'}, page_content='we are doing so basically we are installing the required libraries from lanin right we we are we'),\n",
              " Document(metadata={'source': 'h04DwdAkNZ4'}, page_content=\"right we we are we are building this CI app using Lan chain so we need lch so we'll just copy this\"),\n",
              " Document(metadata={'source': 'h04DwdAkNZ4'}, page_content=\"just copy this and add this here okay copy this thing then add it here and then we'll just run it\"),\n",
              " Document(metadata={'source': 'h04DwdAkNZ4'}, page_content=\"we'll just run it so you will do the same thing next thing is load the PDF so we have this um\"),\n",
              " Document(metadata={'source': 'h04DwdAkNZ4'}, page_content=\"so we have this um publicly available PDF uh we will load that PDF and then what we'll do we'll\"),\n",
              " Document(metadata={'source': 'h04DwdAkNZ4'}, page_content=\"what we'll do we'll split and uh read the content of the PDF we'll split that PDF into chunks and\"),\n",
              " Document(metadata={'source': 'h04DwdAkNZ4'}, page_content=\"PDF into chunks and then what we'll do we'll set up a database to store the contents of a PDF okay\"),\n",
              " Document(metadata={'source': 'h04DwdAkNZ4'}, page_content='of a PDF okay so we are creating a database by name uh Lang DB and uh we are storing uh our content'),\n",
              " Document(metadata={'source': 'h04DwdAkNZ4'}, page_content='uh our content there and uh next thing is uh we need a open API key open API key and then we are we'),\n",
              " Document(metadata={'source': 'h04DwdAkNZ4'}, page_content='and then we are we mentioning that and then we creating embeddings and inserting uh them into the'),\n",
              " Document(metadata={'source': 'h04DwdAkNZ4'}, page_content='uh them into the database okay all the content along with the um vector embeddings and then we are'),\n",
              " Document(metadata={'source': 'h04DwdAkNZ4'}, page_content='and then we are asking a query like query text is um will object oriented database uh bases be'),\n",
              " Document(metadata={'source': 'h04DwdAkNZ4'}, page_content=\"uh bases be commercially successful so this is a query um we are asking and it's going to give us\"),\n",
              " Document(metadata={'source': 'h04DwdAkNZ4'}, page_content='going to give us the relevant answer and then we can also find the most similar text from the PDF'),\n",
              " Document(metadata={'source': 'h04DwdAkNZ4'}, page_content='text from the PDF okay and uh we are uh telling our we are using the model GPT 3.5 TBO and uh the'),\n",
              " Document(metadata={'source': 'h04DwdAkNZ4'}, page_content='3.5 TBO and uh the role of this whole thing is like you know we are telling it like you are a'),\n",
              " Document(metadata={'source': 'h04DwdAkNZ4'}, page_content=\"it like you are a helpful assistant so it'll act as our assistant okay and it'll for any query it's\"),\n",
              " Document(metadata={'source': 'h04DwdAkNZ4'}, page_content=\"for any query it's going to answer us back okay so that's how quickly you can build your AI\"),\n",
              " Document(metadata={'source': 'h04DwdAkNZ4'}, page_content='can build your AI application using uh L chain so notebook code is here uh you can just go to this'),\n",
              " Document(metadata={'source': 'h04DwdAkNZ4'}, page_content=\"can just go to this uh I I'll share this notebook code to you in in the comments or in the video\"),\n",
              " Document(metadata={'source': 'h04DwdAkNZ4'}, page_content='or in the video description and you can go and try this okay so yeah thank you')]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LUHTc4qrGezW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
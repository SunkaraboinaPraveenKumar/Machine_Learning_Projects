{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SunkaraboinaPraveenKumar/Machine_Learning_Projects/blob/main/ParaPhrase_Transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GTzQhQjdx1KT",
        "outputId": "8ed5577a-17bf-4411-df94-a498de246d66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-splitter in /usr/local/lib/python3.10/dist-packages (1.4)\n",
            "Requirement already satisfied: regex>=2017.12.12 in /usr/local/lib/python3.10/dist-packages (from sentence-splitter) (2024.5.15)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n",
            "Requirement already satisfied: SentencePiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n"
          ]
        }
      ],
      "source": [
        "!pip install sentence-splitter\n",
        "!pip install transformers\n",
        "!pip install SentencePiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "y6yIvO2XyB8n"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import PegasusForConditionalGeneration, PegasusTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model=PegasusForConditionalGeneration.from_pretrained(\"tuner007/pegasus_paraphrase\")\n",
        "tokenizer=PegasusTokenizer.from_pretrained(\"tuner007/pegasus_paraphrase\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KD2P4DYZ1jnO",
        "outputId": "029c8db1-68af-4a01-a4cf-0e7582b30455"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at tuner007/pegasus_paraphrase and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"The ultimate test of your knowledge is your capacity to convey it to another.\"\n",
        "\n",
        "batch=tokenizer([text],padding=True,truncation=True,max_length=60,return_tensors='pt')\n",
        "\n",
        "output=model.generate(**batch,max_length=60,num_beams=5,num_return_sequences=5,temperature=1.5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5nLHII3m1oA-",
        "outputId": "f1bd9f5d-f80d-4080-957d-810514c17ecb"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `1.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "paraphrases=tokenizer.batch_decode(output,skip_special_tokens=True)\n",
        "paraphrases"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6EBehYN17M9",
        "outputId": "5764dbaa-642d-4212-af39-d1d347762103"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The test of your knowledge is your ability to convey it.',\n",
              " 'The ability to convey your knowledge is the ultimate test of your knowledge.',\n",
              " 'The test of your knowledge is your ability to communicate it.',\n",
              " 'Your capacity to convey your knowledge is the ultimate test of your knowledge.',\n",
              " 'The test of your knowledge is how well you can convey it.']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"/content/drive/MyDrive/model\")\n",
        "tokenizer.save_pretrained(\"/content/drive/MyDrive/tokenizer\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h1yDFbbs1p9z",
        "outputId": "d691dd28-582b-41e3-e7c2-dbb4133f101b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 60, 'num_beams': 8, 'length_penalty': 0.8, 'forced_eos_token_id': 1}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/MyDrive/tokenizer/tokenizer_config.json',\n",
              " '/content/drive/MyDrive/tokenizer/special_tokens_map.json',\n",
              " '/content/drive/MyDrive/tokenizer/spiece.model',\n",
              " '/content/drive/MyDrive/tokenizer/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Load Saved Model\n",
        "model=PegasusForConditionalGeneration.from_pretrained(\"/content/drive/MyDrive/model\")\n",
        "tokenizer=PegasusTokenizer.from_pretrained(\"/content/drive/MyDrive/tokenizer\")"
      ],
      "metadata": {
        "id": "_VanTA3f1sUk"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Predictive System"
      ],
      "metadata": {
        "id": "Cm79aumY2nF9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_response(input_text,num_return_sequences=5,num_beams=5):\n",
        "  batch=tokenizer([input_text],padding=True,truncation=True,max_length=60,return_tensors='pt')\n",
        "  translated=model.generate(**batch,max_length=60,num_beams=num_beams,num_return_sequences=num_return_sequences,temperature=1.5)\n",
        "  tgt_text=tokenizer.batch_decode(translated,skip_special_tokens=True)\n",
        "  return tgt_text;"
      ],
      "metadata": {
        "id": "k3hMq5vp2lzf"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_beams=10\n",
        "num_return_sequences=10\n",
        "context=\"Machine Learning is a Growing Field in AI.\"\n",
        "get_response(context,num_return_sequences,num_beams)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JlhDvt652yO2",
        "outputId": "2ed69702-0367-4420-fdfc-79922d04d918"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `1.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The field of machine learning is growing.',\n",
              " 'There is a growing field of machine learning.',\n",
              " 'Machine Learning is a growing field.',\n",
              " 'Machine Learning is growing.',\n",
              " 'Machine learning is growing.',\n",
              " 'Machine learning is a growing field.',\n",
              " 'Machine Learning is growing in popularity.',\n",
              " 'Machine Learning is a field that is growing.',\n",
              " 'Machine Learning is growing fast.',\n",
              " 'Machine learning is a field that is growing.']"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B2Pv3J-I35d9"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNiZD+mOxrUah0066v/16to",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}